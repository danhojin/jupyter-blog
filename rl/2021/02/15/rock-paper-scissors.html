<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>가위바위보 강화학습 | fastpages</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="가위바위보 강화학습" />
<meta name="author" content="단호진" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="두 에이전트가 겨루는 가위바위보 환경에서 강화학습을 통하여 승리 정책을 학습한다." />
<meta property="og:description" content="두 에이전트가 겨루는 가위바위보 환경에서 강화학습을 통하여 승리 정책을 학습한다." />
<link rel="canonical" href="https://danhojin.github.io/jupyter-blog/rl/2021/02/15/rock-paper-scissors.html" />
<meta property="og:url" content="https://danhojin.github.io/jupyter-blog/rl/2021/02/15/rock-paper-scissors.html" />
<meta property="og:site_name" content="fastpages" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-02-15T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"두 에이전트가 겨루는 가위바위보 환경에서 강화학습을 통하여 승리 정책을 학습한다.","url":"https://danhojin.github.io/jupyter-blog/rl/2021/02/15/rock-paper-scissors.html","@type":"BlogPosting","headline":"가위바위보 강화학습","dateModified":"2021-02-15T00:00:00-06:00","datePublished":"2021-02-15T00:00:00-06:00","author":{"@type":"Person","name":"단호진"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://danhojin.github.io/jupyter-blog/rl/2021/02/15/rock-paper-scissors.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/jupyter-blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://danhojin.github.io/jupyter-blog/feed.xml" title="fastpages" /><link rel="shortcut icon" type="image/x-icon" href="/jupyter-blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/jupyter-blog/">fastpages</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/jupyter-blog/about/">About Me</a><a class="page-link" href="/jupyter-blog/search/">Search</a><a class="page-link" href="/jupyter-blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">가위바위보 강화학습</h1><p class="page-description">두 에이전트가 겨루는 가위바위보 환경에서 강화학습을 통하여 승리 정책을 학습한다.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-02-15T00:00:00-06:00" itemprop="datePublished">
        Feb 15, 2021
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">단호진</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      5 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/jupyter-blog/categories/#rl">rl</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/danhojin/jupyter-blog/tree/master/_notebooks/2021-02-15-rock-paper-scissors.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/jupyter-blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/danhojin/jupyter-blog/master?filepath=_notebooks%2F2021-02-15-rock-paper-scissors.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/jupyter-blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/danhojin/jupyter-blog/blob/master/_notebooks/2021-02-15-rock-paper-scissors.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/jupyter-blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#가위바위보-환경">가위바위보 환경 </a></li>
<li class="toc-entry toc-h2"><a href="#정책">정책 </a></li>
<li class="toc-entry toc-h2"><a href="#트레이너-설정-및-학습">트레이너 설정 및 학습 </a></li>
<li class="toc-entry toc-h2"><a href="#맺으며">맺으며 </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-02-15-rock-paper-scissors.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="가위바위보-환경">
<a class="anchor" href="#%EA%B0%80%EC%9C%84%EB%B0%94%EC%9C%84%EB%B3%B4-%ED%99%98%EA%B2%BD" aria-hidden="true"><span class="octicon octicon-link"></span></a>가위바위보 환경<a class="anchor-link" href="#%EA%B0%80%EC%9C%84%EB%B0%94%EC%9C%84%EB%B3%B4-%ED%99%98%EA%B2%BD"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">ray.rllib.utils.framework</span> <span class="kn">import</span> <span class="n">try_import_torch</span>
<span class="kn">from</span> <span class="nn">ray.rllib.agents.pg</span> <span class="kn">import</span> <span class="n">PGTorchPolicy</span><span class="p">,</span> <span class="n">PGTrainer</span>
<span class="kn">from</span> <span class="nn">ray.rllib.examples.env.rock_paper_scissors</span> <span class="kn">import</span> <span class="n">RockPaperScissors</span>

<span class="n">torch</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">try_import_torch</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">__version__</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>WARNING:tensorflow:From /home/danhojin/miniconda3/envs/ml8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>'1.7.1+cu110'</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>player1과 player2가 취할 수 있는 action은 0, 1, 2이고, 관측치는 상대방의 action에서 나오는 0, 1, 2이다.</li>
<li>env.reset()은 첫 관측치 {'player1': 0, 'player2': 0}를 내어 준다.</li>
<li>env.step() 함수는 obs, rewards, done, info를 돌려준다.</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">env_config</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">RockPaperScissors</span><span class="p">(</span><span class="n">env_config</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">obs</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{'player1': 0, 'player2': 0}</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(Discrete(3), Discrete(3))</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">player1</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">player2</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>({'player1': 2, 'player2': 1},
 {'player1': -1, 'player2': 1},
 {'__all__': False},
 {})</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">player1</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">player2</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>({'player1': 1, 'player2': 2},
 {'player1': 1, 'player2': -1},
 {'__all__': False},
 {})</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">player1</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">player2</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>({'player1': 2, 'player2': 0},
 {'player1': 1, 'player2': -1},
 {'__all__': False},
 {})</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="정책">
<a class="anchor" href="#%EC%A0%95%EC%B1%85" aria-hidden="true"><span class="octicon octicon-link"></span></a>정책<a class="anchor-link" href="#%EC%A0%95%EC%B1%85"> </a>
</h2>
<p>정책(Policy)은 에이전트가 관측된 환경, 이전 보상 이력을 참고하여 어떤 행동을 취하면 좋은지 결정한다. 에이전트가 둘인 가위바위보 게임에서 학습 가능한 learned 정책과 RandomMove, BeatLastHeuristic, AlwaysSameHeuristic 정책에서 하나를 뽑아 강확학습을 수행하였다. 가위바위보의 최고 전략은 RandomMove로 learned 정책은 이에 도달하는 결과를 보일 것이고, 그 밖에 BeatLastHeuristic, AlwaysSameHeuristic에 데해서는 쉽게 이길 수 있을 것이다.</p>
<p>새로운 정책을 개발한다면 Policy 클래스를 상속하고 compute_actions에 필요한 로직을 구현하면 된다. RandomMove 정책은 obs_batch에 대해서만 고려하여 배치 크기만큼의 임의 행동을 돌려준다. kwargs에 필수 생성 인자 내용을 정리하였다. 환경의 action_space와 observation_space가 그것이다.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">ray.rllib.policy.policy</span> <span class="kn">import</span> <span class="n">Policy</span>
<span class="kn">from</span> <span class="nn">ray.rllib.policy.view_requirement</span> <span class="kn">import</span> <span class="n">ViewRequirement</span>


<span class="k">class</span> <span class="nc">RandomMove</span><span class="p">(</span><span class="n">Policy</span><span class="p">):</span>
    <span class="sd">"""Pick a random move"""</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">random_action</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span>
                <span class="n">RockPaperScissors</span><span class="o">.</span><span class="n">ROCK</span><span class="p">,</span> <span class="n">RockPaperScissors</span><span class="o">.</span><span class="n">PAPER</span><span class="p">,</span>
                <span class="n">RockPaperScissors</span><span class="o">.</span><span class="n">SCISSORS</span><span class="p">,</span>
        <span class="p">])</span>
    
    <span class="k">def</span> <span class="nf">compute_actions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                        <span class="n">obs_batch</span><span class="p">,</span>
                        <span class="n">state_batches</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">prev_action_batch</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">prev_reward_batch</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">info_batch</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">episodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">"""Returns:</span>
<span class="sd">            Tuple:</span>
<span class="sd">                actions: [BATCH_SIZE, ACTION_SHAPE]</span>
<span class="sd">        """</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">random_action</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">obs_batch</span><span class="p">],</span> <span class="p">[],</span> <span class="p">{}</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'observation_space'</span><span class="p">:</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span>
    <span class="s1">'action_space'</span><span class="p">:</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span>
    <span class="s1">'config'</span><span class="p">:</span> <span class="p">{}</span>
<span class="p">}</span>
<span class="n">rm</span> <span class="o">=</span> <span class="n">RandomMove</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">rm</span><span class="o">.</span><span class="n">compute_actions</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>([0, 2, 2], [], {})</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="트레이너-설정-및-학습">
<a class="anchor" href="#%ED%8A%B8%EB%A0%88%EC%9D%B4%EB%84%88-%EC%84%A4%EC%A0%95-%EB%B0%8F-%ED%95%99%EC%8A%B5" aria-hidden="true"><span class="octicon octicon-link"></span></a>트레이너 설정 및 학습<a class="anchor-link" href="#%ED%8A%B8%EB%A0%88%EC%9D%B4%EB%84%88-%EC%84%A4%EC%A0%95-%EB%B0%8F-%ED%95%99%EC%8A%B5"> </a>
</h2>
<p>앞서 정의한 RandomMove외에 ray에서 제공하는 BeatLastHeuristic, AlwaysSameHeuristic 정책을 추가하였다.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">from</span> <span class="nn">gym.spaces</span> <span class="kn">import</span> <span class="n">Discrete</span>
<span class="kn">from</span> <span class="nn">ray.rllib.agents.registry</span> <span class="kn">import</span> <span class="n">get_trainer_class</span>
<span class="kn">from</span> <span class="nn">ray.rllib.examples.policy.rock_paper_scissors_dummies</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">BeatLastHeuristic</span><span class="p">,</span> <span class="n">AlwaysSameHeuristic</span>
<span class="p">)</span>


<span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">'env'</span><span class="p">:</span> <span class="n">RockPaperScissors</span><span class="p">,</span>
    <span class="s1">'gamma'</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>
    <span class="s1">'num_gpus'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">'num_workers'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">'num_envs_per_worker'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="s1">'train_batch_size'</span><span class="p">:</span> <span class="mi">200</span><span class="p">,</span>  <span class="c1"># for the policy model</span>
    <span class="s1">'multiagent'</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">'policies'</span><span class="p">:</span> <span class="p">{</span>
            <span class="s1">'random_move'</span><span class="p">:</span> <span class="p">(</span><span class="n">RandomMove</span><span class="p">,</span> <span class="n">Discrete</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">Discrete</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="p">{}),</span>
            <span class="s1">'beat_last'</span><span class="p">:</span> <span class="p">(</span><span class="n">BeatLastHeuristic</span><span class="p">,</span> <span class="n">Discrete</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">Discrete</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="p">{}),</span>
            <span class="s1">'always_same'</span><span class="p">:</span> <span class="p">(</span><span class="n">AlwaysSameHeuristic</span><span class="p">,</span> <span class="n">Discrete</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">Discrete</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="p">{}),</span>
            <span class="s1">'learned'</span><span class="p">:</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">Discrete</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">Discrete</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="p">{</span>
                <span class="s1">'model'</span><span class="p">:</span> <span class="p">{},</span>  <span class="c1"># use default</span>
                <span class="s1">'framework'</span><span class="p">:</span> <span class="s1">'torch'</span>
            <span class="p">})</span>
        <span class="p">},</span>
        <span class="s1">'policy_mapping_fn'</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">agent_id</span><span class="p">:</span> <span class="p">(</span>
            <span class="s1">'learned'</span> <span class="k">if</span> <span class="n">agent_id</span> <span class="o">==</span> <span class="s1">'player1'</span> <span class="k">else</span> <span class="s1">'beat_last'</span><span class="p">),</span>
        <span class="s1">'policies_to_train'</span><span class="p">:</span> <span class="p">[</span><span class="s1">'learned'</span><span class="p">],</span>
    <span class="p">},</span>
    <span class="s1">'framework'</span><span class="p">:</span> <span class="s1">'torch'</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># ray.shutdown()</span>
<span class="n">ray</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">get_trainer_class</span><span class="p">(</span><span class="s1">'PG'</span><span class="p">)(</span><span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>2021-02-15 07:33:57,811	INFO services.py:1193 -- View the Ray dashboard at <span class="ansi-green-intense-fg ansi-bold">http://127.0.0.1:8265</span>
2021-02-15 07:33:59,214	INFO trainer.py:650 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">list</span><span class="p">(</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">results</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>['episode_reward_max',
 'episode_reward_min',
 'episode_reward_mean',
 'episode_len_mean',
 'episodes_this_iter',
 'policy_reward_min',
 'policy_reward_max',
 'policy_reward_mean',
 'custom_metrics',
 'hist_stats',
 'sampler_perf',
 'off_policy_estimator',
 'num_healthy_workers',
 'timesteps_total',
 'timers',
 'info',
 'done',
 'episodes_total',
 'training_iteration',
 'experiment_id',
 'date',
 'timestamp',
 'time_this_iter_s',
 'time_total_s',
 'pid',
 'hostname',
 'node_ip',
 'config',
 'time_since_restore',
 'timesteps_since_restore',
 'iterations_since_restore',
 'perf']</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span>
          <span class="n">results</span><span class="p">[</span><span class="s1">'episode_reward_mean'</span><span class="p">],</span>
          <span class="n">results</span><span class="p">[</span><span class="s1">'policy_reward_mean'</span><span class="p">],</span>
          <span class="s1">'</span><span class="se">\t</span><span class="s1">'</span><span class="p">,</span> <span class="n">results</span><span class="p">[</span><span class="s1">'timesteps_total'</span><span class="p">],</span>
          <span class="s1">'</span><span class="se">\t</span><span class="s1">'</span><span class="p">,</span> <span class="n">results</span><span class="p">[</span><span class="s1">'episodes_total'</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>0 0.0 {'learned': 0.14, 'beat_last': -0.14} 	 1600 	 160
1 0.0 {'learned': 0.21, 'beat_last': -0.21} 	 2400 	 240
2 0.0 {'learned': 0.55, 'beat_last': -0.55} 	 3200 	 320
3 0.0 {'learned': 0.55, 'beat_last': -0.55} 	 4000 	 400
4 0.0 {'learned': -0.28, 'beat_last': 0.28} 	 4800 	 480
5 0.0 {'learned': 0.25, 'beat_last': -0.25} 	 5600 	 560
6 0.0 {'learned': 0.42, 'beat_last': -0.42} 	 6400 	 640
7 0.0 {'learned': 0.71, 'beat_last': -0.71} 	 7200 	 720
8 0.0 {'learned': 0.5, 'beat_last': -0.5} 	 8000 	 800
9 0.0 {'learned': 0.52, 'beat_last': -0.52} 	 8800 	 880
10 0.0 {'learned': 0.26, 'beat_last': -0.26} 	 9600 	 960
11 0.0 {'learned': 0.08, 'beat_last': -0.08} 	 10400 	 1040
12 0.0 {'learned': 1.15, 'beat_last': -1.15} 	 11200 	 1120
13 0.0 {'learned': 0.45, 'beat_last': -0.45} 	 12000 	 1200
14 0.0 {'learned': 0.37, 'beat_last': -0.37} 	 12800 	 1280
15 0.0 {'learned': 0.79, 'beat_last': -0.79} 	 13600 	 1360
16 0.0 {'learned': 0.96, 'beat_last': -0.96} 	 14400 	 1440
17 0.0 {'learned': 0.44, 'beat_last': -0.44} 	 15200 	 1520
18 0.0 {'learned': 1.05, 'beat_last': -1.05} 	 16000 	 1600
19 0.0 {'learned': 1.05, 'beat_last': -1.05} 	 16800 	 1680
20 0.0 {'learned': 1.03, 'beat_last': -1.03} 	 17600 	 1760
21 0.0 {'learned': 0.36, 'beat_last': -0.36} 	 18400 	 1840
22 0.0 {'learned': 1.15, 'beat_last': -1.15} 	 19200 	 1920
23 0.0 {'learned': 0.61, 'beat_last': -0.61} 	 20000 	 2000
24 0.0 {'learned': 0.91, 'beat_last': -0.91} 	 20800 	 2080
25 0.0 {'learned': 1.9, 'beat_last': -1.9} 	 21600 	 2160
26 0.0 {'learned': 2.53, 'beat_last': -2.53} 	 22400 	 2240
27 0.0 {'learned': 1.66, 'beat_last': -1.66} 	 23200 	 2320
28 0.0 {'learned': 2.06, 'beat_last': -2.06} 	 24000 	 2400
29 0.0 {'learned': 2.59, 'beat_last': -2.59} 	 24800 	 2480
30 0.0 {'learned': 3.11, 'beat_last': -3.11} 	 25600 	 2560
31 0.0 {'learned': 3.87, 'beat_last': -3.87} 	 26400 	 2640
32 0.0 {'learned': 3.56, 'beat_last': -3.56} 	 27200 	 2720
33 0.0 {'learned': 3.36, 'beat_last': -3.36} 	 28000 	 2800
34 0.0 {'learned': 4.24, 'beat_last': -4.24} 	 28800 	 2880
35 0.0 {'learned': 4.14, 'beat_last': -4.14} 	 29600 	 2960
36 0.0 {'learned': 4.63, 'beat_last': -4.63} 	 30400 	 3040
37 0.0 {'learned': 4.39, 'beat_last': -4.39} 	 31200 	 3120
38 0.0 {'learned': 4.36, 'beat_last': -4.36} 	 32000 	 3200
39 0.0 {'learned': 4.68, 'beat_last': -4.68} 	 32800 	 3280
40 0.0 {'learned': 4.81, 'beat_last': -4.81} 	 33600 	 3360
41 0.0 {'learned': 5.21, 'beat_last': -5.21} 	 34400 	 3440
42 0.0 {'learned': 5.1, 'beat_last': -5.1} 	 35200 	 3520
43 0.0 {'learned': 5.06, 'beat_last': -5.06} 	 36000 	 3600
44 0.0 {'learned': 5.17, 'beat_last': -5.17} 	 36800 	 3680
45 0.0 {'learned': 5.26, 'beat_last': -5.26} 	 37600 	 3760
46 0.0 {'learned': 5.28, 'beat_last': -5.28} 	 38400 	 3840
47 0.0 {'learned': 5.3, 'beat_last': -5.3} 	 39200 	 3920
48 0.0 {'learned': 5.35, 'beat_last': -5.35} 	 40000 	 4000
49 0.0 {'learned': 5.3, 'beat_last': -5.3} 	 40800 	 4080
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>학습이 진행되면서 learned 정책이 BeatLastHeuristic 정책을 쉽게 이기는 것을 알 수 있다.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="맺으며">
<a class="anchor" href="#%EB%A7%BA%EC%9C%BC%EB%A9%B0" aria-hidden="true"><span class="octicon octicon-link"></span></a>맺으며<a class="anchor-link" href="#%EB%A7%BA%EC%9C%BC%EB%A9%B0"> </a>
</h2>
<p>ray의 rllib은 다양한 강화학습 방식을 지원할뿐만 아니라 병렬 학습에 대한 처리가 매우 우수하다. 게다가 활발하게 코드가 관리되고 있다. 다만, 사용자 환경이나 모델을 정의할 때 생각대로 되지 않았던 기억이 남아있는데 향후의 블로그에서 관련 내용을 추가해 보겠다.</p>

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/jupyter-blog/rl/2021/02/15/rock-paper-scissors.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/jupyter-blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/jupyter-blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/jupyter-blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/jupyter-blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/jupyter-blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
