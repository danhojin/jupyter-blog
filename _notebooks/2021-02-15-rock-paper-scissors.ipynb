{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"가위바위보 강화학습\"\n",
    "> \"두 에이전트가 겨루는 가위바위보 환경에서 강화학습을 통하여 승리 정책을 학습한다.\"\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- author: 단호진\n",
    "- categories: [rl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가위바위보 환경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/danhojin/miniconda3/envs/ml8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.7.1+cu110'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.agents.pg import PGTorchPolicy, PGTrainer\n",
    "from ray.rllib.examples.env.rock_paper_scissors import RockPaperScissors\n",
    "\n",
    "torch, _ = try_import_torch()\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* player1과 player2가 취할 수 있는 action은 0, 1, 2이고, 관측치는 상대방의 action에서 나오는 0, 1, 2이다.\n",
    "* env.reset()은 첫 관측치 {'player1': 0, 'player2': 0}를 내어 준다.\n",
    "* env.step() 함수는 obs, rewards, done, info를 돌려준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = dict()\n",
    "env = RockPaperScissors(env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'player1': 0, 'player2': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Discrete(3), Discrete(3))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space, env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'player1': 2, 'player2': 1},\n",
       " {'player1': -1, 'player2': 1},\n",
       " {'__all__': False},\n",
       " {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(dict(player1=1, player2=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'player1': 1, 'player2': 2},\n",
       " {'player1': 1, 'player2': -1},\n",
       " {'__all__': False},\n",
       " {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(dict(player1=2, player2=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'player1': 2, 'player2': 0},\n",
       " {'player1': 1, 'player2': -1},\n",
       " {'__all__': False},\n",
       " {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(dict(player1=0, player2=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정책\n",
    "\n",
    "정책(Policy)은 에이전트가 관측된 환경, 이전 보상 이력을 참고하여 어떤 행동을 취하면 좋은지 결정한다. 에이전트가 둘인 가위바위보 게임에서 학습 가능한 learned 정책과 RandomMove, BeatLastHeuristic, AlwaysSameHeuristic 정책에서 하나를 뽑아 강확학습을 수행하였다. 가위바위보의 최고 전략은 RandomMove로 learned 정책은 이에 도달하는 결과를 보일 것이고, 그 밖에 BeatLastHeuristic, AlwaysSameHeuristic에 데해서는 쉽게 이길 수 있을 것이다.\n",
    "\n",
    "새로운 정책을 개발한다면 Policy 클래스를 상속하고 compute_actions에 필요한 로직을 구현하면 된다. RandomMove 정책은 obs_batch에 대해서만 고려하여 배치 크기만큼의 임의 행동을 돌려준다. kwargs에 필수 생성 인자 내용을 정리하였다. 환경의 action_space와 observation_space가 그것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.policy.view_requirement import ViewRequirement\n",
    "\n",
    "\n",
    "class RandomMove(Policy):\n",
    "    \"\"\"Pick a random move\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def random_action(self):\n",
    "        return random.choice([\n",
    "                RockPaperScissors.ROCK, RockPaperScissors.PAPER,\n",
    "                RockPaperScissors.SCISSORS,\n",
    "        ])\n",
    "    \n",
    "    def compute_actions(self,\n",
    "                        obs_batch,\n",
    "                        state_batches=None,\n",
    "                        prev_action_batch=None,\n",
    "                        prev_reward_batch=None,\n",
    "                        info_batch=None,\n",
    "                        episodes=None,\n",
    "                        **kwargs):\n",
    "        \"\"\"Returns:\n",
    "            Tuple:\n",
    "                actions: [BATCH_SIZE, ACTION_SHAPE]\n",
    "        \"\"\"\n",
    "        return [self.random_action() for _ in obs_batch], [], {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    'observation_space': env.observation_space,\n",
    "    'action_space': env.action_space,\n",
    "    'config': {}\n",
    "}\n",
    "rm = RandomMove(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 2, 2], [], {})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm.compute_actions(list(range(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 트레이너 설정 및 학습\n",
    "\n",
    "앞서 정의한 RandomMove외에 ray에서 제공하는 BeatLastHeuristic, AlwaysSameHeuristic 정책을 추가하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-15 07:33:57,811\tINFO services.py:1193 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2021-02-15 07:33:59,214\tINFO trainer.py:650 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from gym.spaces import Discrete\n",
    "from ray.rllib.agents.registry import get_trainer_class\n",
    "from ray.rllib.examples.policy.rock_paper_scissors_dummies import (\n",
    "    BeatLastHeuristic, AlwaysSameHeuristic\n",
    ")\n",
    "\n",
    "\n",
    "config = {\n",
    "    'env': RockPaperScissors,\n",
    "    'gamma': 0.9,\n",
    "    'num_gpus': 0,\n",
    "    'num_workers': 0,\n",
    "    'num_envs_per_worker': 4,\n",
    "    'train_batch_size': 200,  # for the policy model\n",
    "    'multiagent': {\n",
    "        'policies': {\n",
    "            'random_move': (RandomMove, Discrete(3), Discrete(3), {}),\n",
    "            'beat_last': (BeatLastHeuristic, Discrete(3), Discrete(3), {}),\n",
    "            'always_same': (AlwaysSameHeuristic, Discrete(3), Discrete(3), {}),\n",
    "            'learned': (None, Discrete(3), Discrete(3), {\n",
    "                'model': {},  # use default\n",
    "                'framework': 'torch'\n",
    "            })\n",
    "        },\n",
    "        'policy_mapping_fn': lambda agent_id: (\n",
    "            'learned' if agent_id == 'player1' else 'beat_last'),\n",
    "        'policies_to_train': ['learned'],\n",
    "    },\n",
    "    'framework': 'torch',\n",
    "}\n",
    "\n",
    "# ray.shutdown()\n",
    "ray.init()\n",
    "trainer = get_trainer_class('PG')(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1):\n",
    "    results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['episode_reward_max',\n",
       " 'episode_reward_min',\n",
       " 'episode_reward_mean',\n",
       " 'episode_len_mean',\n",
       " 'episodes_this_iter',\n",
       " 'policy_reward_min',\n",
       " 'policy_reward_max',\n",
       " 'policy_reward_mean',\n",
       " 'custom_metrics',\n",
       " 'hist_stats',\n",
       " 'sampler_perf',\n",
       " 'off_policy_estimator',\n",
       " 'num_healthy_workers',\n",
       " 'timesteps_total',\n",
       " 'timers',\n",
       " 'info',\n",
       " 'done',\n",
       " 'episodes_total',\n",
       " 'training_iteration',\n",
       " 'experiment_id',\n",
       " 'date',\n",
       " 'timestamp',\n",
       " 'time_this_iter_s',\n",
       " 'time_total_s',\n",
       " 'pid',\n",
       " 'hostname',\n",
       " 'node_ip',\n",
       " 'config',\n",
       " 'time_since_restore',\n",
       " 'timesteps_since_restore',\n",
       " 'iterations_since_restore',\n",
       " 'perf']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(k for k in results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0 {'learned': 0.14, 'beat_last': -0.14} \t 1600 \t 160\n",
      "1 0.0 {'learned': 0.21, 'beat_last': -0.21} \t 2400 \t 240\n",
      "2 0.0 {'learned': 0.55, 'beat_last': -0.55} \t 3200 \t 320\n",
      "3 0.0 {'learned': 0.55, 'beat_last': -0.55} \t 4000 \t 400\n",
      "4 0.0 {'learned': -0.28, 'beat_last': 0.28} \t 4800 \t 480\n",
      "5 0.0 {'learned': 0.25, 'beat_last': -0.25} \t 5600 \t 560\n",
      "6 0.0 {'learned': 0.42, 'beat_last': -0.42} \t 6400 \t 640\n",
      "7 0.0 {'learned': 0.71, 'beat_last': -0.71} \t 7200 \t 720\n",
      "8 0.0 {'learned': 0.5, 'beat_last': -0.5} \t 8000 \t 800\n",
      "9 0.0 {'learned': 0.52, 'beat_last': -0.52} \t 8800 \t 880\n",
      "10 0.0 {'learned': 0.26, 'beat_last': -0.26} \t 9600 \t 960\n",
      "11 0.0 {'learned': 0.08, 'beat_last': -0.08} \t 10400 \t 1040\n",
      "12 0.0 {'learned': 1.15, 'beat_last': -1.15} \t 11200 \t 1120\n",
      "13 0.0 {'learned': 0.45, 'beat_last': -0.45} \t 12000 \t 1200\n",
      "14 0.0 {'learned': 0.37, 'beat_last': -0.37} \t 12800 \t 1280\n",
      "15 0.0 {'learned': 0.79, 'beat_last': -0.79} \t 13600 \t 1360\n",
      "16 0.0 {'learned': 0.96, 'beat_last': -0.96} \t 14400 \t 1440\n",
      "17 0.0 {'learned': 0.44, 'beat_last': -0.44} \t 15200 \t 1520\n",
      "18 0.0 {'learned': 1.05, 'beat_last': -1.05} \t 16000 \t 1600\n",
      "19 0.0 {'learned': 1.05, 'beat_last': -1.05} \t 16800 \t 1680\n",
      "20 0.0 {'learned': 1.03, 'beat_last': -1.03} \t 17600 \t 1760\n",
      "21 0.0 {'learned': 0.36, 'beat_last': -0.36} \t 18400 \t 1840\n",
      "22 0.0 {'learned': 1.15, 'beat_last': -1.15} \t 19200 \t 1920\n",
      "23 0.0 {'learned': 0.61, 'beat_last': -0.61} \t 20000 \t 2000\n",
      "24 0.0 {'learned': 0.91, 'beat_last': -0.91} \t 20800 \t 2080\n",
      "25 0.0 {'learned': 1.9, 'beat_last': -1.9} \t 21600 \t 2160\n",
      "26 0.0 {'learned': 2.53, 'beat_last': -2.53} \t 22400 \t 2240\n",
      "27 0.0 {'learned': 1.66, 'beat_last': -1.66} \t 23200 \t 2320\n",
      "28 0.0 {'learned': 2.06, 'beat_last': -2.06} \t 24000 \t 2400\n",
      "29 0.0 {'learned': 2.59, 'beat_last': -2.59} \t 24800 \t 2480\n",
      "30 0.0 {'learned': 3.11, 'beat_last': -3.11} \t 25600 \t 2560\n",
      "31 0.0 {'learned': 3.87, 'beat_last': -3.87} \t 26400 \t 2640\n",
      "32 0.0 {'learned': 3.56, 'beat_last': -3.56} \t 27200 \t 2720\n",
      "33 0.0 {'learned': 3.36, 'beat_last': -3.36} \t 28000 \t 2800\n",
      "34 0.0 {'learned': 4.24, 'beat_last': -4.24} \t 28800 \t 2880\n",
      "35 0.0 {'learned': 4.14, 'beat_last': -4.14} \t 29600 \t 2960\n",
      "36 0.0 {'learned': 4.63, 'beat_last': -4.63} \t 30400 \t 3040\n",
      "37 0.0 {'learned': 4.39, 'beat_last': -4.39} \t 31200 \t 3120\n",
      "38 0.0 {'learned': 4.36, 'beat_last': -4.36} \t 32000 \t 3200\n",
      "39 0.0 {'learned': 4.68, 'beat_last': -4.68} \t 32800 \t 3280\n",
      "40 0.0 {'learned': 4.81, 'beat_last': -4.81} \t 33600 \t 3360\n",
      "41 0.0 {'learned': 5.21, 'beat_last': -5.21} \t 34400 \t 3440\n",
      "42 0.0 {'learned': 5.1, 'beat_last': -5.1} \t 35200 \t 3520\n",
      "43 0.0 {'learned': 5.06, 'beat_last': -5.06} \t 36000 \t 3600\n",
      "44 0.0 {'learned': 5.17, 'beat_last': -5.17} \t 36800 \t 3680\n",
      "45 0.0 {'learned': 5.26, 'beat_last': -5.26} \t 37600 \t 3760\n",
      "46 0.0 {'learned': 5.28, 'beat_last': -5.28} \t 38400 \t 3840\n",
      "47 0.0 {'learned': 5.3, 'beat_last': -5.3} \t 39200 \t 3920\n",
      "48 0.0 {'learned': 5.35, 'beat_last': -5.35} \t 40000 \t 4000\n",
      "49 0.0 {'learned': 5.3, 'beat_last': -5.3} \t 40800 \t 4080\n"
     ]
    }
   ],
   "source": [
    "for k in range(50):\n",
    "    results = trainer.train()\n",
    "    print(k,\n",
    "          results['episode_reward_mean'],\n",
    "          results['policy_reward_mean'],\n",
    "          '\\t', results['timesteps_total'],\n",
    "          '\\t', results['episodes_total'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습이 진행되면서 learned 정책이 BeatLastHeuristic 정책을 쉽게 이기는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 맺으며\n",
    "\n",
    "ray의 rllib은 다양한 강화학습 방식을 지원할뿐만 아니라 병렬 학습에 대한 처리가 매우 우수하다. 게다가 활발하게 코드가 관리되고 있다. 다만, 사용자 환경이나 모델을 정의할 때 생각대로 되지 않았던 기억이 남아있는데 향후의 블로그에서 관련 내용을 추가해 보겠다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
