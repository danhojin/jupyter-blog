{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"가위바위보 강화학습\"\n",
    "> \"두 에이전트가 겨루는 가위바위보 환경에서 강화학습을 통하여 승리 정책을 학습한다.\"\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- author: 단호진\n",
    "- categories: [rl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 갱신: 2021-07-04\n",
    "\n",
    "ray 패키지의 rllib은 사용자 모델을 만들어 쓸 수 있게 설계되어 있다. 하지만 세부적인 사항을 이해하고 사용자 모델을 쓰기가 쉽지 않다. 여기에 내부 코드의 이해를 위하여 몇 가지 코드를 작성하여 시험해 본다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.9.0+cu102', '2.0.0.dev0')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.agents.pg import PGTorchPolicy, PGTrainer\n",
    "from ray.rllib.examples.env.rock_paper_scissors import RockPaperScissors\n",
    "\n",
    "torch, nn = try_import_torch()\n",
    "torch.__version__, ray.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가위바위보 환경\n",
    "\n",
    "* player1과 player2가 취할 수 있는 action은 0, 1, 2이고, 관측치는 상대방의 action에서 나오는 0, 1, 2이다.\n",
    "* env.reset()은 첫 관측치 {'player1': 0, 'player2': 0}를 내어 준다.\n",
    "* env.step() 함수는 obs, rewards, done, info를 돌려준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = dict()\n",
    "env = RockPaperScissors(env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'player1': 0, 'player2': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Discrete(3), Discrete(3))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space, env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, reward, done, info = env.step(dict(player1=1, player2=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs   : {'player1': 2, 'player2': 1}\n",
      "reward: {'player1': -1, 'player2': 1}\n",
      "done  : {'__all__': False}\n",
      "info  : {}\n"
     ]
    }
   ],
   "source": [
    "print(f'obs   : {obs}')\n",
    "print(f'reward: {reward}')\n",
    "print(f'done  : {done}')\n",
    "print(f'info  : {info}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, reward, done, info = env.step(dict(player1=1, player2=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs   : {'player1': 1, 'player2': 1}\n",
      "reward: {'player1': 0, 'player2': 0}\n",
      "done  : {'__all__': False}\n",
      "info  : {}\n"
     ]
    }
   ],
   "source": [
    "print(f'obs   : {obs}')\n",
    "print(f'reward: {reward}')\n",
    "print(f'done  : {done}')\n",
    "print(f'info  : {info}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정책\n",
    "\n",
    "정책(Policy)은 에이전트가 관측된 환경, 이전 보상 이력을 참고하여 어떤 행동을 취하면 좋은지 결정한다. 에이전트가 둘인 가위바위보 게임에서 학습 가능한 learned 정책과 RandomMove, BeatLastHeuristic, AlwaysSameHeuristic 정책에서 하나를 뽑아 강확학습을 수행하였다. 가위바위보의 최고 전략은 RandomMove로 learned 정책은 이에 도달하는 결과를 보일 것이고, 그 밖에 BeatLastHeuristic, AlwaysSameHeuristic에 데해서는 쉽게 이길 수 있을 것이다.\n",
    "\n",
    "새로운 정책을 개발한다면 Policy 클래스를 상속하고 compute_actions에 필요한 로직을 구현하면 된다. RandomMove 정책은 obs_batch에 대해서만 고려하여 배치 크기만큼의 임의 행동을 돌려준다. kwargs에 필수 생성 인자 내용을 정리하였다. 환경의 action_space와 observation_space가 그것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from ray.rllib.policy.policy import Policy\n",
    "# from ray.rllib.policy.view_requirement import ViewRequirement\n",
    "\n",
    "\n",
    "class RandomMove(Policy):\n",
    "    \"\"\"Pick a random move\"\"\"\n",
    "    def __init__(self, obs_space, act_space, config):\n",
    "        super().__init__(obs_space, act_space, config)\n",
    "        \n",
    "    def random_action(self):\n",
    "        return random.choice([\n",
    "            RockPaperScissors.ROCK,\n",
    "            RockPaperScissors.PAPER,\n",
    "            RockPaperScissors.SCISSORS,\n",
    "        ])\n",
    "    \n",
    "    def compute_actions(\n",
    "        self,\n",
    "        obs_batch,\n",
    "        state_batches=None,\n",
    "        prev_action_batch=None,\n",
    "        prev_reward_batch=None,\n",
    "        info_batch=None,\n",
    "        episodes=None,\n",
    "        **kwargs):\n",
    "        \"\"\"Returns:\n",
    "            Tuple:\n",
    "                actions: [BATCH_SIZE, ACTION_SHAPE]\n",
    "        \"\"\"\n",
    "        return [self.random_action() for _ in obs_batch], [], {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 2], [], {})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_policy = RandomMove(env.observation_space, env.action_space, {})\n",
    "random_policy.compute_actions(list(range(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom policy with template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ray-project/ray/blob/master/rllib/examples/custom_torch_policy.py\n",
    "\n",
    "from ray.rllib.policy.torch_policy_template import build_policy_class\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "\n",
    "def policy_gradient_loss(policy, model, dist_class, train_batch):\n",
    "    logits, _ = model.from_batch(train_batch)\n",
    "    action_dist = dist_class(logits)\n",
    "    log_probs = action_dist.logp(train_batch[SampleBatch.ACTIONS])\n",
    "#     print(train_batch[SampleBatch.REWARDS].dtype)\n",
    "#     print(log_probs.dtype)\n",
    "    return -train_batch[SampleBatch.REWARDS].float().dot(log_probs)\n",
    "\n",
    "MyTorchPolicy = build_policy_class(\n",
    "    framework='torch', name='MyTorchPolicy', loss_fn=policy_gradient_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom loss model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ray-project/ray/blob/master/rllib/examples/models/custom_loss_model.py\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "\n",
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_action_dist import TorchCategorical\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.framework import TensorType\n",
    "\n",
    "\n",
    "class TorchCustomLossModel(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        \n",
    "        print(f'model config: {model_config}')\n",
    "        \n",
    "        self.fcnet = FullyConnectedNetwork(\n",
    "            self.obs_space,\n",
    "            self.action_space,\n",
    "            num_outputs,\n",
    "            model_config,\n",
    "            name=\"fcnet\"\n",
    "        )\n",
    "        \n",
    "    @override(ModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        return self.fcnet(input_dict, state, seq_lens)\n",
    "    \n",
    "    @override(ModelV2)\n",
    "    def value_function(self):\n",
    "        return self.fcnet.value_function()\n",
    "    \n",
    "    @override(ModelV2)\n",
    "    def custom_loss(self,\n",
    "                    policy_loss: TensorType,\n",
    "                    loss_inputs: Dict[str, TensorType]) -> TensorType:\n",
    "        logits, _ = self.forward({'obs': loss_inputs['obs']}, [], None)\n",
    "        action_dist = TorchCategorical(logits, self.model_config)\n",
    "        imitation_loss = torch.mean(\n",
    "            -action_dist.logp(loss_inputs['actions'].to(policy_loss[0].device))\n",
    "        )\n",
    "        self.imitation_loss_metric = imitation_loss.item()\n",
    "        self.policy_loss_metric = np.mean([\n",
    "            loss.item() for loss in policy_loss\n",
    "        ])\n",
    "        \n",
    "        return [loss_ + 10 * imitation_loss for loss_ in policy_loss]\n",
    "    \n",
    "    def metrics(self):\n",
    "        return {\n",
    "            'policy_loss': self.policy_loss_metric,\n",
    "            'imitation_loss': self.imitation_loss_metric,\n",
    "        }\n",
    "    \n",
    "    \n",
    "ModelCatalog.register_custom_model('my_torch_model', TorchCustomLossModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 트레이너 설정 및 학습\n",
    "\n",
    "앞서 정의한 RandomMove외에 ray에서 제공하는 BeatLastHeuristic, AlwaysSameHeuristic 정책을 추가하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-04 11:54:03,897\tINFO services.py:1330 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2021-07-04 11:54:04,893\tINFO trainer.py:714 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model config: {'_use_default_native_models': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'num_framestacks': 0, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': 'my_torch_model', 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, 'framestack': True}\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from gym.spaces import Discrete\n",
    "from ray.rllib.agents.registry import get_trainer_class\n",
    "from ray.rllib.examples.policy.rock_paper_scissors_dummies import (\n",
    "    BeatLastHeuristic, AlwaysSameHeuristic\n",
    ")\n",
    "\n",
    "\n",
    "config = {\n",
    "    'env': RockPaperScissors,\n",
    "    'gamma': 0.9,\n",
    "    'num_gpus': 0,\n",
    "    'num_workers': 0,\n",
    "    'num_envs_per_worker': 4,\n",
    "    'train_batch_size': 200,  # for the policy model\n",
    "    'multiagent': {\n",
    "        'policies': {\n",
    "            'random_move': (RandomMove, Discrete(3), Discrete(3), {}),\n",
    "            'beat_last': (BeatLastHeuristic, Discrete(3), Discrete(3), {}),\n",
    "            'always_same': (AlwaysSameHeuristic, Discrete(3), Discrete(3), {}),\n",
    "            'learned0': (None, Discrete(3), Discrete(3), {\n",
    "                'framework': 'torch',\n",
    "                'model': {},  # use default\n",
    "            }),\n",
    "            'learned1': (MyTorchPolicy, Discrete(3), Discrete(3), {\n",
    "                'framework': 'torch',\n",
    "                'model': {},  # use default\n",
    "            }),\n",
    "            'learned2': (None, Discrete(3), Discrete(3), {\n",
    "                'framework': 'torch',\n",
    "                'model': {\n",
    "                    'custom_model': 'my_torch_model',\n",
    "                    'custom_model_config': {},\n",
    "                },  # use default\n",
    "            }),\n",
    "        },\n",
    "        'policy_mapping_fn': lambda agent_id, episode, **kwargs: (\n",
    "            'learned2' if agent_id == 'player1' else 'beat_last'),\n",
    "        'policies_to_train': ['learned0', 'learned1', 'learned2'],\n",
    "    },\n",
    "    'framework': 'torch',\n",
    "}\n",
    "\n",
    "# ray.shutdown()\n",
    "ray.init()\n",
    "trainer = get_trainer_class('PG')(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_reward_max: 0.0\n",
      "episode_reward_min: 0.0\n",
      "episode_reward_mean: 0.0\n",
      "episode_len_mean: 10.0\n",
      "episode_media: {}\n",
      "episodes_this_iter: 80\n",
      "policy_reward_min: {'learned2': -6.0, 'beat_last': -6.0}\n",
      "policy_reward_max: {'learned2': 6.0, 'beat_last': 6.0}\n",
      "policy_reward_mean: {'learned2': 0.0625, 'beat_last': -0.0625}\n",
      "custom_metrics: {}\n",
      "hist_stats: {'episode_reward': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'episode_lengths': [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10], 'policy_learned2_reward': [-1.0, -3.0, 0.0, 0.0, -3.0, 3.0, -5.0, 2.0, 3.0, 2.0, 0.0, 0.0, -4.0, -5.0, 4.0, -4.0, -1.0, 2.0, 3.0, 3.0, 0.0, -3.0, 1.0, -3.0, 2.0, 5.0, 3.0, 2.0, 1.0, 0.0, 3.0, -1.0, 0.0, 1.0, 5.0, -1.0, 0.0, -1.0, 0.0, 0.0, 4.0, 0.0, -2.0, 2.0, -1.0, -1.0, -2.0, 2.0, 0.0, -5.0, -3.0, 3.0, 2.0, 0.0, 0.0, 1.0, -1.0, 0.0, 0.0, -2.0, 1.0, 2.0, 0.0, 3.0, 2.0, -1.0, -6.0, 2.0, 6.0, 0.0, -2.0, 0.0, -3.0, -2.0, 0.0, -1.0, 2.0, -2.0, -5.0, 2.0], 'policy_beat_last_reward': [1.0, 3.0, 0.0, 0.0, 3.0, -3.0, 5.0, -2.0, -3.0, -2.0, 0.0, 0.0, 4.0, 5.0, -4.0, 4.0, 1.0, -2.0, -3.0, -3.0, 0.0, 3.0, -1.0, 3.0, -2.0, -5.0, -3.0, -2.0, -1.0, 0.0, -3.0, 1.0, 0.0, -1.0, -5.0, 1.0, 0.0, 1.0, 0.0, 0.0, -4.0, 0.0, 2.0, -2.0, 1.0, 1.0, 2.0, -2.0, 0.0, 5.0, 3.0, -3.0, -2.0, 0.0, 0.0, -1.0, 1.0, 0.0, 0.0, 2.0, -1.0, -2.0, 0.0, -3.0, -2.0, 1.0, 6.0, -2.0, -6.0, 0.0, 2.0, 0.0, 3.0, 2.0, 0.0, 1.0, -2.0, 2.0, 5.0, -2.0]}\n",
      "sampler_perf: {'mean_raw_obs_processing_ms': 0.29636734160617806, 'mean_inference_ms': 1.2038987667406376, 'mean_action_processing_ms': 0.10593850814287933, 'mean_env_wait_ms': 0.04650111222148534, 'mean_env_render_ms': 0.0}\n",
      "off_policy_estimator: {}\n",
      "num_healthy_workers: 0\n",
      "timesteps_total: 800\n",
      "agent_timesteps_total: 1600\n",
      "timers: {'sample_time_ms': 335.555, 'sample_throughput': 2384.111, 'learn_time_ms': 37.361, 'learn_throughput': 21412.483}\n",
      "info: {'learner': {'learned2': {'learner_stats': {'allreduce_latency': 0.0, 'policy_loss': -0.09941699355840683}, 'model': {'policy_loss': -0.09941699355840683, 'imitation_loss': 1.098800539970398}, 'custom_metrics': {}}}, 'num_steps_sampled': 800, 'num_agent_steps_sampled': 1600, 'num_steps_trained': 800, 'num_agent_steps_trained': 1600}\n",
      "done: False\n",
      "episodes_total: 80\n",
      "training_iteration: 1\n",
      "experiment_id: ff7101d03aa04f819ae881e80c7c7916\n",
      "date: 2021-07-04_11-54-05\n",
      "timestamp: 1625367245\n",
      "time_this_iter_s: 0.3737308979034424\n",
      "time_total_s: 0.3737308979034424\n",
      "pid: 70134\n",
      "hostname: omen\n",
      "node_ip: 192.168.0.10\n",
      "config: {'num_workers': 0, 'num_envs_per_worker': 4, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.9, 'lr': 0.0004, 'train_batch_size': 200, 'model': {'_use_default_native_models': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'num_framestacks': 'auto', 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, 'framestack': True}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'RockPaperScissors', 'observation_space': None, 'action_space': None, 'env_config': {}, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'torch', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'random_move': (<class '__main__.RandomMove'>, Discrete(3), Discrete(3), {}), 'beat_last': (<class 'ray.rllib.examples.policy.rock_paper_scissors_dummies.BeatLastHeuristic'>, Discrete(3), Discrete(3), {}), 'always_same': (<class 'ray.rllib.examples.policy.rock_paper_scissors_dummies.AlwaysSameHeuristic'>, Discrete(3), Discrete(3), {}), 'learned0': (None, Discrete(3), Discrete(3), {'framework': 'torch', 'model': {}}), 'learned1': (<class 'ray.rllib.policy.policy_template.MyTorchPolicy'>, Discrete(3), Discrete(3), {'framework': 'torch', 'model': {}}), 'learned2': (None, Discrete(3), Discrete(3), {'framework': 'torch', 'model': {'custom_model': 'my_torch_model', 'custom_model_config': {}}})}, 'policy_mapping_fn': <function <lambda> at 0x7fb213954310>, 'policies_to_train': ['learned0', 'learned1', 'learned2'], 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, 'simple_optimizer': True, 'monitor': -1}\n",
      "time_since_restore: 0.3737308979034424\n",
      "timesteps_since_restore: 0\n",
      "iterations_since_restore: 1\n",
      "perf: {'cpu_util_percent': 31.4, 'ram_util_percent': 9.2, 'gpu_util_percent0': 0.27, 'vram_util_percent0': 0.08259507829977629}\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1):\n",
    "    results = trainer.train()\n",
    "    \n",
    "for k, v in results.items():\n",
    "    print(f'{k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.0 {'learned2': -0.03, 'beat_last': 0.03} \t 8800 \t 880\n",
      "20 0.0 {'learned2': -0.02, 'beat_last': 0.02} \t 16800 \t 1680\n",
      "30 0.0 {'learned2': 0.1, 'beat_last': -0.1} \t 24800 \t 2480\n",
      "40 0.0 {'learned2': 0.0, 'beat_last': 0.0} \t 32800 \t 3280\n",
      "50 0.0 {'learned2': 0.08, 'beat_last': -0.08} \t 40800 \t 4080\n",
      "60 0.0 {'learned2': -0.06, 'beat_last': 0.06} \t 48800 \t 4880\n",
      "70 0.0 {'learned2': 0.02, 'beat_last': -0.02} \t 56800 \t 5680\n",
      "80 0.0 {'learned2': 0.39, 'beat_last': -0.39} \t 64800 \t 6480\n",
      "90 0.0 {'learned2': 0.8, 'beat_last': -0.8} \t 72800 \t 7280\n",
      "100 0.0 {'learned2': 1.31, 'beat_last': -1.31} \t 80800 \t 8080\n",
      "110 0.0 {'learned2': 1.74, 'beat_last': -1.74} \t 88800 \t 8880\n",
      "120 0.0 {'learned2': 2.74, 'beat_last': -2.74} \t 96800 \t 9680\n",
      "130 0.0 {'learned2': 4.71, 'beat_last': -4.71} \t 104800 \t 10480\n",
      "140 0.0 {'learned2': 5.12, 'beat_last': -5.12} \t 112800 \t 11280\n",
      "150 0.0 {'learned2': 5.38, 'beat_last': -5.38} \t 120800 \t 12080\n",
      "160 0.0 {'learned2': 5.67, 'beat_last': -5.67} \t 128800 \t 12880\n",
      "170 0.0 {'learned2': 5.5, 'beat_last': -5.5} \t 136800 \t 13680\n",
      "180 0.0 {'learned2': 5.82, 'beat_last': -5.82} \t 144800 \t 14480\n",
      "190 0.0 {'learned2': 5.89, 'beat_last': -5.89} \t 152800 \t 15280\n",
      "200 0.0 {'learned2': 5.78, 'beat_last': -5.78} \t 160800 \t 16080\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 201):\n",
    "    results = trainer.train()\n",
    "    if k % 10 == 0:\n",
    "        print(k,\n",
    "              results['episode_reward_mean'],\n",
    "              results['policy_reward_mean'],\n",
    "              '\\t', results['timesteps_total'],\n",
    "              '\\t', results['episodes_total'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습이 진행되면서 learned 정책이 BeatLastHeuristic 정책을 쉽게 이기는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 맺으며\n",
    "\n",
    "ray의 rllib은 다양한 강화학습 방식을 지원할뿐만 아니라 병렬 학습에 대한 처리가 매우 우수하다. 게다가 활발하게 코드가 관리되고 있다. 다만, 사용자 환경이나 모델을 잘 정의하여 활용하기 위해선 이론적 배경과 rllib의 내부 호출 구조를 잘 알아야 한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
