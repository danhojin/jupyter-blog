{
  
    
        "post0": {
            "title": "네이버 영화 리뷰의 감성 분석",
            "content": "한글로 표기된 한국어는 어형변화가 많고 조사가 많은 역할을 한다. 조사로 말미암아 어순도 유연하게 구사할 수 있을뿐만 아니라 띄어쓰기가 내용을 전달하는데 결정적인 역할을 하지도 않는다. 이런 특징은 문장의 토큰화에 어려움을 가중시킨다. 게다가 최근 한국어 문서에서 알파벳 표기가 같이 들어오는 경우도 많은데 이를 처리하는 것도 또 다른 숙제이다. 그럼에도 다른 언어를 위하여 개발된 자연어 처리를 위한 기계학습과 심층학습의 원리는 한국어에도 마찬가지로 적용할 수 있으며, 한국어 처리를 위한 원리는 보편적으로도 유용하게 쓰일 수 있을 것이다. . 이 포스트에서는 카카오에서 공개한 khaiii 형태소 분석기를 이용하여 네이버 영화 리뷰 말뭉치에 대한 감성 분석을 진행해 보겠다. . 참고 . 전창욱, 최태균, 조중현, 신성진, 텐서플로2와 머신러닝으로 시작하는 자연어 처리, 위키북스, 2020 | 프랑소와 숄레, 박해선 옮김, 케라스 창시자에게 배우는 딥러닝 | &#45936;&#51060;&#53552; . 전체 영화 리뷰 데이터에서 형태소 분석을 진행한 후 훈련셋과 검정셋으로 나누어 진행하겠다. 20만 건의 리뷰가 있으며 긍정과 부정 레이블이 들어있다. . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline df = pd.read_csv(&#39;https://github.com/e9t/nsmc/raw/master/ratings.txt&#39;, delimiter=&#39; t&#39;) print(f&#39;The shape of dataframe: {df.shape}&#39;) df.head() . The shape of dataframe: (200000, 3) . id document label . 0 8112052 | 어릴때보고 지금다시봐도 재밌어요ㅋㅋ | 1 | . 1 8132799 | 디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산... | 1 | . 2 4655635 | 폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고. | 1 | . 3 9251303 | 와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런... | 1 | . 4 10067386 | 안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화. | 1 | . fig, ax = plt.subplots(figsize=(10, 8)) median = df[&#39;document&#39;].astype(str).apply(len).median() ax = sns.histplot(df[&#39;document&#39;].astype(str).apply(len), bins=30, ax=ax); ax.axvline(x=median, linewidth=2.5, color=&#39;blue&#39;) ax.text(median - 20, -2000, f&#39;median={median}&#39;, color=&#39;blue&#39;, fontsize=15); . 리뷰 길이는 최대 140자로 제한되어 있으며 중앙값이 27로 짧은 리뷰이다. . &#54805;&#53468;&#49548; &#48516;&#49437; . 형태소 분석에 khaiii를 이용한다. 가장 간단하게 분석을 진행하기 위하여 알파벳이나 숫자, 기타 기호 등을 제거해 보면, 빈 리뷰가 생기는데 데이터 셋에서 이를 제거하였다. document_1 열에 정리된 리뷰가 저장된다. . import re p = re.compile(r&#39;^[ s]*$&#39;) df = df.dropna() df[&#39;document_1&#39;] = (df[&#39;document&#39;].copy() .map(lambda el: re.sub(r&quot;[^가-힣ㅏ-ㅣ s]&quot;, &quot; &quot;, el)) .map(lambda el: re.sub(r&#39;[ s]+&#39;, &quot; &quot;, el)) ) df[&#39;document_2&#39;] = df[&#39;document_1&#39;].copy().map( lambda el: True if p.search(el) == None else False ) df = df[df[&#39;document_2&#39;]].drop(columns=[&#39;document_2&#39;]) print(f&#39;The shape of dataframe: {df.shape}&#39;) df.head() . The shape of dataframe: (197900, 4) . id document label document_1 . 0 8112052 | 어릴때보고 지금다시봐도 재밌어요ㅋㅋ | 1 | 어릴때보고 지금다시봐도 재밌어요 | . 1 8132799 | 디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산... | 1 | 디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업... | . 2 4655635 | 폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고. | 1 | 폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고 | . 3 9251303 | 와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런... | 1 | 와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지 | . 4 10067386 | 안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화. | 1 | 안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화 | . 2100개의 리뷰가 제거되었다. 형태소 분석을 위한 함수를 작성하고 리뷰에 적용하여 morphemes 열에 저장하였다. . from khaiii import KhaiiiApi api = KhaiiiApi() def get_morphemes(review): morphemes = [w for w in api.analyze(review)] morphemes = [m.lex for w in morphemes for m in w.morphs] return list(morphemes) df[&#39;morphemes&#39;] = df[&#39;document_1&#39;].apply(get_morphemes) df.head() . id document label document_1 morphemes . 0 8112052 | 어릴때보고 지금다시봐도 재밌어요ㅋㅋ | 1 | 어릴때보고 지금다시봐도 재밌어요 | [어리, ㄹ, 때, 보, 고, 지금다시, 보, 아도, 재밌, 어요] | . 1 8132799 | 디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산... | 1 | 디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업... | [디자인, 을, 배우, 는, 학생, 으로, 외국, 디자이, 너, 와, 그, 들, 이... | . 2 4655635 | 폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고. | 1 | 폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고 | [폴리스스토리, 시리즈, 는, 부, 터, 뉴, 까지, 버리, ㄹ께, 하나, 도, 없... | . 3 9251303 | 와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런... | 1 | 와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지 | [오, 아, 연기, 가, 진, 짜, 개, 쩔, 구나, 지루, 하, ㄹ, 거, 이, ... | . 4 10067386 | 안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화. | 1 | 안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화 | [안개, 자욱, 하, ㄴ, 밤하늘, 에, 뜨, 어, 있, 는, 초승달, 같, 은, 영화] | . morphemes = [m for r in df[&#39;morphemes&#39;] for m in r] morphemes = list(morphemes) print(f&#39;The length of morphemes: {len(morphemes)}&#39;) morphemes[:5] . The length of morphemes: 3530533 . [&#39;어리&#39;, &#39;ㄹ&#39;, &#39;때&#39;, &#39;보&#39;, &#39;고&#39;] . 약 20만개의 리뷰가 350만개의 형태소로 분석되었으니, 평균적으로 각 리뷰는 형태소 18개로 구성된다. 리뷰 길이의 중앙값은 27이었으므로 상당 수의 형태소는 길이 1 또는 2인 것을 알 수 있다. 형태소에 대하여 더 분석해 보자. . from collections import Counter morphemes_freq = Counter(morphemes) morphemes_freq = list(zip(*[(k, v) for (k, v) in morphemes_freq.items()])) morphemes_freq = pd.DataFrame({&#39;morpheme&#39;: morphemes_freq[0], &#39;n&#39;: morphemes_freq[1]}) morphemes_freq = morphemes_freq.sort_values(by=&#39;n&#39;, ascending=False) morphemes_freq = morphemes_freq.reset_index(drop=True) morphemes_freq = morphemes_freq.reset_index() morphemes_freq[&#39;index&#39;] = morphemes_freq[&#39;index&#39;] + 1 morphemes_freq.head() . index morpheme n . 0 1 | 이 | 154942 | . 1 2 | 하 | 139855 | . 2 3 | ㄴ | 99839 | . 3 4 | 는 | 89213 | . 4 5 | 다 | 75460 | . morphemes_freq.morpheme[:100].values . array([&#39;이&#39;, &#39;하&#39;, &#39;ㄴ&#39;, &#39;는&#39;, &#39;다&#39;, &#39;영화&#39;, &#39;고&#39;, &#39;보&#39;, &#39;의&#39;, &#39;가&#39;, &#39;도&#39;, &#39;에&#39;, &#39;은&#39;, &#39;을&#39;, &#39;었&#39;, &#39;지&#39;, &#39;ㄹ&#39;, &#39;어&#39;, &#39;게&#39;, &#39;들&#39;, &#39;았&#39;, &#39;나&#39;, &#39;있&#39;, &#39;것&#39;, &#39;를&#39;, &#39;없&#39;, &#39;아&#39;, &#39;되&#39;, &#39;만&#39;, &#39;좋&#39;, &#39;는데&#39;, &#39;기&#39;, &#39;로&#39;, &#39;적&#39;, &#39;주&#39;, &#39;너무&#39;, &#39;였&#39;, &#39;여&#39;, &#39;으로&#39;, &#39;음&#39;, &#39;정말&#39;, &#39;네&#39;, &#39;어요&#39;, &#39;같&#39;, &#39;지만&#39;, &#39;ㄴ다&#39;, &#39;ㅁ&#39;, &#39;에서&#39;, &#39;않&#39;, &#39;안&#39;, &#39;수&#39;, &#39;말&#39;, &#39;면&#39;, &#39;아니&#39;, &#39;과&#39;, &#39;점&#39;, &#39;거&#39;, &#39;네요&#39;, &#39;시&#39;, &#39;만들&#39;, &#39;그&#39;, &#39;뭐&#39;, &#39;연기&#39;, &#39;재미있&#39;, &#39;평점&#39;, &#39;던&#39;, &#39;진짜&#39;, &#39;재밌&#39;, &#39;나오&#39;, &#39;잘&#39;, &#39;이런&#39;, &#39;겠&#39;, &#39;라&#39;, &#39;ㅠ&#39;, &#39;습니다&#39;, &#39;듯&#39;, &#39;ㅂ니다&#39;, &#39;이것&#39;, &#39;생각&#39;, &#39;싶&#39;, &#39;왜&#39;, &#39;와&#39;, &#39;최고&#39;, &#39;요&#39;, &#39;더&#39;, &#39;내&#39;, &#39;어서&#39;, &#39;스토리&#39;, &#39;아서&#39;, &#39;사람&#39;, &#39;까지&#39;, &#39;감동&#39;, &#39;오&#39;, &#39;한&#39;, &#39;보다&#39;, &#39;재미&#39;, &#39;ㅡ&#39;, &#39;배우&#39;, &#39;때&#39;, &#39;지루&#39;], dtype=object) . 영화 리뷰인 만큼 영화라는 낱말이 많이 사용되고 있어 사실상 의미가 없는 불용어로 처리해도 되겠다. 기분을 나타내는 재미는 재미, 재밌, 재미있 등의 별개의 형태소 khaiii가 분석한 것을 알 수 있다. 과도하게 많이 사용한 형태소를 찾아 보자. . # fig = px.line(morph_freq.iloc[:200], x=&#39;index&#39;, y=&#39;n&#39;, log_y=True) # fig.show() fig, ax = plt.subplots(figsize=(10, 8)) # ax.set(yscale=&#39;log&#39;) sns.lineplot(x=&#39;index&#39;, y=&#39;n&#39;, data=morphemes_freq.iloc[:200], ax=ax); . 기울기가 급격하게 변하는 15000회 이상 사용된 다음 형태소는 불용어로 지정하겠다. . stop_words = morphemes_freq.loc[morphemes_freq.n &gt; 15000, &#39;morpheme&#39;].values stop_words = set(stop_words) &#39; &#39;.join(stop_words) . &#39;의 다 하 만 를 아 어 고 보 은 있 을 좋 았 는 도 게 영화 들 가 이 었 없 나 는데 되 지 ㄴ ㄹ 에 것&#39; . morphemes_freq[&#39;morpheme&#39;].apply(len).value_counts() . 3 35970 2 32527 4 19306 5 7683 6 2387 1 2272 7 848 8 341 9 139 10 61 11 31 12 18 13 7 14 4 17 4 16 3 15 3 21 3 24 3 18 3 28 2 30 2 22 2 31 2 129 1 36 1 35 1 39 1 133 1 29 1 26 1 23 1 20 1 47 1 Name: morpheme, dtype: int64 . 대부분의 형태소는 2, 3, 4글자인 것을 알 수 있다. 100자가 넘는 형태소도 존재하는데 다음과 같은 것이며 토큰화를 거치며 제거될 것이다. . morphemes_freq[morphemes_freq[&#39;morpheme&#39;].apply(len) &gt; 100] . index morpheme n . 40688 40689 | ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ... | 1 | . 93706 93707 | 의리의리의리의리의리의리의리의리의리의리의리의리의리의리의리의리의리의리의리의리의리의리의리... | 1 | . 수동으로 불용어를 형태소 사전에서 제거하겠다. . morphemes = [m for m in morphemes if m not in stop_words] morphemes = [(k, v) for (v, k) in enumerate(morphemes)] morphemes = dict(morphemes) len(morphemes) . 101600 . 이렇게 얻어진 10만개의 형태소를 이용하여 감성 분석을 진행해 보자. . &#53664;&#53360;&#54868;&#50752; &#54984;&#47144;&#183;&#44160;&#51221;&#49483; &#51456;&#48708; . 우선 불용어를 제거하고, 이 과정에서 길이가 0이된 리뷰를 제거한다. . df[&#39;morphemes_2&#39;] = df[&#39;morphemes&#39;].map(lambda el: [m for m in el if m not in stop_words]) df[&#39;morphemes_2_len&#39;] = df[&#39;morphemes_2&#39;].apply(len) df = df[df[&#39;morphemes_2_len&#39;] &gt; 0] df.shape . (197640, 7) . df = (df.drop(columns=[&#39;morphemes&#39;, &#39;morphemes_2_len&#39;]) .rename(columns={&#39;morphemes_2&#39;: &#39;morphemes&#39;})) df.head() . id document label document_1 morphemes . 0 8112052 | 어릴때보고 지금다시봐도 재밌어요ㅋㅋ | 1 | 어릴때보고 지금다시봐도 재밌어요 | [어리, 때, 지금다시, 아도, 재밌, 어요] | . 1 8132799 | 디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산... | 1 | 디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업... | [디자인, 배우, 학생, 으로, 외국, 디자이, 너, 와, 그, 일, 군, 전통, ... | . 2 4655635 | 폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고. | 1 | 폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고 | [폴리스스토리, 시리즈, 부, 터, 뉴, 까지, 버리, ㄹ께, 하나, 음, 최고] | . 3 9251303 | 와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런... | 1 | 와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지 | [오, 연기, 진, 짜, 개, 쩔, 구나, 지루, 거, 라고, 생각, 였, 몰입, ... | . 4 10067386 | 안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화. | 1 | 안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화 | [안개, 자욱, 밤하늘, 뜨, 초승달, 같] | . fig, ax = plt.subplots(figsize=(10, 8)) sns.histplot(x=df[&#39;morphemes&#39;].apply(len), bins=30); . 리뷰의 형태소 분포는 위와 같다. 리뷰의 길이는 가변적이므로 동일한 길이의 입력으로 변환하겠다. 길이 40 이상은 절단하고 부족한 리뷰에 대해서는 패팅을 추가한다. 이를 위해서 keras의 pad_sequence 함수를 이용할 것이다. . from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences . maxlen = 40 # The number of morphemes per review max_words = 80000 # 80% morphemes tokenizer = Tokenizer(num_words=max_words) tokenizer.fit_on_texts(df.morphemes) sequences = tokenizer.texts_to_sequences(df.morphemes) len(tokenizer.word_index) . 101600 . wi = [(k, v) for (k, v) in tokenizer.word_index.items()] wi = [kv for (_, kv) in zip(range(5), wi)] wi = list(wi) wi . [(&#39;기&#39;, 1), (&#39;로&#39;, 2), (&#39;적&#39;, 3), (&#39;주&#39;, 4), (&#39;너무&#39;, 5)] . data = pad_sequences(sequences, maxlen=maxlen) labels = df.label.values data.shape, labels.shape . ((197640, 40), (197640,)) . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( data, labels, test_size=0.3, random_state=952) . &#47784;&#45944; 1 . import tensorflow as tf from tensorflow.keras import models, layers embedding_dim = 100 mod_1 = models.Sequential([ layers.Embedding(max_words, embedding_dim, input_length=maxlen), layers.Flatten(), layers.Dropout(0.5), layers.Dense(32, activation=&#39;relu&#39;), layers.Dropout(0.5), layers.Dense(1, activation=&#39;sigmoid&#39;) ]) mod_1.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding (Embedding) (None, 40, 100) 8000000 _________________________________________________________________ flatten (Flatten) (None, 4000) 0 _________________________________________________________________ dropout (Dropout) (None, 4000) 0 _________________________________________________________________ dense (Dense) (None, 32) 128032 _________________________________________________________________ dropout_1 (Dropout) (None, 32) 0 _________________________________________________________________ dense_1 (Dense) (None, 1) 33 ================================================================= Total params: 8,128,065 Trainable params: 8,128,065 Non-trainable params: 0 _________________________________________________________________ . mod_1.compile( optimizer=&#39;rmsprop&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;] ) early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor=&#39;val_accuracy&#39;, min_delta=0.0001, patience=2) history = mod_1.fit( X_train, y_train, epochs=10, batch_size=512, validation_split=0.2, callbacks=[early_stopping_cb] ) . Epoch 1/10 217/217 [==============================] - 5s 19ms/step - loss: 0.5980 - accuracy: 0.6558 - val_loss: 0.4060 - val_accuracy: 0.8125 Epoch 2/10 217/217 [==============================] - 4s 16ms/step - loss: 0.3921 - accuracy: 0.8288 - val_loss: 0.3911 - val_accuracy: 0.8215 Epoch 3/10 217/217 [==============================] - 4s 16ms/step - loss: 0.3617 - accuracy: 0.8464 - val_loss: 0.3780 - val_accuracy: 0.8319 Epoch 4/10 217/217 [==============================] - 4s 18ms/step - loss: 0.3349 - accuracy: 0.8599 - val_loss: 0.3825 - val_accuracy: 0.8312 Epoch 5/10 217/217 [==============================] - 4s 17ms/step - loss: 0.3166 - accuracy: 0.8697 - val_loss: 0.3928 - val_accuracy: 0.8286 . results = mod_1.evaluate(X_test, y_test) results . 1853/1853 [==============================] - 2s 968us/step - loss: 0.3906 - accuracy: 0.8293 . [0.3905869126319885, 0.8293361663818359] . 정확도 82.9%를 얻었다. . &#47784;&#45944; 2: &#54633;&#49457;&#44273; &#49888;&#44221;&#47581; . 전창욱 등의 자연어 처리[1]에서 모델을 가져왔다. 다만 maxlen이나 max_words는 다르게 설정하였다. . import tensorflow as tf from tensorflow.keras import models, layers from tensorflow.keras import Input from tensorflow.keras.constraints import max_norm maxlen = 40 max_words = 80000 embedding_dim = 128 inp = Input(shape=(maxlen,)) embedded_inp = layers.Embedding(max_words, embedding_dim, input_length=maxlen)(inp) embedded_inp = layers.Dropout(0.5)(embedded_inp) conv_3 = layers.Conv1D(100, 3, activation=&#39;relu&#39;, kernel_constraint=max_norm(3.0))(embedded_inp) conv_3 = layers.GlobalMaxPooling1D()(conv_3) conv_4 = layers.Conv1D(100, 4, activation=&#39;relu&#39;, kernel_constraint=max_norm(3.0))(embedded_inp) conv_4 = layers.GlobalMaxPooling1D()(conv_4) conv_5 = layers.Conv1D(100, 5, activation=&#39;relu&#39;, kernel_constraint=max_norm(3.0))(embedded_inp) conv_5 = layers.GlobalMaxPooling1D()(conv_5) out = tf.concat([conv_3, conv_4, conv_5], axis=-1) # out = layers.Dropout(0.5)(out) # out = layers.Dense(250, &quot;relu&quot;)(out) out = layers.Dense(250, &quot;relu&quot;, kernel_constraint=max_norm(3.0))(out) # out = layers.Dropout(0.5)(out) # out = layers.Dense(1, &quot;relu&quot;)(out) out = layers.Dense(1, &quot;relu&quot;, kernel_constraint=max_norm(3.0))(out) mod_2 = models.Model(inp, out) mod_2.summary() . Model: &#34;model&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 40)] 0 __________________________________________________________________________________________________ embedding_1 (Embedding) (None, 40, 128) 10240000 input_1[0][0] __________________________________________________________________________________________________ dropout_2 (Dropout) (None, 40, 128) 0 embedding_1[0][0] __________________________________________________________________________________________________ conv1d (Conv1D) (None, 38, 100) 38500 dropout_2[0][0] __________________________________________________________________________________________________ conv1d_1 (Conv1D) (None, 37, 100) 51300 dropout_2[0][0] __________________________________________________________________________________________________ conv1d_2 (Conv1D) (None, 36, 100) 64100 dropout_2[0][0] __________________________________________________________________________________________________ global_max_pooling1d (GlobalMax (None, 100) 0 conv1d[0][0] __________________________________________________________________________________________________ global_max_pooling1d_1 (GlobalM (None, 100) 0 conv1d_1[0][0] __________________________________________________________________________________________________ global_max_pooling1d_2 (GlobalM (None, 100) 0 conv1d_2[0][0] __________________________________________________________________________________________________ tf.concat (TFOpLambda) (None, 300) 0 global_max_pooling1d[0][0] global_max_pooling1d_1[0][0] global_max_pooling1d_2[0][0] __________________________________________________________________________________________________ dense_2 (Dense) (None, 250) 75250 tf.concat[0][0] __________________________________________________________________________________________________ dense_3 (Dense) (None, 1) 251 dense_2[0][0] ================================================================================================== Total params: 10,469,401 Trainable params: 10,469,401 Non-trainable params: 0 __________________________________________________________________________________________________ . tf.keras.utils.plot_model(mod_2, show_shapes=True) . mod_2.compile( optimizer=&#39;rmsprop&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;] ) early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor=&#39;val_accuracy&#39;, min_delta=0.0001, patience=2) history = mod_2.fit( X_train, y_train, epochs=10, batch_size=512, validation_split=0.2, callbacks=[early_stopping_cb] ) . Epoch 1/10 217/217 [==============================] - 10s 36ms/step - loss: 0.8143 - accuracy: 0.6626 - val_loss: 0.5413 - val_accuracy: 0.7874 Epoch 2/10 217/217 [==============================] - 8s 36ms/step - loss: 0.4931 - accuracy: 0.8087 - val_loss: 0.6397 - val_accuracy: 0.7467 Epoch 3/10 217/217 [==============================] - 8s 36ms/step - loss: 0.4788 - accuracy: 0.8198 - val_loss: 0.4737 - val_accuracy: 0.8172 Epoch 4/10 217/217 [==============================] - 8s 37ms/step - loss: 0.4582 - accuracy: 0.8346 - val_loss: 0.4968 - val_accuracy: 0.8169 Epoch 5/10 217/217 [==============================] - 8s 37ms/step - loss: 0.4499 - accuracy: 0.8460 - val_loss: 0.5240 - val_accuracy: 0.8102 . results = mod_2.evaluate(X_test, y_test) results . 1853/1853 [==============================] - 3s 2ms/step - loss: 0.5296 - accuracy: 0.8124 . [0.5295822620391846, 0.812386155128479] . 정확도 81.2%를 얻었다. . &#47610;&#51004;&#47728; . 한국어에 대한 형태소 분석, 토큰화, 이진 분류 모델을 구성 | 이를 이용하여 네이버 영화 리뷰에 대한 감성 분류 | khaiii 형태소 분석기로도 어느 정도의 성과 확인 | .",
            "url": "https://danhojin.github.io/jupyter-blog/nlp/2021/01/01/nsmc.html",
            "relUrl": "/nlp/2021/01/01/nsmc.html",
            "date": " • Jan 1, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "모분산 불편 추정량과 모 표준편차",
            "content": "수리통계학 연습 문제 4.2.10을 살피다 지금까지 별 의심 없이 써왔던 모분산의 불편 추청량 $S^2$을 다시 생각해보게 되었다. 모 표준편차의 분편 추정량을 $ sqrt{S^2}$으로 볼 수 있냐는 문제이다. 모분산의 불편 추정량은 다음과 같다. . $S^2 = frac{ sum_{i=1}^{n} (X_i - bar X_n)^2}{n - 1}$ . $X_i sim N( mu, sigma^2)$이고, $ bar X_n = sum_{i=1}^{n} X_i$이다. 불편 추청량이라 함은 $E(S^2) = sigma^2$ 식이 성립한다는 것이다. 그렇다면, $E(S) = sigma$이라고 볼 수 있나? 그렇지 않다. 더 자세한 내용은 모 표준 편차의 불편 추청에 관한 위키피디아 문서를 참고하기 바란다[2]. 문제의 힌트를 사용해서 $E(S)$를 계산해 보겠다. . $E(S) = frac{ sigma}{ sqrt{n-1}} E left[ sqrt{ frac{(n-1)S^2}{ sigma^2}} right]$ . 제곱근 안의 항은 $ frac{(n-1)S^2}{ sigma^2} sim chi^2(n-1)$이므로 감마분포의 확률밀도함수(pdf) $f(x)$를 이용하여 다음과 같이 쓸 수 있다. . $E(S) = frac{ sigma}{ sqrt{n-1}} int_0^{ infty} x^{1/2} f(x) dx$ . 단, 확률밀도함수 f(x)는 자유도 $r$과 정의 구간 $0&lt;x&lt; infty$에 대하여 다음과 같다. . $f(x) = frac{1}{ Gamma(r/2) 2^{r/2}} x^{r/2 - 1} e^{-x/2}$ . 연습문제에서 $n=9$이고, $r=n-1=8$이다. 자유도를 적분식에 넣고 감마 함수로 정리하면 어렵지 않게 $E(s)$ 값을 계산할 수도 있지만, 여기에서는 sympy 패키지를 이용하여 적분을 풀어보겠다. . 참고 문헌 . 호그, 매킨, 크레이그, 박태영 옮김, 수리통계학 개론, 7판, Pearson/경문사, 2018 | 위키피디아, Unbiased estimation of standard deviation, 최종 편집 2020년 5월 7일, https://en.wikipedia.org/wiki/Unbiased_estimation_of_standard_deviation | from sympy import * init_printing() x, sigma = symbols(&#39;x, sigma&#39;) n = 9 r = Rational(n - 1, 1) f = 1 / gamma(r / 2) / 2**(r / 2) * x**(r / 2 - 1) * exp(-x / 2) f . $ displaystyle frac{x^{3} e^{- frac{x}{2}}}{96}$ E_S = sigma / sqrt(r) * integrate(sqrt(x) * f, [x, 0, oo]) # print(E_S) E_S . $ displaystyle frac{35 sqrt{ pi} sigma}{64}$ E_S.evalf() . $ displaystyle 0.969310699713954 sigma$ 불편 분산의 제곱근 기댓값은 $E(S) = 0.969 sigma$로 불편 분산의 제곱근은 모 표준 편차의 불편량으로 사용할 수 없다는 점을 확인하였다. . 4.2.10 (b)에서 신뢰구간이 확률변수 $t(8) = sqrt{9} ( bar X - mu) / S$에 근거를 두므로 95% 신뢰 구간의 길이는 $2t_{ alpha/2, n-1} S / sqrt{n}$이다. 마지막으로 $S$와 $ sigma$ 관계를 삽입하면 신뢰 구간의 길이를 얻을 수 있다. . from scipy.stats import t import numpy as np rv = t(n - 1) 2 * rv.ppf(0.975) / np.sqrt(9) * 0.96931 . $ displaystyle 1.49015524541946$ 최종적으로 계산된 신뢰 구간의 길이는 1.49 $ sigma$이다. .",
            "url": "https://danhojin.github.io/jupyter-blog/statistics/2020/12/27/%EB%AA%A8%EB%B6%84%EC%82%B0-%EB%AA%A8%ED%91%9C%EC%A4%80%ED%8E%B8%EC%B0%A8-%EB%B6%88%ED%8E%B8-%EC%B6%94%EC%A0%95%EB%9F%89.html",
            "relUrl": "/statistics/2020/12/27/%EB%AA%A8%EB%B6%84%EC%82%B0-%EB%AA%A8%ED%91%9C%EC%A4%80%ED%8E%B8%EC%B0%A8-%EB%B6%88%ED%8E%B8-%EC%B6%94%EC%A0%95%EB%9F%89.html",
            "date": " • Dec 27, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://danhojin.github.io/jupyter-blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://danhojin.github.io/jupyter-blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://danhojin.github.io/jupyter-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://danhojin.github.io/jupyter-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}