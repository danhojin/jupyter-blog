{
  
    
        "post0": {
            "title": "노드 간 연결성 예측",
            "content": "&#44536;&#47000;&#54532; &#45236; &#45432;&#46300; &#50672;&#44208;&#49457; . 그래프의 노드 간 엣지 혹은 링크 예측은 여러 응용이 존재한다. 사회 연결망 또는 소셜 네트워크에서 친구를 추천하거나, 아이템 추천 시스템, 지식 그래프를 완결시키는데 링크 예측을 사용할 수 있다. dgl 패키지가 제공하는 기본 예제를 통하여 링크 예측 문제를 이해하고, 패키지의 기본 용례를 살펴보겠다. . 예제에 사용하는 Cora 데이타셋은 인용 네트워크로 Kipf의 논문을 참고하자. . import dgl import torch import torch.nn as nn import torch.nn.functional as F import itertools import numpy as np import scipy.sparse as sp from scipy.special import comb print(&#39;torch: {}&#39;.format(torch.__version__)) print(&#39;dgl: {}&#39;.format(dgl.__version__)) print() import dgl.data dataset = dgl.data.CoraGraphDataset() g = dataset[0] . Using backend: pytorch . torch: 1.9.0+cu102 dgl: 0.6.1 NumNodes: 2708 NumEdges: 10556 NumFeats: 1433 NumClasses: 7 NumTrainingSamples: 140 NumValidationSamples: 500 NumTestSamples: 1000 Done loading data from cached files. . Cora 데이타셋은 2708개의 노드와 10556 / 2 = 5278 개의 엣지가 있는 무향 그래프이다. 각 노드는 1433 차원의 특징 벡터로 표현되며, 7개 클래스로 레이블되어 있다. 그래프의 밀도는 0.144%이다. . nCk = int(comb(2708, 2)) print(f&#39;n choose k: {nCk}&#39;) print(f&#39;positive examples: {5278}&#39;) print(f&#39;negative examples: {nCk - 5278}&#39;) print(&#39;Graph density: {:.3%}&#39;.format(5278 / nCk)) . n choose k: 3665278 positive examples: 5278 negative examples: 3660000 Graph density: 0.144% . [(k, v) for (k, v) in g.ndata.items()] . [(&#39;feat&#39;, tensor([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]])), (&#39;label&#39;, tensor([3, 4, 4, ..., 3, 3, 3])), (&#39;test_mask&#39;, tensor([False, False, False, ..., True, True, True])), (&#39;train_mask&#39;, tensor([ True, True, True, ..., False, False, False])), (&#39;val_mask&#39;, tensor([False, False, False, ..., False, False, False]))] . g.edges() . (tensor([ 0, 0, 0, ..., 2707, 2707, 2707]), tensor([ 633, 1862, 2582, ..., 598, 1473, 2706])) . &#44032;&#51221; . 임의로 두 노드 사이에 엣지가 있을 확률은 매우 낮다. 주어진 엣지를 positive 사례라고 부르자. 그리고 나머지 가능한 엣지에서 positive 사례를 찾을 가능성은 그래프 밀도 정도라 가정하자. positive example 수 만큼 추가로 사례를 추출하여 negative 사례라고 하자. negative 사례에 극소수는 실제로 엣지가 존재할 수 있지만 노이즈로 간주한다. positive/negative 사례로 나누고 분류 문제로 문제를 구성하여 링크 예측 문제를 해결하겠다. . negative &#49324;&#47168; &#52628;&#52636;&#51032; &#51060;&#54644; . 인접 행렬은 엣지의 유무를 확인할 수 있으므로 negative 사례를 추출하는데 사용할 수 있다. . import matplotlib.pyplot as plt import networkx as nx %matplotlib inline . u = np.array([1, 1, 2, 2, 5, 3, 4, 2, 5, 3, 5, 4, 4, 6]) v = np.array([2, 5, 3, 5, 4, 4, 6, 1, 1, 2, 2, 5, 3, 4]) G = nx.Graph() G.add_edges_from(np.c_[u, v]) G.number_of_nodes(), G.number_of_edges() . (6, 7) . nx.draw(G, with_labels=True, font_weight=&#39;bold&#39;) . adj = sp.coo_matrix((np.ones(len(u)), (u - 1, v - 1))) adj.todense() . matrix([[0., 1., 0., 0., 1., 0.], [1., 0., 1., 0., 1., 0.], [0., 1., 0., 1., 0., 0.], [0., 0., 1., 0., 1., 1.], [1., 1., 0., 1., 0., 0.], [0., 0., 0., 1., 0., 0.]]) . adj_neg = 1 - adj.todense() - np.eye(6) adj_neg . matrix([[0., 0., 1., 1., 0., 1.], [0., 0., 0., 1., 0., 1.], [1., 0., 0., 0., 1., 1.], [1., 1., 0., 0., 0., 0.], [0., 0., 1., 0., 0., 1.], [1., 1., 1., 0., 1., 0.]]) . np.where(adj_neg != 0) . (array([0, 0, 0, 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 5, 5]), array([2, 3, 5, 3, 5, 0, 4, 5, 0, 1, 2, 5, 0, 1, 2, 4])) . negative &#49324;&#47168; &#52628;&#52636;&#44284; &#54984;&#47144; &#48143; &#44160;&#51221; &#45936;&#51060;&#53440; &#51456;&#48708; . 양방향 엣지를 가지고 있으므로 검정 데이타가 누구 되지 않도록 주의해서 분할하였다. 각 노드의 특징 평가를 위하여 tr_g, 엣지 점수 평가를 위하여 tr_p_g, te_p_g, tr_n_g, te_n_g 그래프를 준비하였다. . u, v = g.edges() p_edges = np.c_[u.numpy(), v.numpy()] p_edges = np.r_[[e for e in p_edges if e[0] &lt; e[1]]] num_edges = len(p_edges) num_train = int(num_edges * 0.8) np.random.shuffle(p_edges) tr_p_edges = p_edges[:num_train] tr_p_edges = np.r_[tr_p_edges, tr_p_edges[:, [1, 0]]] tr_p_g = dgl.graph((tr_p_edges[:, 0], tr_p_edges[:, 1]), num_nodes=g.number_of_nodes()) te_p_edges = p_edges[num_train:] te_p_edges = np.r_[te_p_edges, te_p_edges[:, [1, 0]]] te_p_g = dgl.graph((te_p_edges[:, 0], te_p_edges[:, 1]), num_nodes=g.number_of_nodes()) tr_p_g, te_p_g . (Graph(num_nodes=2708, num_edges=8444, ndata_schemes={} edata_schemes={}), Graph(num_nodes=2708, num_edges=2112, ndata_schemes={} edata_schemes={})) . te_p_edges_set = {tuple(e) for e in te_p_edges} eids = [k for (k, e) in enumerate(zip(u.numpy(), v.numpy())) if e in te_p_edges_set] len(eids), eids[:10] . (2112, [1, 7, 8, 9, 11, 18, 23, 38, 40, 41]) . tr_g = dgl.remove_edges(g, eids) tr_g . Graph(num_nodes=2708, num_edges=8444, ndata_schemes={&#39;feat&#39;: Scheme(shape=(1433,), dtype=torch.float32), &#39;label&#39;: Scheme(shape=(), dtype=torch.int64), &#39;test_mask&#39;: Scheme(shape=(), dtype=torch.bool), &#39;train_mask&#39;: Scheme(shape=(), dtype=torch.bool), &#39;val_mask&#39;: Scheme(shape=(), dtype=torch.bool)} edata_schemes={}) . # num_edges = num_edges * 2 # num_train = num_train * 2 adj = sp.coo_matrix((np.ones(g.number_of_edges()), (u.numpy(), v.numpy()))) adj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes()) u_neg, v_neg = np.where(adj_neg != 0) n_edges = np.r_[[uv for uv in np.c_[u_neg, v_neg] if uv[0] &lt; uv[1]]] np.random.shuffle(n_edges) tr_n_edges = n_edges[:num_train] tr_n_edges = np.r_[tr_n_edges, tr_n_edges[:, [1, 0]]] tr_n_g = dgl.graph((tr_n_edges[:, 0], tr_n_edges[:, 1]), num_nodes=g.number_of_nodes()) te_n_edges = n_edges[num_train:num_edges] te_n_edges = np.r_[te_n_edges, te_n_edges[:, [1, 0]]] te_n_g = dgl.graph((te_n_edges[:, 0], te_n_edges[:, 1]), num_nodes=g.number_of_nodes()) tr_n_g, te_n_g . (Graph(num_nodes=2708, num_edges=8444, ndata_schemes={} edata_schemes={}), Graph(num_nodes=2708, num_edges=2112, ndata_schemes={} edata_schemes={})) . GraphSAGE model . 링크를 예측 문제는 우리가 가진 그래프의 엣지 정보가 불완전하다는 것을 의미한다. 정확한 그래프 라플라스 행렬을 알 지 못하므로 GAGEConv 모델을 이용한다. SAGE는 Sample &amp; Aggregate에서 나온 말이다. . from dgl.nn import SAGEConv class GraphSAGE(nn.Module): def __init__(self, in_feats, h_feats): super().__init__() self.conv1 = SAGEConv(in_feats, h_feats, &#39;mean&#39;) self.conv2 = SAGEConv(h_feats, h_feats, &#39;mean&#39;) def forward(self, g, in_feat): h = F.relu(self.conv1(g, in_feat)) h = self.conv2(g, h) return h . EdgeScore model . 그래프에서 각 노드의 특징으로부터 엣지의 특성값을 도출하는 모델을 작성하였다. . class EdgeScore(nn.Module): # original name: MLPPredictor def __init__(self, h_feats): super().__init__() self.W1 = nn.Linear(h_feats * 2, h_feats) self.W2 = nn.Linear(h_feats, 1) def apply_edges(self, edges): &quot;&quot;&quot; Computes a scalar score for each edge of the given graph. Parameters - edges : Has three members ``src``, ``dst`` and ``data``, each of which is a dictionary representing the features of the source nodes, the destination nodes, and the edges themselves. Returns - dict A dictionary of new edge features. &quot;&quot;&quot; h = torch.cat([edges.src[&#39;h&#39;], edges.dst[&#39;h&#39;]], 1) return {&#39;score&#39;: self.W2(F.relu(self.W1(h))).squeeze(1)} def forward(self, g, h): with g.local_scope(): g.ndata[&#39;h&#39;] = h g.apply_edges(self.apply_edges) return g.edata[&#39;score&#39;] . def compute_loss(pos_score, neg_score): scores = torch.cat([pos_score, neg_score]) labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]) return F.binary_cross_entropy_with_logits(scores, labels) def compute_auc(pos_score, neg_score): scores = torch.cat([pos_score, neg_score]).numpy() labels = torch.cat( [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy() return roc_auc_score(labels, scores) . node_feature_model = GraphSAGE(tr_g.ndata[&#39;feat&#39;].shape[1], 32) edge_score_model = EdgeScore(32) optimizer = torch.optim.Adam( itertools.chain(node_feature_model.parameters(), edge_score_model.parameters()), lr=0.001) # -- 4. training -- # all_logits = [] for epoch in range(1, 61): # forward h = node_feature_model(tr_g, tr_g.ndata[&#39;feat&#39;]) pos_score = edge_score_model(tr_p_g, h) neg_score = edge_score_model(tr_n_g, h) loss = compute_loss(pos_score, neg_score) # backward optimizer.zero_grad() loss.backward() optimizer.step() if epoch % 5 == 0: with torch.no_grad(): pos_score = edge_score_model(te_p_g, h) neg_score = edge_score_model(te_n_g, h) va_loss = compute_loss(pos_score, neg_score) print(f&#39;Epoch {epoch:03d}, loss: {loss:.3f} va loss: {va_loss:.3f}&#39;) # -- 5. check results # print() from sklearn.metrics import roc_auc_score with torch.no_grad(): pos_score = edge_score_model(te_p_g, h) neg_score = edge_score_model(te_n_g, h) print(&#39;AUC&#39;, compute_auc(pos_score, neg_score)) . Epoch 005, loss: 0.693 va loss: 0.693 Epoch 010, loss: 0.691 va loss: 0.693 Epoch 015, loss: 0.689 va loss: 0.692 Epoch 020, loss: 0.686 va loss: 0.691 Epoch 025, loss: 0.680 va loss: 0.688 Epoch 030, loss: 0.673 va loss: 0.685 Epoch 035, loss: 0.662 va loss: 0.681 Epoch 040, loss: 0.647 va loss: 0.676 Epoch 045, loss: 0.629 va loss: 0.672 Epoch 050, loss: 0.608 va loss: 0.671 Epoch 055, loss: 0.586 va loss: 0.676 Epoch 060, loss: 0.564 va loss: 0.690 AUC 0.6191352444903581 . &#47560;&#47924;&#47532; . dgl 패키지를 기본 예제를 통하여 링크 예측 문제를 살펴보면서 dgl 패키지는 코드 가독성이 높다는 인상을 받았다. 즉, dgl 패키지를 처음 접해도 그래프에 대한 개념만 갖춘다면 내 생각을 코드로 구성하여 실험해 보기 쉽다는 뜻으로, 여러 응용을 시도해 보기 좋은 패키지이다. .",
            "url": "https://danhojin.github.io/jupyter-blog/graph/2021/07/02/link-prediction-graph-nn.html",
            "relUrl": "/graph/2021/07/02/link-prediction-graph-nn.html",
            "date": " • Jul 2, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "가위바위보 강화학습",
            "content": "마지막 갱신: 2021-07-04 . ray 패키지의 rllib은 사용자 모델을 만들어 쓸 수 있게 설계되어 있다. 하지만 세부적인 사항을 이해하고 사용자 모델을 쓰기가 쉽지 않다. 여기에 내부 코드의 이해를 위하여 몇 가지 코드를 작성하여 시험해 본다. . import ray from ray.rllib.utils.framework import try_import_torch from ray.rllib.agents.pg import PGTorchPolicy, PGTrainer from ray.rllib.examples.env.rock_paper_scissors import RockPaperScissors torch, nn = try_import_torch() torch.__version__, ray.__version__ . (&#39;1.9.0+cu102&#39;, &#39;2.0.0.dev0&#39;) . &#44032;&#50948;&#48148;&#50948;&#48372; &#54872;&#44221; . player1과 player2가 취할 수 있는 action은 0, 1, 2이고, 관측치는 상대방의 action에서 나오는 0, 1, 2이다. | env.reset()은 첫 관측치 {&#39;player1&#39;: 0, &#39;player2&#39;: 0}를 내어 준다. | env.step() 함수는 obs, rewards, done, info를 돌려준다. | . env_config = dict() env = RockPaperScissors(env_config) . obs = env.reset() obs . {&#39;player1&#39;: 0, &#39;player2&#39;: 0} . env.action_space, env.observation_space . (Discrete(3), Discrete(3)) . obs, reward, done, info = env.step(dict(player1=1, player2=2)) . print(f&#39;obs : {obs}&#39;) print(f&#39;reward: {reward}&#39;) print(f&#39;done : {done}&#39;) print(f&#39;info : {info}&#39;) . obs : {&#39;player1&#39;: 2, &#39;player2&#39;: 1} reward: {&#39;player1&#39;: -1, &#39;player2&#39;: 1} done : {&#39;__all__&#39;: False} info : {} . obs, reward, done, info = env.step(dict(player1=1, player2=1)) . print(f&#39;obs : {obs}&#39;) print(f&#39;reward: {reward}&#39;) print(f&#39;done : {done}&#39;) print(f&#39;info : {info}&#39;) . obs : {&#39;player1&#39;: 1, &#39;player2&#39;: 1} reward: {&#39;player1&#39;: 0, &#39;player2&#39;: 0} done : {&#39;__all__&#39;: False} info : {} . &#51221;&#52293; . 정책(Policy)은 에이전트가 관측된 환경, 이전 보상 이력을 참고하여 어떤 행동을 취하면 좋은지 결정한다. 에이전트가 둘인 가위바위보 게임에서 학습 가능한 learned 정책과 RandomMove, BeatLastHeuristic, AlwaysSameHeuristic 정책에서 하나를 뽑아 강확학습을 수행하였다. 가위바위보의 최고 전략은 RandomMove로 learned 정책은 이에 도달하는 결과를 보일 것이고, 그 밖에 BeatLastHeuristic, AlwaysSameHeuristic에 데해서는 쉽게 이길 수 있을 것이다. . 새로운 정책을 개발한다면 Policy 클래스를 상속하고 compute_actions에 필요한 로직을 구현하면 된다. RandomMove 정책은 obs_batch에 대해서만 고려하여 배치 크기만큼의 임의 행동을 돌려준다. kwargs에 필수 생성 인자 내용을 정리하였다. 환경의 action_space와 observation_space가 그것이다. . import random from ray.rllib.policy.policy import Policy # from ray.rllib.policy.view_requirement import ViewRequirement class RandomMove(Policy): &quot;&quot;&quot;Pick a random move&quot;&quot;&quot; def __init__(self, obs_space, act_space, config): super().__init__(obs_space, act_space, config) def random_action(self): return random.choice([ RockPaperScissors.ROCK, RockPaperScissors.PAPER, RockPaperScissors.SCISSORS, ]) def compute_actions( self, obs_batch, state_batches=None, prev_action_batch=None, prev_reward_batch=None, info_batch=None, episodes=None, **kwargs): &quot;&quot;&quot;Returns: Tuple: actions: [BATCH_SIZE, ACTION_SHAPE] &quot;&quot;&quot; return [self.random_action() for _ in obs_batch], [], {} . random_policy = RandomMove(env.observation_space, env.action_space, {}) random_policy.compute_actions(list(range(3))) . ([1, 2, 2], [], {}) . Custom policy with template . from ray.rllib.policy.torch_policy_template import build_policy_class from ray.rllib.policy.sample_batch import SampleBatch def policy_gradient_loss(policy, model, dist_class, train_batch): logits, _ = model.from_batch(train_batch) action_dist = dist_class(logits) log_probs = action_dist.logp(train_batch[SampleBatch.ACTIONS]) # print(train_batch[SampleBatch.REWARDS].dtype) # print(log_probs.dtype) return -train_batch[SampleBatch.REWARDS].float().dot(log_probs) MyTorchPolicy = build_policy_class( framework=&#39;torch&#39;, name=&#39;MyTorchPolicy&#39;, loss_fn=policy_gradient_loss, ) . Custom loss model . from typing import Dict import numpy as np from ray.rllib.models.modelv2 import ModelV2 from ray.rllib.models import ModelCatalog from ray.rllib.models.torch.torch_action_dist import TorchCategorical from ray.rllib.models.torch.torch_modelv2 import TorchModelV2 from ray.rllib.models.torch.fcnet import FullyConnectedNetwork from ray.rllib.utils.annotations import override from ray.rllib.utils.framework import TensorType class TorchCustomLossModel(TorchModelV2, nn.Module): def __init__(self, obs_space, action_space, num_outputs, model_config, name): TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name) nn.Module.__init__(self) print(f&#39;model config: {model_config}&#39;) self.fcnet = FullyConnectedNetwork( self.obs_space, self.action_space, num_outputs, model_config, name=&quot;fcnet&quot; ) @override(ModelV2) def forward(self, input_dict, state, seq_lens): return self.fcnet(input_dict, state, seq_lens) @override(ModelV2) def value_function(self): return self.fcnet.value_function() @override(ModelV2) def custom_loss(self, policy_loss: TensorType, loss_inputs: Dict[str, TensorType]) -&gt; TensorType: logits, _ = self.forward({&#39;obs&#39;: loss_inputs[&#39;obs&#39;]}, [], None) action_dist = TorchCategorical(logits, self.model_config) imitation_loss = torch.mean( -action_dist.logp(loss_inputs[&#39;actions&#39;].to(policy_loss[0].device)) ) self.imitation_loss_metric = imitation_loss.item() self.policy_loss_metric = np.mean([ loss.item() for loss in policy_loss ]) return [loss_ + 10 * imitation_loss for loss_ in policy_loss] def metrics(self): return { &#39;policy_loss&#39;: self.policy_loss_metric, &#39;imitation_loss&#39;: self.imitation_loss_metric, } ModelCatalog.register_custom_model(&#39;my_torch_model&#39;, TorchCustomLossModel) . &#53944;&#47112;&#51060;&#45320; &#49444;&#51221; &#48143; &#54617;&#49845; . 앞서 정의한 RandomMove외에 ray에서 제공하는 BeatLastHeuristic, AlwaysSameHeuristic 정책을 추가하였다. . import ray from gym.spaces import Discrete from ray.rllib.agents.registry import get_trainer_class from ray.rllib.examples.policy.rock_paper_scissors_dummies import ( BeatLastHeuristic, AlwaysSameHeuristic ) config = { &#39;env&#39;: RockPaperScissors, &#39;gamma&#39;: 0.9, &#39;num_gpus&#39;: 0, &#39;num_workers&#39;: 0, &#39;num_envs_per_worker&#39;: 4, &#39;train_batch_size&#39;: 200, # for the policy model &#39;multiagent&#39;: { &#39;policies&#39;: { &#39;random_move&#39;: (RandomMove, Discrete(3), Discrete(3), {}), &#39;beat_last&#39;: (BeatLastHeuristic, Discrete(3), Discrete(3), {}), &#39;always_same&#39;: (AlwaysSameHeuristic, Discrete(3), Discrete(3), {}), &#39;learned0&#39;: (None, Discrete(3), Discrete(3), { &#39;framework&#39;: &#39;torch&#39;, &#39;model&#39;: {}, # use default }), &#39;learned1&#39;: (MyTorchPolicy, Discrete(3), Discrete(3), { &#39;framework&#39;: &#39;torch&#39;, &#39;model&#39;: {}, # use default }), &#39;learned2&#39;: (None, Discrete(3), Discrete(3), { &#39;framework&#39;: &#39;torch&#39;, &#39;model&#39;: { &#39;custom_model&#39;: &#39;my_torch_model&#39;, &#39;custom_model_config&#39;: {}, }, # use default }), }, &#39;policy_mapping_fn&#39;: lambda agent_id, episode, **kwargs: ( &#39;learned2&#39; if agent_id == &#39;player1&#39; else &#39;beat_last&#39;), &#39;policies_to_train&#39;: [&#39;learned0&#39;, &#39;learned1&#39;, &#39;learned2&#39;], }, &#39;framework&#39;: &#39;torch&#39;, } # ray.shutdown() ray.init() trainer = get_trainer_class(&#39;PG&#39;)(config=config) . 2021-07-04 11:54:03,897 INFO services.py:1330 -- View the Ray dashboard at http://127.0.0.1:8265 2021-07-04 11:54:04,893 INFO trainer.py:714 -- Current log_level is WARN. For more information, set &#39;log_level&#39;: &#39;INFO&#39; / &#39;DEBUG&#39; or use the -v and -vv flags. . model config: {&#39;_use_default_native_models&#39;: False, &#39;fcnet_hiddens&#39;: [256, 256], &#39;fcnet_activation&#39;: &#39;tanh&#39;, &#39;conv_filters&#39;: None, &#39;conv_activation&#39;: &#39;relu&#39;, &#39;post_fcnet_hiddens&#39;: [], &#39;post_fcnet_activation&#39;: &#39;relu&#39;, &#39;free_log_std&#39;: False, &#39;no_final_linear&#39;: False, &#39;vf_share_layers&#39;: True, &#39;use_lstm&#39;: False, &#39;max_seq_len&#39;: 20, &#39;lstm_cell_size&#39;: 256, &#39;lstm_use_prev_action&#39;: False, &#39;lstm_use_prev_reward&#39;: False, &#39;_time_major&#39;: False, &#39;use_attention&#39;: False, &#39;attention_num_transformer_units&#39;: 1, &#39;attention_dim&#39;: 64, &#39;attention_num_heads&#39;: 1, &#39;attention_head_dim&#39;: 32, &#39;attention_memory_inference&#39;: 50, &#39;attention_memory_training&#39;: 50, &#39;attention_position_wise_mlp_dim&#39;: 32, &#39;attention_init_gru_gate_bias&#39;: 2.0, &#39;attention_use_n_prev_actions&#39;: 0, &#39;attention_use_n_prev_rewards&#39;: 0, &#39;num_framestacks&#39;: 0, &#39;dim&#39;: 84, &#39;grayscale&#39;: False, &#39;zero_mean&#39;: True, &#39;custom_model&#39;: &#39;my_torch_model&#39;, &#39;custom_model_config&#39;: {}, &#39;custom_action_dist&#39;: None, &#39;custom_preprocessor&#39;: None, &#39;lstm_use_prev_action_reward&#39;: -1, &#39;framestack&#39;: True} . for _ in range(1): results = trainer.train() for k, v in results.items(): print(f&#39;{k}: {v}&#39;) . episode_reward_max: 0.0 episode_reward_min: 0.0 episode_reward_mean: 0.0 episode_len_mean: 10.0 episode_media: {} episodes_this_iter: 80 policy_reward_min: {&#39;learned2&#39;: -6.0, &#39;beat_last&#39;: -6.0} policy_reward_max: {&#39;learned2&#39;: 6.0, &#39;beat_last&#39;: 6.0} policy_reward_mean: {&#39;learned2&#39;: 0.0625, &#39;beat_last&#39;: -0.0625} custom_metrics: {} hist_stats: {&#39;episode_reward&#39;: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], &#39;episode_lengths&#39;: [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10], &#39;policy_learned2_reward&#39;: [-1.0, -3.0, 0.0, 0.0, -3.0, 3.0, -5.0, 2.0, 3.0, 2.0, 0.0, 0.0, -4.0, -5.0, 4.0, -4.0, -1.0, 2.0, 3.0, 3.0, 0.0, -3.0, 1.0, -3.0, 2.0, 5.0, 3.0, 2.0, 1.0, 0.0, 3.0, -1.0, 0.0, 1.0, 5.0, -1.0, 0.0, -1.0, 0.0, 0.0, 4.0, 0.0, -2.0, 2.0, -1.0, -1.0, -2.0, 2.0, 0.0, -5.0, -3.0, 3.0, 2.0, 0.0, 0.0, 1.0, -1.0, 0.0, 0.0, -2.0, 1.0, 2.0, 0.0, 3.0, 2.0, -1.0, -6.0, 2.0, 6.0, 0.0, -2.0, 0.0, -3.0, -2.0, 0.0, -1.0, 2.0, -2.0, -5.0, 2.0], &#39;policy_beat_last_reward&#39;: [1.0, 3.0, 0.0, 0.0, 3.0, -3.0, 5.0, -2.0, -3.0, -2.0, 0.0, 0.0, 4.0, 5.0, -4.0, 4.0, 1.0, -2.0, -3.0, -3.0, 0.0, 3.0, -1.0, 3.0, -2.0, -5.0, -3.0, -2.0, -1.0, 0.0, -3.0, 1.0, 0.0, -1.0, -5.0, 1.0, 0.0, 1.0, 0.0, 0.0, -4.0, 0.0, 2.0, -2.0, 1.0, 1.0, 2.0, -2.0, 0.0, 5.0, 3.0, -3.0, -2.0, 0.0, 0.0, -1.0, 1.0, 0.0, 0.0, 2.0, -1.0, -2.0, 0.0, -3.0, -2.0, 1.0, 6.0, -2.0, -6.0, 0.0, 2.0, 0.0, 3.0, 2.0, 0.0, 1.0, -2.0, 2.0, 5.0, -2.0]} sampler_perf: {&#39;mean_raw_obs_processing_ms&#39;: 0.29636734160617806, &#39;mean_inference_ms&#39;: 1.2038987667406376, &#39;mean_action_processing_ms&#39;: 0.10593850814287933, &#39;mean_env_wait_ms&#39;: 0.04650111222148534, &#39;mean_env_render_ms&#39;: 0.0} off_policy_estimator: {} num_healthy_workers: 0 timesteps_total: 800 agent_timesteps_total: 1600 timers: {&#39;sample_time_ms&#39;: 335.555, &#39;sample_throughput&#39;: 2384.111, &#39;learn_time_ms&#39;: 37.361, &#39;learn_throughput&#39;: 21412.483} info: {&#39;learner&#39;: {&#39;learned2&#39;: {&#39;learner_stats&#39;: {&#39;allreduce_latency&#39;: 0.0, &#39;policy_loss&#39;: -0.09941699355840683}, &#39;model&#39;: {&#39;policy_loss&#39;: -0.09941699355840683, &#39;imitation_loss&#39;: 1.098800539970398}, &#39;custom_metrics&#39;: {}}}, &#39;num_steps_sampled&#39;: 800, &#39;num_agent_steps_sampled&#39;: 1600, &#39;num_steps_trained&#39;: 800, &#39;num_agent_steps_trained&#39;: 1600} done: False episodes_total: 80 training_iteration: 1 experiment_id: ff7101d03aa04f819ae881e80c7c7916 date: 2021-07-04_11-54-05 timestamp: 1625367245 time_this_iter_s: 0.3737308979034424 time_total_s: 0.3737308979034424 pid: 70134 hostname: omen node_ip: 192.168.0.10 config: {&#39;num_workers&#39;: 0, &#39;num_envs_per_worker&#39;: 4, &#39;create_env_on_driver&#39;: False, &#39;rollout_fragment_length&#39;: 200, &#39;batch_mode&#39;: &#39;truncate_episodes&#39;, &#39;gamma&#39;: 0.9, &#39;lr&#39;: 0.0004, &#39;train_batch_size&#39;: 200, &#39;model&#39;: {&#39;_use_default_native_models&#39;: False, &#39;fcnet_hiddens&#39;: [256, 256], &#39;fcnet_activation&#39;: &#39;tanh&#39;, &#39;conv_filters&#39;: None, &#39;conv_activation&#39;: &#39;relu&#39;, &#39;post_fcnet_hiddens&#39;: [], &#39;post_fcnet_activation&#39;: &#39;relu&#39;, &#39;free_log_std&#39;: False, &#39;no_final_linear&#39;: False, &#39;vf_share_layers&#39;: True, &#39;use_lstm&#39;: False, &#39;max_seq_len&#39;: 20, &#39;lstm_cell_size&#39;: 256, &#39;lstm_use_prev_action&#39;: False, &#39;lstm_use_prev_reward&#39;: False, &#39;_time_major&#39;: False, &#39;use_attention&#39;: False, &#39;attention_num_transformer_units&#39;: 1, &#39;attention_dim&#39;: 64, &#39;attention_num_heads&#39;: 1, &#39;attention_head_dim&#39;: 32, &#39;attention_memory_inference&#39;: 50, &#39;attention_memory_training&#39;: 50, &#39;attention_position_wise_mlp_dim&#39;: 32, &#39;attention_init_gru_gate_bias&#39;: 2.0, &#39;attention_use_n_prev_actions&#39;: 0, &#39;attention_use_n_prev_rewards&#39;: 0, &#39;num_framestacks&#39;: &#39;auto&#39;, &#39;dim&#39;: 84, &#39;grayscale&#39;: False, &#39;zero_mean&#39;: True, &#39;custom_model&#39;: None, &#39;custom_model_config&#39;: {}, &#39;custom_action_dist&#39;: None, &#39;custom_preprocessor&#39;: None, &#39;lstm_use_prev_action_reward&#39;: -1, &#39;framestack&#39;: True}, &#39;optimizer&#39;: {}, &#39;horizon&#39;: None, &#39;soft_horizon&#39;: False, &#39;no_done_at_end&#39;: False, &#39;env&#39;: &#39;RockPaperScissors&#39;, &#39;observation_space&#39;: None, &#39;action_space&#39;: None, &#39;env_config&#39;: {}, &#39;env_task_fn&#39;: None, &#39;render_env&#39;: False, &#39;record_env&#39;: False, &#39;clip_rewards&#39;: None, &#39;normalize_actions&#39;: True, &#39;clip_actions&#39;: False, &#39;preprocessor_pref&#39;: &#39;deepmind&#39;, &#39;log_level&#39;: &#39;WARN&#39;, &#39;callbacks&#39;: &lt;class &#39;ray.rllib.agents.callbacks.DefaultCallbacks&#39;&gt;, &#39;ignore_worker_failures&#39;: False, &#39;log_sys_usage&#39;: True, &#39;fake_sampler&#39;: False, &#39;framework&#39;: &#39;torch&#39;, &#39;eager_tracing&#39;: False, &#39;explore&#39;: True, &#39;exploration_config&#39;: {&#39;type&#39;: &#39;StochasticSampling&#39;}, &#39;evaluation_interval&#39;: None, &#39;evaluation_num_episodes&#39;: 10, &#39;evaluation_parallel_to_training&#39;: False, &#39;in_evaluation&#39;: False, &#39;evaluation_config&#39;: {}, &#39;evaluation_num_workers&#39;: 0, &#39;custom_eval_function&#39;: None, &#39;sample_async&#39;: False, &#39;sample_collector&#39;: &lt;class &#39;ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector&#39;&gt;, &#39;observation_filter&#39;: &#39;NoFilter&#39;, &#39;synchronize_filters&#39;: True, &#39;tf_session_args&#39;: {&#39;intra_op_parallelism_threads&#39;: 2, &#39;inter_op_parallelism_threads&#39;: 2, &#39;gpu_options&#39;: {&#39;allow_growth&#39;: True}, &#39;log_device_placement&#39;: False, &#39;device_count&#39;: {&#39;CPU&#39;: 1}, &#39;allow_soft_placement&#39;: True}, &#39;local_tf_session_args&#39;: {&#39;intra_op_parallelism_threads&#39;: 8, &#39;inter_op_parallelism_threads&#39;: 8}, &#39;compress_observations&#39;: False, &#39;collect_metrics_timeout&#39;: 180, &#39;metrics_smoothing_episodes&#39;: 100, &#39;remote_worker_envs&#39;: False, &#39;remote_env_batch_wait_ms&#39;: 0, &#39;min_iter_time_s&#39;: 0, &#39;timesteps_per_iteration&#39;: 0, &#39;seed&#39;: None, &#39;extra_python_environs_for_driver&#39;: {}, &#39;extra_python_environs_for_worker&#39;: {}, &#39;num_gpus&#39;: 0, &#39;_fake_gpus&#39;: False, &#39;num_cpus_per_worker&#39;: 1, &#39;num_gpus_per_worker&#39;: 0, &#39;custom_resources_per_worker&#39;: {}, &#39;num_cpus_for_driver&#39;: 1, &#39;placement_strategy&#39;: &#39;PACK&#39;, &#39;input&#39;: &#39;sampler&#39;, &#39;actions_in_input_normalized&#39;: False, &#39;input_evaluation&#39;: [&#39;is&#39;, &#39;wis&#39;], &#39;postprocess_inputs&#39;: False, &#39;shuffle_buffer_size&#39;: 0, &#39;output&#39;: None, &#39;output_compress_columns&#39;: [&#39;obs&#39;, &#39;new_obs&#39;], &#39;output_max_file_size&#39;: 67108864, &#39;multiagent&#39;: {&#39;policies&#39;: {&#39;random_move&#39;: (&lt;class &#39;__main__.RandomMove&#39;&gt;, Discrete(3), Discrete(3), {}), &#39;beat_last&#39;: (&lt;class &#39;ray.rllib.examples.policy.rock_paper_scissors_dummies.BeatLastHeuristic&#39;&gt;, Discrete(3), Discrete(3), {}), &#39;always_same&#39;: (&lt;class &#39;ray.rllib.examples.policy.rock_paper_scissors_dummies.AlwaysSameHeuristic&#39;&gt;, Discrete(3), Discrete(3), {}), &#39;learned0&#39;: (None, Discrete(3), Discrete(3), {&#39;framework&#39;: &#39;torch&#39;, &#39;model&#39;: {}}), &#39;learned1&#39;: (&lt;class &#39;ray.rllib.policy.policy_template.MyTorchPolicy&#39;&gt;, Discrete(3), Discrete(3), {&#39;framework&#39;: &#39;torch&#39;, &#39;model&#39;: {}}), &#39;learned2&#39;: (None, Discrete(3), Discrete(3), {&#39;framework&#39;: &#39;torch&#39;, &#39;model&#39;: {&#39;custom_model&#39;: &#39;my_torch_model&#39;, &#39;custom_model_config&#39;: {}}})}, &#39;policy_mapping_fn&#39;: &lt;function &lt;lambda&gt; at 0x7fb213954310&gt;, &#39;policies_to_train&#39;: [&#39;learned0&#39;, &#39;learned1&#39;, &#39;learned2&#39;], &#39;observation_fn&#39;: None, &#39;replay_mode&#39;: &#39;independent&#39;, &#39;count_steps_by&#39;: &#39;env_steps&#39;}, &#39;logger_config&#39;: None, &#39;simple_optimizer&#39;: True, &#39;monitor&#39;: -1} time_since_restore: 0.3737308979034424 timesteps_since_restore: 0 iterations_since_restore: 1 perf: {&#39;cpu_util_percent&#39;: 31.4, &#39;ram_util_percent&#39;: 9.2, &#39;gpu_util_percent0&#39;: 0.27, &#39;vram_util_percent0&#39;: 0.08259507829977629} . for k in range(1, 201): results = trainer.train() if k % 10 == 0: print(k, results[&#39;episode_reward_mean&#39;], results[&#39;policy_reward_mean&#39;], &#39; t&#39;, results[&#39;timesteps_total&#39;], &#39; t&#39;, results[&#39;episodes_total&#39;]) . 10 0.0 {&#39;learned2&#39;: -0.03, &#39;beat_last&#39;: 0.03} 8800 880 20 0.0 {&#39;learned2&#39;: -0.02, &#39;beat_last&#39;: 0.02} 16800 1680 30 0.0 {&#39;learned2&#39;: 0.1, &#39;beat_last&#39;: -0.1} 24800 2480 40 0.0 {&#39;learned2&#39;: 0.0, &#39;beat_last&#39;: 0.0} 32800 3280 50 0.0 {&#39;learned2&#39;: 0.08, &#39;beat_last&#39;: -0.08} 40800 4080 60 0.0 {&#39;learned2&#39;: -0.06, &#39;beat_last&#39;: 0.06} 48800 4880 70 0.0 {&#39;learned2&#39;: 0.02, &#39;beat_last&#39;: -0.02} 56800 5680 80 0.0 {&#39;learned2&#39;: 0.39, &#39;beat_last&#39;: -0.39} 64800 6480 90 0.0 {&#39;learned2&#39;: 0.8, &#39;beat_last&#39;: -0.8} 72800 7280 100 0.0 {&#39;learned2&#39;: 1.31, &#39;beat_last&#39;: -1.31} 80800 8080 110 0.0 {&#39;learned2&#39;: 1.74, &#39;beat_last&#39;: -1.74} 88800 8880 120 0.0 {&#39;learned2&#39;: 2.74, &#39;beat_last&#39;: -2.74} 96800 9680 130 0.0 {&#39;learned2&#39;: 4.71, &#39;beat_last&#39;: -4.71} 104800 10480 140 0.0 {&#39;learned2&#39;: 5.12, &#39;beat_last&#39;: -5.12} 112800 11280 150 0.0 {&#39;learned2&#39;: 5.38, &#39;beat_last&#39;: -5.38} 120800 12080 160 0.0 {&#39;learned2&#39;: 5.67, &#39;beat_last&#39;: -5.67} 128800 12880 170 0.0 {&#39;learned2&#39;: 5.5, &#39;beat_last&#39;: -5.5} 136800 13680 180 0.0 {&#39;learned2&#39;: 5.82, &#39;beat_last&#39;: -5.82} 144800 14480 190 0.0 {&#39;learned2&#39;: 5.89, &#39;beat_last&#39;: -5.89} 152800 15280 200 0.0 {&#39;learned2&#39;: 5.78, &#39;beat_last&#39;: -5.78} 160800 16080 . 학습이 진행되면서 learned 정책이 BeatLastHeuristic 정책을 쉽게 이기는 것을 알 수 있다. . &#47610;&#51004;&#47728; . ray의 rllib은 다양한 강화학습 방식을 지원할뿐만 아니라 병렬 학습에 대한 처리가 매우 우수하다. 게다가 활발하게 코드가 관리되고 있다. 다만, 사용자 환경이나 모델을 잘 정의하여 활용하기 위해선 이론적 배경과 rllib의 내부 호출 구조를 잘 알아야 한다. .",
            "url": "https://danhojin.github.io/jupyter-blog/rl/2021/02/15/rock-paper-scissors.html",
            "relUrl": "/rl/2021/02/15/rock-paper-scissors.html",
            "date": " • Feb 15, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "그래프 합성곱 신경망",
            "content": "&#48120;&#48516; &#48169;&#51221;&#49885;&#51060; &#52628;&#49345;&#54868;&#46108; &#44536;&#47000;&#54532; . 그래프에서 라플라스 연산자를 만들고 이용하는 것을 처음 보면 상당히 추상적으로 여겨진다. 그 개념을 이해하기 위하여 라플라스 연산자를 살펴보고 그것으로 무엇을 할 수 있는지 알아보자. 그리고 이를 그래프에 확장하고 그래프 푸리에 변환 및 그래프 합성곱 신경망을 설명한다. 마지막으로 spektral 패키지의 예제를 통해 논문 인용 문제에 접근해 보겠다. . &#50976;&#54620; &#52264;&#48516; . 1차원 연속 공간에서 정의된 함수 $u(x)$에 대해 작용하는 라플라스 연산자 $ Delta$는 공간을 등간격 h로 이산화 한 후 다음과 같이 근사할 수 있다[1]. . $ Delta u(x) = u&#39;&#39;(x) approx frac{ frac{u(x+h) - u(x)}{h} - frac{u(x) - u(x-h)}{h}}{h} = frac{u(x - h) - 2u(x) + u(x+h)}{h^2}$ . 2차원인 경우, . $ Delta u(x, y) approx frac{u(x-h, y) + u(x+h, y) + u(x, y-h) + u(x, y+h) - 4 u(x, y)}{h^2}$ . 식과 같이 된다. $(x, y)$에 인접한 혹은 연결된 점들에서 함수 값을 더하고 그 점들의 수만큼 $u(x, y)$ 값을 제한 것에 비례하여 각 점에서의 라플라스 값을 근사할 수 있다. 그 개념을 확장하여 그래프에서 사용하는 $L = D - A$ 행렬을 얻을 수 있고, 자연스럽게 라플라스 행렬이라는 이름을 붙였다[3]. . 라플라스 연산자가 있는 문제에 대하여 어떤 접근을 할 수 있는지 물리적인 문제를 살펴보자. . &#54392;&#47532;&#50640; &#48320;&#54872;&#51012; &#51060;&#50857;&#54620; &#50676;&#54869;&#49328; &#48169;&#51221;&#49885; &#54400;&#51060; . $x in [0, 1]$, $t ge 0$에서 1D 열확산에 대한 열전달 모형으로 시간에 따른 온도 분포는 $u(x, t)$는 $k ge 0$ 계수에 대하여 지배방정식 $u_t - k u_{xx} = 0$을 따른다. 문제를 간단하게 만들기 위하여 $u(0, t) = u(1, t) = 0$인 디리클레 경계치 문제이며 $u(x, 0) = phi(x)$ 초기 분포가 주어진다고 하자. 변수 분리 $(x, t) = X(x)T(t)$ 관계를 적용하면 2개의 상미분 방정식으로 바꿀 수 있다[4]. . $ frac{ dot T}{k T} = frac{X&#39;&#39;}{X} = text{const}$ . 경계 조건에 의하여 상수는 $-(n pi)^2$이고, $X_n(x) = D_n sin(n pi x)$이다. 단, $n=1, 2, ...$ 변수 $t$에 대하여 지수 함수로 일반 해를 구할 수 있으므로, $u(x, t)$는 다음과 같다. . $u(x, t) = sum_n A_n sin(n pi x) exp(-k (n pi)^2 t)$ . $u(x, 0) = phi(x)$ 조건으로부터 $A_n$을 구할 수 있다. . $ int phi(x) sin (m pi x) dx = sum_n A_n int sin (n pi x) sin(m pi x) dx = sum_n A_n frac{ delta_{nm}}{2} = frac{A_m}{2}$ . 이 문제를 푸는 과정에서 발견한 다음 사항에 주목하자. . $ sin (n pi x)$는 라플라스 연산자의 고유 벡터이다. | 고유 벡터 사이에 직교성이 존재하면 계산이 간편하다. | . 고유 벡터로부터 푸리에 급수를 연결해보자[5]. . 프리에 급수에서 사용하는 사인함수는 라플라스 연산자의 고유 벡터이다. | 연속 정의역의 이산 표본은 이산 푸리에 변환으로 연결된다. | . 이를 다음 절에서 그래프에 확장해보겠다. . 이 절을 마무리하면서 실제 값을 대입하여 열확산 문제를 풀어보고 해를 음미해 보겠다. . from sympy import * from sympy.abc import x, t, n from sympy.plotting import plot phi = Piecewise((0, x &lt; 0.4), (1, x &lt; 0.6), (0, True)) . k = 1 A = 2 * integrate(phi * sin(n * pi * x), (x, 0, 1)) u = Sum(A * sin(n * pi * x) * exp(-k * (n * pi) ** 2 * t), (n, 1, 30)) . p = plot( phi, u.doit().subs({t: 0}), u.doit().subs({t: 0.0005}), u.doit().subs({t: 0.005}), u.doit().subs({t: 0.02}), (x, 0, 1), show=False ) p[2].line_color = &#39;orange&#39; p[3].line_color = &#39;red&#39; p[4].line_color = &#39;black&#39; p.show(); . $t=0$에서 불연속 점에서 깁스 현상이 목격된다. 시간이 흐름에 따라 깁스 물결은 확산에 의해 사라지고 온도가 평형 $u(x, infty) = 0$을 향하여 변화되는 것을 잘 모사하고 있다. . &#44536;&#47000;&#54532; &#54392;&#47532;&#50640; &#48320;&#54872; . 이산 시스템에서 이산 푸리에 변환은 다음과 같다[5]. . $A_k = sum_{r = 0}^{n - 1} a_m e^{-i (2 pi mk/n)}$ . $a_m = frac{1}{n} sum_{k = 0}^{n - 1} A_k e^{i (2 pi mk/n)}$ . 지수항은 라플라스 연산자의 고유 벡터이다. 그래프 라플라스 연산자 $ mathcal{L}$는 고유치 $ lambda_{l}$과 고유 벡터 $x_{l}$를 가진다. . $ mathcal{L} x_{l} = lambda_{l} x_{l}$ . N개의 꼭지점으로 구성된 그래프 $ mathcal{G} ( mathcal{V, E})$에서 고유치는 음이 아닌 실수이다. . $0 = lambda_0 lt lambda_1 le lambda_1 cdots le lambda_{N - 1}$ . 이산 푸리에 변환과 마찬가지로 그래프 푸리에 연산을 다음과 같이 정의하여 활용할 수 있다. . $ hat f (l) = sum_{n=1}^{N} x_{l}^* (n) f(n)$ . $f(n) = sum_{l=0}^{N-1} hat f (l) x_{l} (n)$ . &#44536;&#47000;&#54532; &#54633;&#49457;&#44273; &#49888;&#44221;&#47581; . 앞서 라플라스 방정식으로 표현된 내부 열원이 없는 열확산 방정식을 살펴보았다. 이를 확장한 방정식을 푸와송 방정식이라고 한다[7]. . $ Delta phi = f$ . 전산 유체 역학의 유한 체적으로 위 푸와송 방정식의 단위 체적에서 의미는 주변과 주고 받는 정보와 내부에서 생성되는 정보는 서로 상쇄된다는 것이다. 그래프의 각 꼭짓점에서 스칼라 값이 정의되면 주변 꼭짓점과 정보를 주고 받아서 새로운 스칼라 값으로 투사한다. 차수 행렬 D와 인접 행렬 A로 라플라스 연산자 $L = D - A$ 혹은 정규화 하여 $L = I - D^{-1/2} A D^{-1/2}$로 그래프를 나타낼 수 있다. 그래프 내에 여러 섬이 있을 수도 있으므로 수치적인 안정성을 위하여 자기 연결항을 추가하여 $ tilde A = A + I$로 하여 새로 $L = I - tilde D^{-1/2} tilde A tilde D^{-1/2}$로 표현하여 보자. $L x = x + delta x$일 때 새로운 정보에 해당하는 $ tilde D^{-1/2} tilde A tilde D^{-1/2}$을 이용하여 그래프 합성곱 신경망을 다음과 같이 정의하였다[8]. . $H^{(l + 1)} = sigma left( tilde D^{-1/2} tilde A tilde D^{-1/2} H^{(l)} W^{(l)} right) $, $H^{(0)} = X$ . 이 식은 각 꼭짓점의 정보와 인접 정보를 모아서 정보를 생성하고 이에 대하여 학습 층을 쌓겠다는 뜻이다. 추가 항 뿐만 아니라 $I$ 행렬까지 이용한다면 Resnet스러운 접근이 될 것이다. 이 부분에 대해서는 추가적인 숙제로 남겨두겠다. . 이 신경망의 이용을 다음 CORA 인용 데이터를 가지고 그래프 학습을 수행하여 보겠다. Spektral 튜토리얼 코드와 동일하며 설명을 추가하여 정리한다. . &#52280;&#44256; . 위키피디아, https://en.wikipedia.org/wiki/Finite_difference_method | 위키피디아, https://en.wikipedia.org/wiki/Discrete_Laplace_operator | 위키피디아, https://en.wikipedia.org/wiki/Laplacian_matrix | Standford, 편미분 방정식 2003년 여름 강의 노트, Math 220B, https://web.stanford.edu/class/math220b/handouts/heateqn.pdf, 2003 | 단호진 블로그, 푸리에 급수, https://danhojin.github.io/jupyter-blog/general/2021/01/30/dft.html | D.k. Hammond, P. Vandergheynst, and R. Gribonval, Wavelets on graphs via spectral graph theory, arXiv:0912.3848v1, 2009 | 위키피디아, https://en.wikipedia.org/wiki/Poisson&#39;s_equation | T.N. Kipf, M. Welling, Semi-supervised classification with graph convolutional networks, arXiv:1609.02907v4, 2017 | spektral 패키지는 cpu로 학습한다. 잦은 메시지 전달로 cuda의 이점이 없는 듯 하다. | . import os os.environ[&#39;CUDA_VISIBLE_DEVICES&#39;] = &#39;-1&#39; # tensorflow device: cpu import numpy as np import matplotlib.pyplot as plt import tensorflow as tf from tensorflow.keras.callbacks import EarlyStopping from tensorflow.keras.losses import CategoricalCrossentropy from tensorflow.keras.optimizers import Adam %matplotlib inline tf.__version__ . &#39;2.4.0&#39; . CORA &#51064;&#50857; &#45936;&#51060;&#53552; . https://relational.fit.cvut.cz/dataset/CORA . . 2708개의 과학 논문에서 인용 관계를 가지고 온 데이터이다. 각 논문은 7개로 카테고리가 중 하나로 분류되어 있으며 1433개의 어휘집에서 논문에 포함된 어휘가 각 논문의 특징 변수이다. . 내부 코드를 살펴보지 못한 관계로 추정 사항만 정리해본다. . GCNConv 층은 위에서 보인 그래프 합성곱 신경망 층이다. 활성 함수가 없는 것이 기본으로 $X&#39; = hat D^{-1/2} hat A hat D^{-1/2} X W + b$ 항을 계산한다. $ hat D^{-1/2} hat A hat D^{-1/2}$ 부분은 한번만 계산하여 저장해 두면 되므로 LayerPreprocess를 통하여 처리하고, 결과를 희소 행렬에 저장한다. | 다른 종류의 학습 층도 GCNConv 층과 비슷한 transforms 구조로 정의한다. spektral의 다른 예제를 살펴보자. | . from spektral.data.loaders import SingleLoader from spektral.datasets.citation import Citation from spektral.transforms import AdjToSpTensor, LayerPreprocess from spektral.layers import GCNConv from spektral.models.gcn import GCN learning_rate = 1e-2 epochs = 200 patience = 10 data = &#39;cora&#39; seed = 0 tf.random.set_seed(seed=seed) dataset = Citation( data, normalize_x=True, transforms=[LayerPreprocess(GCNConv), AdjToSpTensor()] ) . Pre-processing node features . /home/danhojin/miniconda3/envs/ml8/lib/python3.8/site-packages/scipy/sparse/_index.py:125: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient. self._set_arrayXarray(i, j, x) . 인용 데이터셋에는 하나의 그래프가 들어있다. | 꼭짓점은 2708개, 각 꼭짓점은 1433개의 특징 변수의 데이터가 들어있다. 단어 사전의 어휘가 1433개 이므로 각 단어가 특징 변수가 되었다. | 간선(edge)에는 특징 변수 설정이 되어 있지 않다. | 각 꼭지점은 7개의 클래스 중 하나이다. | . print(len(dataset)) g = dataset[0] g . 1 . Graph(n_nodes=2708, n_node_features=1433, n_edge_features=None, n_labels=7) . a: 인접 행렬 | x: 꼭짓점 특징 | e: 간선 특징 | y: 레이블 | . print(g.a.shape, g.x.shape) # 1433-word dictionary assert(g.e == None) # edges have not features . (2708, 2708) (2708, 1433) . g.y.shape . (2708, 7) . g.y[0] # one-hot . array([0., 0., 0., 1., 0., 0., 0.], dtype=float32) . &#47784;&#45944; . def mask_to_weights(mask): return mask.astype(np.float32) / np.count_nonzero(mask) weights_tr, weights_va, weights_te = ( mask_to_weights(mask) for mask in (dataset.mask_tr, dataset.mask_va, dataset.mask_te) ) . GCN에서 channels은 GCNConv의 은닉층의 크기를 결정한다. | . mod_1 = GCN(n_labels=dataset.n_labels, channels=16, n_input_channels=dataset.n_node_features) mod_1.compile( optimizer=Adam(learning_rate), loss=CategoricalCrossentropy(reduction=&#39;sum&#39;), weighted_metrics=[&#39;acc&#39;] ) . &#54617;&#49845; . loader_tr = SingleLoader(dataset, sample_weights=weights_tr) loader_va = SingleLoader(dataset, sample_weights=weights_va) logs = mod_1.fit( loader_tr.load(), steps_per_epoch=loader_tr.steps_per_epoch, validation_data=loader_va.load(), validation_steps=loader_va.steps_per_epoch, epochs=epochs, callbacks=[EarlyStopping(patience=patience, restore_best_weights=True)], verbose=0, ) . list([k for k in logs.history]) . [&#39;loss&#39;, &#39;acc&#39;, &#39;val_loss&#39;, &#39;val_acc&#39;] . fig, axes = plt.subplots(2, 1, figsize=(6, 8)) axes[0].plot(logs.history[&#39;loss&#39;], label=&#39;tr loss&#39;) axes[0].plot(logs.history[&#39;val_loss&#39;], label=&#39;va loss&#39;) axes[0].legend() axes[1].plot(logs.history[&#39;acc&#39;], label=&#39;tr acc&#39;) axes[1].plot(logs.history[&#39;val_acc&#39;], label=&#39;va acc&#39;) axes[1].legend(); . &#54217;&#44032; . loader_te = SingleLoader(dataset, sample_weights=weights_te) eval_results = mod_1.evaluate(loader_te.load(), steps=loader_te.steps_per_epoch) eval_results . 1/1 [==============================] - 0s 11ms/step - loss: 1.0158 - acc: 0.8230 . [1.0157898664474487, 0.8230001330375671] . &#47610;&#51004;&#47728; . 그래프 학습의 기본 개념을 이해하게 되었다. | 그래프를 다룰 때 물리적인 의미를 가지는 미분 방정식에서 나오는 아이디어를 확장하여 적용하면 좋은 결과를 얻을 수도 있다. | 그래프 합성곱 신경망을 Resnet 스럽게 구성해 볼 수 있을 듯 하다. | .",
            "url": "https://danhojin.github.io/jupyter-blog/graph/2021/02/05/graph-convolutiojnal-networks.html",
            "relUrl": "/graph/2021/02/05/graph-convolutiojnal-networks.html",
            "date": " • Feb 5, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "음성 명령어 인식",
            "content": "torchaudio&#47484; &#51060;&#50857;&#54620; &#51020;&#49457; &#47749;&#47161; &#51064;&#49885; . 음성 처리를 위한 방법으로 신호 처리 기법인 단기 푸리에 변환 stft나 웨이블릿 변환 등을 이용할 수 있다. 이를 이용하여 오디오 신호를 스펙트로그램의 2차원 이미지로 바꾸고 이미지 기법을 이용하는 것이다. tensorflow 튜토리얼에서 이 방법을 엿볼 수 있다[1]. 꽤 좋은 결과를 얻을 수 있으나 신호처리에 대한 개념이 필요하다. 한편 이미지 처리에서 성공적으로 정립된 방법을 응용하여 오디오 신호 처리를 위한 심층 모델을 구성해 보는 방법이 있다. torchaudio 튜토리얼[2]에는 Dai 등[3]이 제안한 모델의 M5모델을 구현하여 음성 명령 인식 문제에 접근하였다. 이 글에서는 Dai 등의 모델을 현재 문제에 맞게 변형하여 구현해보았다. d2l의 Resnet[4] 구현 방법을 참고하였다. . 대부분의 코드는 torchaudio 튜토리얼에서 따왔고, 모델 m9를 제안하여 구현하였다. 모델 m9에 맞도록 일부 튜토리얼 코드는 변형되었다. . &#52280;&#44256; . tensorflow 튜토리얼, https://www.tensorflow.org/tutorials/audio/simple_audio | torchaudio 튜토리얼, https://pytorch.org/tutorials/intermediate/speech_command_recognition_with_torchaudio.html | Wei Dai, Chia Dai, Shuhui Qu, Juncheng Li, Samarjit Das, Very deep convolutional neural networks for raw waveforms, arXiv:1610.00087, 2016 | d2l, 7.6. Residual Networks (Resnet) https://d2l.ai/chapter_convolutional-modern/resnet.html | %matplotlib inline import numpy as np import matplotlib.pyplot as plt import torch import torch.nn as nn import torch.nn.functional as F import torchaudio import IPython.display as ipd import librosa import librosa.display from tqdm.notebook import tqdm device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) print(device) . /home/danhojin/miniconda3/envs/ml8/lib/python3.8/site-packages/torchaudio/backend/utils.py:53: UserWarning: &#34;sox&#34; backend is being deprecated. The default backend will be changed to &#34;sox_io&#34; backend in 0.8.0 and &#34;sox&#34; backend will be removed in 0.9.0. Please migrate to &#34;sox_io&#34; backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail. warnings.warn( . cuda . &#45936;&#51060;&#53440; . 코드와 관련 내용은 torchaudio 튜토리얼 참고하자. . from torchaudio.datasets import SPEECHCOMMANDS import os class SubsetSC(SPEECHCOMMANDS): def __init__(self, subset: str = None): super().__init__(&quot;/home/danhojin/.torch/data&quot;, download=True) def load_list(filename): filepath = os.path.join(self._path, filename) with open(filepath) as fileobj: return [os.path.join(self._path, line.strip()) for line in fileobj] if subset == &quot;validation&quot;: self._walker = load_list(&quot;validation_list.txt&quot;) elif subset == &quot;testing&quot;: self._walker = load_list(&quot;testing_list.txt&quot;) elif subset == &quot;training&quot;: excludes = load_list(&quot;validation_list.txt&quot;) + load_list(&quot;testing_list.txt&quot;) excludes = set(excludes) self._walker = [w for w in self._walker if w not in excludes] # Create training and testing split of the data. We do not use validation in this tutorial. train_set = SubsetSC(&quot;training&quot;) test_set = SubsetSC(&quot;testing&quot;) waveform, sample_rate, label, speaker_id, utterance_number = train_set[0] print(waveform.shape, sample_rate, label) # librosa.display.waveplot(waveform.numpy()[0], sample_rate) ipd.Audio(data=waveform.numpy()[0], rate=sample_rate) # back . torch.Size([1, 16000]) 16000 backward . Your browser does not support the audio element. print(&quot;Shape of waveform: {}&quot;.format(waveform.size())) print(&quot;Sample rate of waveform: {}&quot;.format(sample_rate)) plt.plot(waveform.t().numpy()); . Shape of waveform: torch.Size([1, 16000]) Sample rate of waveform: 16000 . labels = sorted(list(set(datapoint[2] for datapoint in train_set))) print(len(labels), labels[:5]) . 35 [&#39;backward&#39;, &#39;bed&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;dog&#39;] . new_sample_rate = 8000 batch_size = 256 transform = torchaudio.transforms.Resample( orig_freq=sample_rate, new_freq=new_sample_rate) def label_to_index(word): return torch.tensor(labels.index(word)) def index_to_label(index): return labels[index] def pad_sequence(batch): batch = [item.t() for item in batch] batch = torch.nn.utils.rnn.pad_sequence( batch, batch_first=True, padding_value=0.0) return batch.permute(0, 2, 1) def collate_fn(batch): tensors, targets = [], [] for waveform, _, label, *_ in batch: tensors += [waveform] targets += [label_to_index(label)] tensors = pad_sequence(tensors) targets = torch.stack(targets) return tensors, targets if device == torch.device(&#39;cpu&#39;): num_workers = 0 pin_memory = False else: num_workers = 1 pin_memory = True train_loader = torch.utils.data.DataLoader( train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=num_workers, pin_memory=pin_memory ) test_loader = torch.utils.data.DataLoader( test_set, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate_fn, num_workers=num_workers, pin_memory=pin_memory ) . for data, target in train_loader: data = data.to(device) target = target.to(device) data = transform(data) break data.shape . torch.Size([256, 1, 8000]) . &#47784;&#45944; m9 . 파라미터는 0.5M이며 ResidualBlock은 d2l 문서를 참고하여 오디오 처리를 위하여 일부 수정하였다. 모델에 대한 하이퍼파라미터에 대한 탐색은 수행하지 않았다. . class ResidualBlock(nn.Module): def __init__(self, in_channels, out_channels, use_1x1conv=False, stride=1): super().__init__() self.conv1 = nn.Conv1d( in_channels, out_channels, kernel_size=3, padding=1, stride=stride ) self.conv2 = nn.Conv1d( out_channels, out_channels, kernel_size=3, padding=1 ) if use_1x1conv: self.conv3 = nn.Conv1d( in_channels, out_channels, kernel_size=1, stride=stride ) else: self.conv3 = None self.bn1 = nn.BatchNorm1d(out_channels) self.bn2 = nn.BatchNorm1d(out_channels) def forward(self, x): f = F.relu(self.bn1(self.conv1(x))) f = self.bn2(self.conv2(f)) if self.conv3: x = self.conv3(x) return F.relu(f + x) . blk = ResidualBlock(3, 5, use_1x1conv=True) # blk = ResidualBlock(3, 5) x = torch.rand(4, 3, 50) y = blk(x) y.shape . torch.Size([4, 5, 50]) . mod_m9 = nn.Sequential( nn.Conv1d(1, 64, kernel_size=64, stride=4), # 8000 -&gt; 1985 nn.MaxPool1d(4), # 1985 -&gt; 496.35 ResidualBlock(64, 64), # 496 nn.MaxPool1d(4), # 124 ResidualBlock(64, 128, use_1x1conv=True), # 124 nn.MaxPool1d(4), # 124 -&gt; 31 ResidualBlock(128, 256, use_1x1conv=True), # 31 nn.MaxPool1d(4), # 31 -&gt; 7.75 ResidualBlock(256, 35, use_1x1conv=True), # 7 nn.AvgPool1d(7), # 1, [, 35, 1] nn.Flatten(), ) mod_m9.to(device) def count_parameters(model): return sum(p.numel() for p in model.parameters() if p.requires_grad) n = count_parameters(mod_m9) n . 481168 . &#54617;&#49845; . torchaudio 튜토리얼을 참고하자. 다만, train 함수에 loss 함수를 인수로 받게 변경하였다. 패딩 함수의 수정이 필요하나 학습에는 문제가 없으므로 그대로 사용하였다. . def train(model, loss_fn, epoch, log_interval): model.train() for batch_idx, (data, target) in enumerate(train_loader): data = data.to(device) target = target.to(device) # apply transform and model on whole batch directly on device data = transform(data) output = model(data) # negative log-likelihood for a tensor of size (batch x 1 x n_output) loss = loss_fn(output.squeeze(), target) optimizer.zero_grad() loss.backward() optimizer.step() # print training stats if batch_idx % log_interval == 0: print(f&quot;Train Epoch: {epoch} &#39; f&#39;[{batch_idx * len(data)}/{len(train_loader.dataset)} &#39; f&#39;({100. * batch_idx / len(train_loader):.0f}%)] tLoss: &#39; f&#39;{loss.item():.6f}&quot;) # update progress bar pbar.update(pbar_update) # record loss losses.append(loss.item()) def number_of_correct(pred, target): # count number of correct predictions return pred.squeeze().eq(target).sum().item() def get_likely_index(tensor): # find most likely label index for each element in the batch return tensor.argmax(dim=-1) def test(model, epoch): model.eval() correct = 0 for data, target in test_loader: data = data.to(device) target = target.to(device) # apply transform and model on whole batch directly on device data = transform(data) output = model(data) pred = get_likely_index(output) correct += number_of_correct(pred, target) # update progress bar pbar.update(pbar_update) print(f&quot; nTest Epoch: {epoch} tAccuracy: &#39; f&#39;{correct}/{len(test_loader.dataset)} &#39; f&#39;({100. * correct / len(test_loader.dataset):.0f}%) n&quot;) . optimizer = torch.optim.Adam(mod_m9.parameters(), lr=0.01, weight_decay=0.0001) # reduce the learning after 20 epochs by a factor of 10 scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1) loss_fn = nn.CrossEntropyLoss() . log_interval = 250 n_epoch = 21 pbar_update = 1 / (len(train_loader) + len(test_loader)) losses = [] # The transform needs to live on the same device as the model and the data. transform = transform.to(device) with tqdm(total=n_epoch) as pbar: for epoch in range(1, n_epoch + 1): train(mod_m9, loss_fn, epoch, log_interval) test(mod_m9, epoch) scheduler.step() # Let&#39;s plot the training loss versus the number of iteration. plt.plot(losses); plt.title(&quot;training loss&quot;); . Train Epoch: 1 [0/84843 (0%)] Loss: 3.650642 Train Epoch: 1 [64000/84843 (75%)] Loss: 1.062886 Test Epoch: 1 Accuracy: 5963/11005 (54%) Train Epoch: 2 [0/84843 (0%)] Loss: 0.679522 Train Epoch: 2 [64000/84843 (75%)] Loss: 0.519506 Test Epoch: 2 Accuracy: 5584/11005 (51%) Train Epoch: 3 [0/84843 (0%)] Loss: 0.447941 Train Epoch: 3 [64000/84843 (75%)] Loss: 0.467961 Test Epoch: 3 Accuracy: 8768/11005 (80%) Train Epoch: 4 [0/84843 (0%)] Loss: 0.503005 Train Epoch: 4 [64000/84843 (75%)] Loss: 0.391096 Test Epoch: 4 Accuracy: 8954/11005 (81%) Train Epoch: 5 [0/84843 (0%)] Loss: 0.349612 Train Epoch: 5 [64000/84843 (75%)] Loss: 0.334594 Test Epoch: 5 Accuracy: 9356/11005 (85%) Train Epoch: 6 [0/84843 (0%)] Loss: 0.408630 Train Epoch: 6 [64000/84843 (75%)] Loss: 0.305073 Test Epoch: 6 Accuracy: 9592/11005 (87%) Train Epoch: 7 [0/84843 (0%)] Loss: 0.328290 Train Epoch: 7 [64000/84843 (75%)] Loss: 0.324227 Test Epoch: 7 Accuracy: 7881/11005 (72%) Train Epoch: 8 [0/84843 (0%)] Loss: 0.409967 Train Epoch: 8 [64000/84843 (75%)] Loss: 0.224613 Test Epoch: 8 Accuracy: 10132/11005 (92%) Train Epoch: 9 [0/84843 (0%)] Loss: 0.229665 Train Epoch: 9 [64000/84843 (75%)] Loss: 0.204258 Test Epoch: 9 Accuracy: 10083/11005 (92%) Train Epoch: 10 [0/84843 (0%)] Loss: 0.200014 Train Epoch: 10 [64000/84843 (75%)] Loss: 0.220954 Test Epoch: 10 Accuracy: 10180/11005 (93%) Train Epoch: 11 [0/84843 (0%)] Loss: 0.174688 Train Epoch: 11 [64000/84843 (75%)] Loss: 0.168313 Test Epoch: 11 Accuracy: 10169/11005 (92%) Train Epoch: 12 [0/84843 (0%)] Loss: 0.137410 Train Epoch: 12 [64000/84843 (75%)] Loss: 0.204632 Test Epoch: 12 Accuracy: 10192/11005 (93%) Train Epoch: 13 [0/84843 (0%)] Loss: 0.104666 Train Epoch: 13 [64000/84843 (75%)] Loss: 0.115536 Test Epoch: 13 Accuracy: 10205/11005 (93%) Train Epoch: 14 [0/84843 (0%)] Loss: 0.155812 Train Epoch: 14 [64000/84843 (75%)] Loss: 0.200555 Test Epoch: 14 Accuracy: 10175/11005 (92%) Train Epoch: 15 [0/84843 (0%)] Loss: 0.144147 Train Epoch: 15 [64000/84843 (75%)] Loss: 0.100597 Test Epoch: 15 Accuracy: 10243/11005 (93%) Train Epoch: 16 [0/84843 (0%)] Loss: 0.100257 Train Epoch: 16 [64000/84843 (75%)] Loss: 0.118328 Test Epoch: 16 Accuracy: 10239/11005 (93%) Train Epoch: 17 [0/84843 (0%)] Loss: 0.122601 Train Epoch: 17 [64000/84843 (75%)] Loss: 0.103207 Test Epoch: 17 Accuracy: 10243/11005 (93%) Train Epoch: 18 [0/84843 (0%)] Loss: 0.161450 Train Epoch: 18 [64000/84843 (75%)] Loss: 0.098663 Test Epoch: 18 Accuracy: 10230/11005 (93%) Train Epoch: 19 [0/84843 (0%)] Loss: 0.139633 Train Epoch: 19 [64000/84843 (75%)] Loss: 0.137016 Test Epoch: 19 Accuracy: 10236/11005 (93%) Train Epoch: 20 [0/84843 (0%)] Loss: 0.087519 Train Epoch: 20 [64000/84843 (75%)] Loss: 0.105188 Test Epoch: 20 Accuracy: 10243/11005 (93%) Train Epoch: 21 [0/84843 (0%)] Loss: 0.136401 Train Epoch: 21 [64000/84843 (75%)] Loss: 0.109694 Test Epoch: 21 Accuracy: 10234/11005 (93%) . def predict(tensor): # Use the model to predict the label of the waveform tensor = tensor.to(device) tensor = transform(tensor) tensor = mod_m9(tensor.unsqueeze(0)) tensor = get_likely_index(tensor) tensor = index_to_label(tensor.squeeze()) return tensor waveform, sample_rate, utterance, *_ = train_set[-1] ipd.Audio(waveform.numpy(), rate=sample_rate) print(f&quot;Expected: {utterance}. Predicted: {predict(waveform)}.&quot;) . Expected: zero. Predicted: zero. . &#47610;&#51004;&#47728; . 신호 처리에 대한 개념이 적어도 심층 학습을 통하여 오디오 신호를 처리할 수 있다. | 튜토리얼의 M5를 이용하면 검정 세트에 대하여 85%정도의 정확성을 얻는데 반해 m9모델은 93%정도의 정확성을 얻을 수 있었다. | 모델의 크기 차이는 M5 27k에서 m9은 0.5M로 커졌다. | Resnet과 같은 성공적인 모델은 다른 분야에도 성공할 확률이 높다는 점을 확인하였다. | .",
            "url": "https://danhojin.github.io/jupyter-blog/audio/2021/02/02/speech-command-classification.html",
            "relUrl": "/audio/2021/02/02/speech-command-classification.html",
            "date": " • Feb 2, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "푸리에 급수",
            "content": "sympy&#47484; &#51060;&#50857;&#54620; &#54392;&#47532;&#50640; &#44553;&#49688; &#51204;&#44060; . &#52280;&#44256; . D.E.Newland, An introduction to spectral &amp; wavelet analysis, 3rd., Longman scientific &amp; Technical with John Wiley &amp; Sons, Inc., New York, 1994 | Sympy reference, https://docs.sympy.org/latest/modules/series/fourier.html | Numpy reference, https://numpy.org/doc/stable/reference/routines.fft.html | %matplotlib inline import numpy as np import matplotlib.pyplot as plt from sympy import * from sympy.abc import x, t from sympy.plotting import plot . 0에서 $2 pi$를 한 주기로 하는 함수에 대하여 푸리에 전개를 수행해 보고, 다음절에서 동일 함수의 이산 표본에 대한 이산 푸리에 변환 분석을 수행해 보겠다. . step_fn = Piecewise((1, t &lt;= pi), (-1, t &gt; -1)) plot(step_fn); . s = fourier_series(step_fn, (t, 0, 2 * pi)) s5 = s.truncate(5) s5 . $ displaystyle frac{4 sin{ left(t right)}}{ pi} + frac{4 sin{ left(3 t right)}}{3 pi} + frac{4 sin{ left(5 t right)}}{5 pi} + frac{4 sin{ left(7 t right)}}{7 pi} + frac{4 sin{ left(9 t right)}}{9 pi}$ plot(s5); . 계수를 직접 계산해보자[1]. . (1) $x(t) = a_0 + 2 sum_{k=1}^{ infty} left( a_k cos frac{2 pi k t}{T} + b_k sin frac{2 pi k t}{T} right)$ . 단, $k ge 0$에 대하여, . $a_k = frac{1}{T} int_{T} x(t) cos frac{2 pi k t}{T} dt$ . 그리고, $k gt 0$ . $b_k = frac{1}{T} int_{T} x(t) sin frac{2 pi k t}{T} dt$ . from functools import partial def b_k(fn, k): T = 2 * pi return 1 / T * integrate( fn * sin(2 * pi * k * t / T), (t, 0, 2 * pi)) list(map(partial(b_k, step_fn), range(1, 10))) . [2/pi, 0, 2/(3*pi), 0, 2/(5*pi), 0, 2/(7*pi), 0, 2/(9*pi)] . $T = 2 pi$ 인 계산 함수에 대하여 푸리에 전개는 다음과 같다. . $x(t) = sum_{k in text{odd}} frac{4}{k pi} sin kt$ . 복소 푸리에 계수는 $k ge 0$에 대하여, . (2) $X_k = frac{1}{T} int_{T} x(t) e^{ -i ( 2 pi k t / T ) }dt$ . def X_k(k, fn=step_fn): T = 2 * pi return 1 / T * integrate( fn * exp(-I * 2 * pi * k * t / T), (t, 0, 2 * pi) ) list(map(X_k, range(10))) . [0, -2*I/pi, 0, -2*I/(3*pi), 0, -2*I/(5*pi), 0, -2*I/(7*pi), 0, -2*I/(9*pi)] . 복소 푸리에 전개를 수행하였기에 복소항이 보인다. 실수 푸리에 전개 항에서 나온 계수는 복소 푸리에 전개 계수에 비하여 크기가 2배이다. 이에 대한 사항은 이산 푸리에 변환에 대하여 설명하겠다. . numpy &#54056;&#53412;&#51648;&#47484; &#51060;&#50857;&#54620; &#51060;&#49328; &#54392;&#47532;&#50640; &#48320;&#54872; DFT . 연속 함수에서 일정한 간격으로 추출된 데이터를 가지고 있다고 하자. 연속 변수에 대한 푸리에 전개 식 (1)은 다음과 같이 DFT식으로 정리된다[1]. . (3) $X_k = frac{1}{N} sum_{r = 0}^{N - 1} x_r e^{-i (2 pi kr/N)}$ . $x_r = sum_{k = 0}^{N - 1} X_k e^{i (2 pi kr/N)}$ . 푸리에 전개나 변환 관련 식은 응용 분야에 따라 선호하는 형식이 존재하므로 정의를 잘 살펴보아야 한다. numpy에서는 다음과 같이 DFT 식을 정의하였다. . (4) $A_k = sum_{m = 0}^{n - 1} a_m e^{-i (2 pi mk/n)}$ . $a_m = frac{1}{n} sum_{k = 0}^{n - 1} A_k e^{i (2 pi mk/n)}$ . np.set_printoptions(4) n = 128 am = [1.0] * (n // 2) + [-1.0] * (n // 2) Ak = np.fft.fft(am) freq = np.fft.fftfreq(n, d=2* np.pi / n) print(freq[:5]) print(freq[(64 - 5):64]) print(freq[64:(64 + 5)]) print(freq[-5:]) . [0. 0.1592 0.3183 0.4775 0.6366] [ 9.3901 9.5493 9.7085 9.8676 10.0268] [-10.1859 -10.0268 -9.8676 -9.7085 -9.5493] [-0.7958 -0.6366 -0.4775 -0.3183 -0.1592] . plt.plot(freq, Ak.imag, &#39;.&#39;); . 식 (4)을 식 (3)로 맞추어 비교하기 위하여 n으로 나누어 보자. . Ak.imag[:64] / n . array([ 0.0000e+00, -6.3649e-01, 0.0000e+00, -2.1182e-01, 0.0000e+00, -1.2668e-01, 0.0000e+00, -9.0049e-02, 0.0000e+00, -6.9581e-02, 0.0000e+00, -5.6461e-02, 0.0000e+00, -4.7298e-02, 0.0000e+00, -4.0506e-02, 0.0000e+00, -3.5249e-02, 0.0000e+00, -3.1042e-02, 0.0000e+00, -2.7582e-02, 0.0000e+00, -2.4675e-02, 0.0000e+00, -2.2186e-02, 0.0000e+00, -2.0022e-02, 0.0000e+00, -1.8114e-02, 0.0000e+00, -1.6411e-02, 0.0000e+00, -1.4876e-02, 0.0000e+00, -1.3478e-02, 0.0000e+00, -1.2194e-02, 0.0000e+00, -1.1004e-02, 0.0000e+00, -9.8944e-03, 0.0000e+00, -8.8515e-03, 0.0000e+00, -7.8650e-03, 0.0000e+00, -6.9261e-03, 0.0000e+00, -6.0272e-03, 0.0000e+00, -5.1618e-03, 0.0000e+00, -4.3240e-03, 0.0000e+00, -3.5087e-03, 0.0000e+00, -2.7112e-03, 0.0000e+00, -1.9272e-03, 0.0000e+00, -1.1526e-03, 0.0000e+00, -3.8357e-04]) . 이 결과는 식 (2)의 복소 푸리에 계수의 계산치와 일치한다. . list([-2 / (k * np.pi) for k in range(10) if k % 2 != 0]) . [-0.6366197723675814, -0.2122065907891938, -0.12732395447351627, -0.09094568176679733, -0.0707355302630646] . 이산 푸리에 변환에서는 음의 주파수 개념이 존재한다. 이것은 $ theta gt pi$일 때 $e^{i theta}$를 $e^{i(2 pi - theta)}$로 생각하여 연속 함수에서 일정 간격으로 표본을 추출할 때 빠른 주파수의 신호보다는 느린 시계 방향의 신호로 보자는 것이다. DFT의 결과로부터 연속 함수를 재현해 본다면, . $x(t) = A_1 e^{i t} + A_{-1} e^{-i t} + cdots = frac{-2i}{ pi} [ cos(t) + i sin(t)] + frac{2i}{ pi} [ cos(t) - i sin(t)] cdots$ . 최종적으로, . $x(t) = frac{4}{ pi} sin(t) + frac{4}{3 pi} sin(3t) cdots$ . 식으로 정리되어 애초의 sympy에서 얻은 푸리에 급수 식을 얻을 수 있다. 음과 양의 주파수의 합의 의미에서 식 (1)에 합 $ Sigma$ 앞에 2를 붙었다고 볼 수 있다. . 실수 표본 순차열의 이산 푸리에 변환 결과는 대칭이고 주기성을 갖는다. 이 관점에서 음의 주파수 뿐만아니라 에일리어싱(aliasing)도 발생하게 된다. .",
            "url": "https://danhojin.github.io/jupyter-blog/general/2021/01/30/dft.html",
            "relUrl": "/general/2021/01/30/dft.html",
            "date": " • Jan 30, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Pyro에서 독립 차원 선언: plate",
            "content": "시계열 데이터가 아닌 데이터 셋의 각 사례(example)를 생각해보자. 그 사례는 어떤 파라미터를 가진 확률 변수의 실현값이라고 생각할 수 있다. 고정된 파라미터라고 해도 확률 과정이므로 표본 사례의 값은 서로 다를 수 있다. 한편 사례와 사례의 차이는 파라이터의 차이에서도 나올 수 있다. 데이터의 사례뿐만 아니라 숨은 변수가 숨은 확률 변수들의 결합 확률 분포를 따르는 상황도 있을 수 있다. 어는 경우든 데이터의 독립성을 선언해야 할 필요가 생긴다. 이 포스트에서는 plate 컨텍스트를 이용하여 데이터의 각 사례가 독립적이라고 선언하는 방법을 정리해 보겠다. . Pyro의 확률적 통계 추론 입문 튜토리얼의 동전 편형성 문제를 plate를 사용하여 재 구성해보겠다. . &#46041;&#51204; &#54200;&#54693; &#47928;&#51228; . 동전을 열번 던져서 앞면이 7회 나왔다고 할 때 빈도주의에서는 p=0.5로 가정하고 분석을 진행한다. 이항 분포를 따른다고 할 때 관측의 확률은 $ binom{10}{3} frac{1}{2^{10}} = 0.117$이다. 3개보다 더 적게 나오거나 7개 이상 나오는 경우를 따져보면 다음과 같다. . from math import comb from functools import reduce pr = map(lambda k: comb(10, k) / 2 ** 10, [0, 1, 2, 3]) pr = reduce(lambda a, x: a + x, pr) * 2 # two-tail pr . 0.34375 . 빈도주의 관점에서 p=0.5라는 것을 기각하기 어렵다. 베이즈 통계라면 관측 데이터에서 최선의 p를 추정해 보고자 한다. 사전 분포를 B(15.0, 15.0)인 베타 함수로 정의하여 p=0.5 중심에서 약간의 편향이 존재할 수 있다고 보았다. 관측 데이터를 얻은 후 사후 분포를 pyro로 구해 보자. . import numpy as np import pandas as pd import seaborn as sns from scipy.stats import beta import matplotlib.pyplot as plt def plot(a, b, ax): rv = beta(a, b) p = np.linspace(0.1, 0.9, 41) df = pd.DataFrame(dict(p=p, pdf=rv.pdf(p))) return sns.lineplot(x=&#39;p&#39;, y=&#39;pdf&#39;, data=df, ax=ax) fig, ax = plt.subplots() a, b = 15.0, 15.0 ax = plot(a, b, ax) ax.legend([&#39;prior&#39;]); . import torch from torch.distributions import constraints import pyro import pyro.distributions as dist pyro.enable_validation(True) . ELBO . ELBO(Evidence Lower BOund)의 최소화 하고자 한다. $ELBO equiv mathbb{E}_{q_{ phi} (z)} [ log p_{ theta} (x, z) - log q_{ phi} (z)]$ 식으로 주어 진다면 $ log p_{ theta} (x, z)$는 model 함수에서 $ log q_{ phi} (z)$는 guide 함수를 통해서 구하게 된다. 배경 이론에 대해서는 Pyro의 확률적 통계 추론 입문 튜토리얼을 참고하자. pyro.sample 함수가 정의되면 내부적으로 log_prob 함수를 통하여 로그 확률을 평가하게 된다. . 이 문제에서 guide에 베이즈 분석의 사전 분포를 정의하였다. 알고자 하는 분포의 근사로 생각해도 좋다. . &#45936;&#51060;&#53552; . data = [1.0] * 7 + [0.0] * 3 data = torch.tensor(data) data . tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 0.]) . &#47784;&#45944; . model1 + guide1은 숨은 확률 변수 z에서 하나의 값을 실현하고 그 값에 대하여 관측치를 평가한다. 동일 동전으로 10번의 동전 던지기를 수행했다고 볼 수 있다. . def guide1(data): alpha_q = pyro.param( &#39;alpha_q&#39;, torch.tensor(15.0), constraint=constraints.positive) beta_q = pyro.param( &#39;beta_q&#39;, torch.tensor(15.0), constraint=constraints.positive) pyro.sample(&#39;z&#39;, dist.Beta(alpha_q, beta_q)) # shape: [] def model1(data): alpha0 = torch.tensor(10.0) beta0 = torch.tensor(10.0) z = pyro.sample(&#39;z&#39;, dist.Beta(alpha0, beta0)) with pyro.plate(&#39;data&#39;, len(data)): pyro.sample( &#39;obs&#39;, dist.Bernoulli(z), obs=data) . &#54984;&#47144; . import numpy as np svi = pyro.infer.SVI( model1, guide1, pyro.optim.Adam({&#39;lr&#39;: 0.0005}), pyro.infer.Trace_ELBO()) steps = 2000 for step in range(steps): l = svi.step(data) if step % 100 == 0: alpha_q, beta_q = pyro.param(&#39;alpha_q&#39;).item(), pyro.param(&#39;beta_q&#39;).item() print(f&#39;loss: {l:.2f}, alpha_q: {alpha_q:.2f}, beta_q: {beta_q:.2f}&#39;) inferred_mean = alpha_q / (alpha_q + beta_q) # compute inferred standard deviation factor = beta_q / (alpha_q * (1.0 + alpha_q + beta_q)) inferred_std = inferred_mean * np.sqrt(factor) print(&quot; nbased on the data and our prior belief, the fairness &quot; + &quot;of the coin is %.3f +- %.3f&quot; % (inferred_mean, inferred_std)) . loss: 6.84, alpha_q: 15.01, beta_q: 14.99 loss: 7.22, alpha_q: 15.38, beta_q: 14.60 loss: 7.12, alpha_q: 15.75, beta_q: 14.21 loss: 6.91, alpha_q: 15.99, beta_q: 13.95 loss: 7.20, alpha_q: 16.12, beta_q: 13.85 loss: 7.20, alpha_q: 16.24, beta_q: 13.74 loss: 7.13, alpha_q: 16.43, beta_q: 13.56 loss: 6.90, alpha_q: 16.58, beta_q: 13.45 loss: 6.84, alpha_q: 16.58, beta_q: 13.47 loss: 6.79, alpha_q: 16.65, beta_q: 13.41 loss: 6.75, alpha_q: 16.71, beta_q: 13.33 loss: 6.84, alpha_q: 16.71, beta_q: 13.32 loss: 6.80, alpha_q: 16.88, beta_q: 13.18 loss: 6.90, alpha_q: 16.94, beta_q: 13.15 loss: 6.89, alpha_q: 16.92, beta_q: 13.15 loss: 6.87, alpha_q: 17.02, beta_q: 13.05 loss: 6.86, alpha_q: 17.07, beta_q: 13.00 loss: 6.85, alpha_q: 16.99, beta_q: 13.06 loss: 6.88, alpha_q: 16.90, beta_q: 13.11 loss: 6.85, alpha_q: 16.99, beta_q: 13.04 based on the data and our prior belief, the fairness of the coin is 0.566 +- 0.089 . &#49324;&#54980; &#48516;&#54252; . fig, ax = plt.subplots() ax = plot(15.0, 15.0, ax) a, b = pyro.param(&#39;alpha_q&#39;).item(), pyro.param(&#39;beta_q&#39;).item() ax = plot(a, b, ax) ax.legend([&#39;prior&#39;, &#39;model/guide 1&#39;]); . 사후 분포가 사전 분포에서 관측에 의한 p=7/10 방향으로 이동하였다. | MAP로 동전의 편향을 점추정하거나 범위를 추정할 수 있다. | 추가로 동전 던지기를 한다면 새로운 prior로 이 모델을 이용할 수도 있다. | .",
            "url": "https://danhojin.github.io/jupyter-blog/ppl/2021/01/22/pyro-plate.html",
            "relUrl": "/ppl/2021/01/22/pyro-plate.html",
            "date": " • Jan 22, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "공간 변형기 네트워크",
            "content": "F.affine_grid . 이미지의 x, y 표준 좌표: $[-1, 1]$ | $ theta$(N x 2 x 3): $ left[ begin{array}{c} x^s 1 end{array} right] = left[ begin{array}{cc} theta &amp; theta_{ text{translate}} 0 &amp; 1 end{array} right] left[ begin{array}{c} x^t 1 end{array} right]$ 좌변이 출발항(source)인 점에 주의 | . | size N x C x H x W: output image size | align_corners(default False): pixel은 하나의 점이 아니라 크기를 가지는 면으로 취급된다. 아래 그림에서 x, y는 -0.5에서 시작되는 점을 주목하자. 기본 값인 align_corners=False는 출발과 도착 영상이 있을 때 아래 그림 전체를 정규화한 후 보간 샘플링을 진행한다. 반면 align_corners=True로 설정하면 왼쪽 그림에서 $x in [0, 6]$, 오른쪽 그림에서 $x in [0, 2]$의 부분을 맞추고 보간 샘플링을 수행한다. 코드를 직접 짠다면 True의 경우가 더 쉽지만 영상 전체에서 영상 전체로 변형기를 작용시킨다고 할 때 align_corners가 더 직관적이기에 기본 값도 True에서 False로 변경되었다. | . F.grid_sample . input N x C x Hin x Win: input image | . &#52280;&#44256; . Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu, Spatial Transformer Networks, https://arxiv.org/abs/1506.02025 | Pytorch API docs, torch.nn.functional, https://pytorch.org/docs/stable/nn.functional.html | %matplotlib inline import numpy as np import matplotlib.pyplot as plt fig, axes = plt.subplots(1, 2) x1 = np.zeros((5, 7)) x2 = np.zeros((3, 3)) x1[0, :] = x1[2, :] = x1[4, :] = np.array([0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]) x1[1, :] = x1[3, :] = np.array([1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]) x2[0, :] = x2[2, :] = np.array([1.0, 0.0, 1.0]) x2[1, :] = np.array([0.0, 1.0, 0.0]) axes[0].imshow(x1) axes[1].imshow(x2) . &lt;matplotlib.image.AxesImage at 0x7f29eab2a8e0&gt; . import io import requests from PIL import Image import torch import torch.nn.functional as F img = requests.get(&#39;https://raw.githubusercontent.com/d2l-ai/d2l-en/master/img/catdog.jpg&#39;) img = io.BytesIO(img.content) img = Image.open(img).convert(&#39;RGB&#39;) img = np.array(img) img = torch.from_numpy(img).float() / 255.0 plt.imshow(img) . &lt;matplotlib.image.AxesImage at 0x7f2a0a544fa0&gt; . img.shape . (561, 728, 3) . theta_translate = lambda t: torch.tensor([[1.0, 0.0, t[0]], [0.0, 1.0, t[1]]]) def theta_rotate(theta: torch.Tensor): c, s = torch.cos(theta), torch.sin(theta) return torch.tensor([[c, -s, 0.0], [s, c, 0.0]]) def get_transformed_img(src: torch.Tensor, theta): theta = theta.unsqueeze(0) src = src.permute(2, 0, 1).unsqueeze(0) grid = F.affine_grid(theta, src.shape, align_corners=False) return F.grid_sample(src, grid, align_corners=False) . transformed = get_transformed_img(img, theta_translate([1.0, 0.0])) transformed = transformed.squeeze().permute(1, 2, 0) plt.imshow(transformed) . &lt;matplotlib.image.AxesImage at 0x7f2a0ead69d0&gt; . transformed = get_transformed_img(img, theta_rotate(torch.tensor(30.0 * np.pi / 180.0))) transformed = transformed.squeeze().permute(1, 2, 0) plt.imshow(transformed) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . &lt;matplotlib.image.AxesImage at 0x7f2a0ee4cfd0&gt; . y+ 방향이 아래쪽을 향하고 있다는 것에 주의하여 회전 행렬을 생각해야 한다. .",
            "url": "https://danhojin.github.io/jupyter-blog/pytorch/2021/01/21/spatial-transformer-networks.html",
            "relUrl": "/pytorch/2021/01/21/spatial-transformer-networks.html",
            "date": " • Jan 21, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "안녕! Tensorflow Probability - 3",
            "content": "import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline import tensorflow as tf from tensorflow.keras import models, layers import tensorflow_probability as tfp tfd = tfp.distributions if tf.test.gpu_device_name() != &#39;/device:GPU:0&#39;: print(&#39;WARNING: GPU device not found.&#39;) else: print(&#39;SUCCESS: Found GPU: {}&#39;.format(tf.test.gpu_device_name())) . SUCCESS: Found GPU: /device:GPU:0 . &#45936;&#51060;&#53552; . 가상의 데이터에 대하여 회귀 문제를 구성해 보자. 다음 코드는 확률젹 회귀 튜토리얼에서 따온 것이다. 이전 포스트에 이어 분석을 진행해 보겠다. . w0 = 0.125 b0 = 5. x_range = [-20, 60] def load_dataset(n=150, n_tst=150): np.random.seed(43) def s(x): g = (x - x_range[0]) / (x_range[1] - x_range[0]) return 3 * (0.25 + g**2.) x = (x_range[1] - x_range[0]) * np.random.rand(n) + x_range[0] eps = np.random.randn(n) * s(x) y0 = (w0 * x * (1. + np.sin(x)) + b0) y = y0 + eps x = x[..., np.newaxis] x_tst = np.linspace(*x_range, num=n_tst).astype(np.float32) x_tst = x_tst[..., np.newaxis] return y0, y, x, x_tst y0, y, x, x_tst = load_dataset() . ax = sns.lineplot(x=x.squeeze(), y=y0) ax = sns.scatterplot(x=x.squeeze(), y=y, color=sns.color_palette()[1]) ax.set_aspect(&#39;equal&#39;); . &#47784;&#45944;6 . 관측 데이터는 이분산성을 보인다. 변동에 대한 표준 편차도 선형으로 변한다고 생각하고 모델을 세우고 싶다. 이를 위하여 Dense 층을 추가하여 하나는 평균에 하나는 표준 편차에 부가하여 확률 변수를 얻겠다. . mod_6 = models.Sequential([ layers.Input(1), layers.Dense(2), tfp.layers.DistributionLambda( lambda t: tfd.Normal( loc=t[..., :1], scale=tf.math.pow(1e-3 + tf.math.softplus(0.01 * t[..., 1:]), 2.0)) ) ]) mod_6.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 2) 4 _________________________________________________________________ distribution_lambda (Distrib multiple 0 ================================================================= Total params: 4 Trainable params: 4 Non-trainable params: 0 _________________________________________________________________ . epochs = 5000 negloglik = lambda y, rv_y: -rv_y.log_prob(y) mod_6.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01), loss=negloglik) history = mod_6.fit(x, y, epochs=epochs, verbose=False) . [print(np.squeeze(w.numpy())) for w in mod_6.weights]; . [0.12824 2.0271993] [ 5.1934476 98.73518 ] . df = history.history df[&#39;epoch&#39;] = list(range(epochs)) df = pd.DataFrame(df) ax = sns.lineplot(data=df, x=&#39;epoch&#39;, y=&#39;loss&#39;) ax.set_yscale(&#39;log&#39;); . fig, ax = plt.subplots(figsize=(10, 5)) ax = sns.scatterplot(x=x.squeeze(), y=y, color=&#39;orange&#39;, label=&#39;observed&#39;) rv_yhat = mod_6(x_tst) ax = sns.scatterplot(x=x_tst.squeeze(), y=rv_yhat.sample().numpy().squeeze(), color=&#39;b&#39;, label=&#39;sample&#39;) ax.set_aspect(&#39;equal&#39;) ax.legend(); . 모델6에서 얻은 표본은 관측 데이터와 비슷한 경향의 분포를 보인다. 또한 앙상블을 통하여 추론도 가능한 모델을 얻었다. . Tensorflow probability는 keras에 확률 프로그래밍을 매끄럽게 넣었다는 느낌을 받았다. 향후 포스트에서는 이 도구로 어떤 것을 수행할 수 있는지 정리해 보겠다. .",
            "url": "https://danhojin.github.io/jupyter-blog/ppl/2021/01/17/hello-tfp3.html",
            "relUrl": "/ppl/2021/01/17/hello-tfp3.html",
            "date": " • Jan 17, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "안녕! Tensorflow Probability - 2",
            "content": "import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline import tensorflow as tf from tensorflow.keras import models, layers import tensorflow_probability as tfp tfd = tfp.distributions if tf.test.gpu_device_name() != &#39;/device:GPU:0&#39;: print(&#39;WARNING: GPU device not found.&#39;) else: print(&#39;SUCCESS: Found GPU: {}&#39;.format(tf.test.gpu_device_name())) . SUCCESS: Found GPU: /device:GPU:0 . &#48176;&#52824;&#50752; &#51060;&#48292;&#53944; . 확률 프로그래밍 언어인 pyro나 tfp에서 batch_size와 event_size의 의미를 잘 파악해야 한다. 텐서 모양에 대한 pyro 문서를 참고하고 tfp를 통하여 그 개념을 알아보겠다. . &#48176;&#52824; . 익숙한 샘플 평균의 분포에 대하여 생각해보자. 독립·동일 분포를 따르는 확률 변수 $X_k$로부터 평균에 대한 확률 변수를 $ bar X = sum_{k=1}^{n} X_k$로 기술한다. 여기서 독립적인 표본의 수 n은 확률 프로그래밍에서 batch_size를 의미한다. 하나의 표본의 차원은 event_size가 된다. tfp를 통하여 정리해 보자. . def print_sizes(rv): print(f&#39;batch_size: {rv.batch_shape}, event_shape: {rv.event_shape}&#39;) s = rv.sample() print(f&#39;sample: {s}&#39;) print(f&#39;sample size: {s.shape}, log_prob size: {rv.log_prob(s).shape}&#39;) assert rv.sample().shape == rv.batch_shape + rv.event_shape assert rv.log_prob(s).shape == rv.batch_shape return s . N(10, 2)를 따르는 모집단의 평균을 추정하기 위하여 n=6 표본을 얻어서 평균을 구했다고 하자. 다음과 같이 tfd.Normal을 설정하면 n개의 표본을 얻을 수 있다. rv의 batch_size가 6으로 설정되며 n개의 표본이 나오고, log_prob의 size도 그것에 맞게 나오는 것을 확인할 수 있다. . np.random.seed(52) n = 6 rv = tfd.Normal(loc=[10.0] * n, scale=2.0) samp = print_sizes(rv) . batch_size: (6,), event_shape: () sample: [ 9.519256 6.036186 11.836839 9.53339 12.814849 9.927071] sample size: (6,), log_prob size: (6,) . t 분포를 통하여 95% 신뢰 구간을 구하는 방법은 다음과 같다. . from scipy.stats import t m, S = np.mean(samp), np.sqrt(np.var(samp) * n / (n - 1)) m, S . (9.944598, 2.342857862432549) . (m - t.ppf(0.025, n - 1) * S / np.sqrt(n), m + t.ppf(0.025, n - 1) * S / np.sqrt(n)) . (12.403276738920084, 7.485919656953939) . &#51060;&#48292;&#53944; . 한번의 표본이 스칼라가 아닌 차원을 갖는 텐서인 경우도 있다. 다변량 정규분포가 그와 같은 경우이다. 이때의 차원은 event_size에 저장된다. . rv = tfd.MultivariateNormalDiag(loc=[1.0, -1.0], scale_diag=[1.0, 2.0]) s = print_sizes(rv) . batch_size: (), event_shape: (2,) sample: [-0.01391554 -0.6594347 ] sample size: (2,), log_prob size: () . 포본 하나를 추출하였을 때 배치 예에서 나온 샘플과 크기는 2로 같으나 이벤트 예의 샘플은 한번의 이벤트에서 나온 것이기 때문에 log_prob 크기도 그에 맞게 1개로 나온다. . samp = rv.sample((2, 3)) samp.shape, rv.log_prob(samp).shape . (TensorShape([2, 3, 2]), TensorShape([2, 3])) . &#48176;&#52824;-&#51060;&#48292;&#53944; &#51204;&#54872; . 독립적이나 동일 분포가 아닌 확률 변수를 묶어서 하나의 이벤트로 처리하는 것이 필요할 때가 있다. 이때 사용할 수 있는 객체가 tfd.Independent이다. . rv = tfd.Normal(loc=[0.0, 1.0], scale=1.0) ind = tfd.Independent( distribution=rv, reinterpreted_batch_ndims=1 ) rv.batch_shape, rv.event_shape, ind.batch_shape, ind.event_shape . (TensorShape([2]), TensorShape([]), TensorShape([]), TensorShape([2])) . rv = tfd.Normal(loc=np.random.randn(2, 3, 4), scale=1.0) ind = tfd.Independent( distribution=rv, reinterpreted_batch_ndims=2 ) rv.batch_shape, rv.event_shape, ind.batch_shape, ind.event_shape . (TensorShape([2, 3, 4]), TensorShape([]), TensorShape([2]), TensorShape([3, 4])) . &#45936;&#51060;&#53552; . 가상의 데이터에 대하여 회귀 문제를 구성해 보자. 다음 코드는 확률젹 회귀 튜토리얼에서 따온 것이며, 이 포스트 전체에서 그 튜토리얼을 따라가며 이해한 바를 정리하도록 하겠다. . w0 = 0.125 b0 = 5. x_range = [-20, 60] def load_dataset(n=150, n_tst=150): np.random.seed(43) def s(x): g = (x - x_range[0]) / (x_range[1] - x_range[0]) return 3 * (0.25 + g**2.) x = (x_range[1] - x_range[0]) * np.random.rand(n) + x_range[0] eps = np.random.randn(n) * s(x) y0 = (w0 * x * (1. + np.sin(x)) + b0) y = y0 + eps x = x[..., np.newaxis] x_tst = np.linspace(*x_range, num=n_tst).astype(np.float32) x_tst = x_tst[..., np.newaxis] return y0, y, x, x_tst y0, y, x, x_tst = load_dataset() . ax = sns.lineplot(x=x.squeeze(), y=y0) ax = sns.scatterplot(x=x.squeeze(), y=y, color=sns.color_palette()[1]) ax.set_aspect(&#39;equal&#39;); . &#47784;&#45944;4 . 확률젹 회귀 튜토리얼의 모델4을 살펴보자. tfp.layers.DenseVariational 층은 units=1+1 크기의 확률 변수가 나온다. makeposterior 함수는 kernel K와 bias B를 내 주는데 출력 확률 변수는 $Y{DV, k} = x_j K_i + B_i | X_j = xj $가 된다. 다음 층에서 $Y{DV, k=1, 2}$는 각각 정규 함수의 loc과 scale의 인수로 사용되어 새로운 확률 변수가 만들어져 관측치와 비교하게 된다. make_prior는 $K_i$와 $B_i$의 사전 분포로 충분히 넓게 설정해도 된다. . def make_prior(kernel_size, bias_size, dtype): n = kernel_size + bias_size return models.Sequential([ tfp.layers.VariableLayer(n, dtype=dtype), tfp.layers.DistributionLambda( lambda t: tfd.Independent( tfd.Normal(loc=t, scale=3.0), reinterpreted_batch_ndims=1 ) ) # batch_shape, event_shape = ([], [n]) ]) def make_posterior(kernel_size, bias_size, dtype): n = kernel_size + bias_size c = np.log(np.expm1(1.0)) return models.Sequential([ tfp.layers.VariableLayer(2 * n, dtype=dtype), tfp.layers.DistributionLambda( lambda t: tfd.Independent( tfd.Normal(loc=t[..., :n], scale=1e-5 + tf.nn.softplus(c + t[..., n:])), # softplus(c + 0) = 1 reinterpreted_batch_ndims=1 ) ) # batch_shape, event_shape = ([], [n]), X x K + b ]) . mod_4 = tf.keras.models.Sequential([ tf.keras.layers.Input(1), tfp.layers.DenseVariational(1 + 1, make_posterior, make_prior), tfp.layers.DistributionLambda( lambda t: tfd.Normal(loc=t[..., :1], scale=1e-3 + tf.math.softplus(0.01 * t[..., 1:])) ) ]) mod_4.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_variational (DenseVari (None, 2) 12 _________________________________________________________________ distribution_lambda (Distrib multiple 0 ================================================================= Total params: 12 Trainable params: 12 Non-trainable params: 0 _________________________________________________________________ . epochs = 1000 negloglik = lambda y, rv_y: -rv_y.log_prob(y) mod_4.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01), loss=negloglik) history = mod_4.fit(x, y, epochs=epochs, verbose=False) . df = history.history df[&#39;epoch&#39;] = list(range(epochs)) df = pd.DataFrame(df) ax = sns.lineplot(data=df, x=&#39;epoch&#39;, y=&#39;loss&#39;) ax.set_yscale(&#39;log&#39;); . fig, ax = plt.subplots(figsize=(10, 5)) ax = sns.scatterplot(x=x.squeeze(), y=y, color=&#39;orange&#39;, label=&#39;observed&#39;) for i in range(15): rv_yhat = mod_4(x_tst) m = rv_yhat.mean().numpy().squeeze() s = rv_yhat.stddev().numpy().squeeze() ax = sns.lineplot(x=x_tst.squeeze(), y=m, color=&#39;r&#39;, linewidth=1.0, label=&#39;ensemble means&#39; if i == 0 else None) ax = sns.lineplot(x=x_tst.squeeze(), y=m + 2 * s, color=&#39;g&#39;, linewidth=0.5, label=&#39;ensemble means + 2 stdev&#39; if i == 0 else None) ax = sns.lineplot(x=x_tst.squeeze(), y=m - 2 * s, color=&#39;g&#39;, linewidth=0.5, label=&#39;ensemble means - 2 stdev&#39; if i == 0 else None) ax = sns.scatterplot(x=x_tst.squeeze(), y=rv_yhat.sample().numpy().squeeze(), color=&#39;b&#39;, label=&#39;sample&#39;) ax.set_aspect(&#39;equal&#39;) ax.legend(); # ax.legend([&#39;base&#39;, &#39;lm fit&#39;, &#39;train=base+noise&#39;, &#39;sample&#39;]); . 모델 4는 관측 데이터에서 놀 수 있는 확률 선형 함수를 뽑는 모델이다. 하나의 샘플에서 볼 수 있는 데이터는 여전히 관측 데이터처럼 보이지 않지만 앙상불을 거치면 추론이 가능하다. .",
            "url": "https://danhojin.github.io/jupyter-blog/ppl/2021/01/17/hello-tfp2.html",
            "relUrl": "/ppl/2021/01/17/hello-tfp2.html",
            "date": " • Jan 17, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "안녕! Tensorflow Probability - 1",
            "content": "확률 프로그래밍은 통계 추론을 목적으로 발전되었으며 확률 변수나 벡터가 도입되므로 생성 모델에도 사용된다. 대표적으로 stan, pymc3, pyro 등은 중심 라이브러리뿐만 아니라 주변 생태계가 계속 발전해 나가고 있다. 여기에 선택할 수 있는 Tensorflow Probability 라이브러리가 추가되어 개인적으로 관심을 갖게 되었다. 우선 Hello World 격인 확률적 회귀 튜토리얼을 따라가면서 여러 개념을 정리해 보겠다. . import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline import tensorflow as tf import tensorflow_probability as tfp tfd = tfp.distributions if tf.test.gpu_device_name() != &#39;/device:GPU:0&#39;: print(&#39;WARNING: GPU device not found.&#39;) else: print(&#39;SUCCESS: Found GPU: {}&#39;.format(tf.test.gpu_device_name())) . SUCCESS: Found GPU: /device:GPU:0 . &#54869;&#47456; &#48320;&#49688;: tfd . 확률 프로그래밍의 특징으로 샘플링이 가능한 확률 변수의 도입이다. Tensorflow probability에서 이를 가능하게 하는 것이 tfd 모듈이다. tfd 모듈에는 여러 확률 분포가 정의되어 있고, 그로부터 확률 변수를 생성할 수 있다. 확률 변수이므로 샘플링이 가능하고, 확률 밀도 함수 등의 여러 기본 함수에 접근할 수 있다. . rv = tfd.Normal(loc=[0.0, 2.0], scale=[1.0, 0.5]) x = np.linspace(-1.0, 3.0, 41, dtype=np.float32).reshape((-1, 1)) x = np.tile(x, (1, 2)) x = tf.convert_to_tensor(x) . pdf = rv.prob(x) pdf = pd.DataFrame({&#39;x&#39;: x[:, 0], &#39;col1&#39;: pdf[:, 0], &#39;col2&#39;: pdf[:, 1]}) pdf = pdf.melt(id_vars=&#39;x&#39;, value_vars=[&#39;col1&#39;, &#39;col2&#39;]) sns.lineplot(data=pdf, x=&#39;x&#39;, y=&#39;value&#39;, hue=&#39;variable&#39;); . s = rv.sample([100]) s = pd.DataFrame({&#39;col1&#39;: s[:, 0], &#39;col2&#39;: s[:, 1]}).reset_index() s = s.melt(id_vars=&#39;index&#39;, value_vars=[&#39;col1&#39;, &#39;col2&#39;]) sns.histplot(data=s, x=&#39;value&#39;, hue=&#39;variable&#39;, binwidth=0.5, multiple=&#39;dodge&#39;, shrink=0.8); . 분포 함수의 추론 문제라면 주어진 데이터에 대하여 나왔을 경우에 대하여 우도 함수를 계산할 수 있어야 한다. 음의 로그 우도 함수를 다음과 같이 정의하여 사용할 수 있다. . negloglik = lambda y, rv_y: -rv_y.log_prob(y) . &#45936;&#51060;&#53552; . 가상의 데이터에 대하여 회귀 문제를 구성해 보자. 다음 코드는 확률적 회귀 튜토리얼에서 따온 것이며, 이 포스트 전체에서 그 튜토리얼을 따라가며 이해한 바를 정리하도록 하겠다. . w0 = 0.125 b0 = 5. x_range = [-20, 60] def load_dataset(n=150, n_tst=150): np.random.seed(43) def s(x): g = (x - x_range[0]) / (x_range[1] - x_range[0]) return 3 * (0.25 + g**2.) x = (x_range[1] - x_range[0]) * np.random.rand(n) + x_range[0] eps = np.random.randn(n) * s(x) y0 = (w0 * x * (1. + np.sin(x)) + b0) y = y0 + eps x = x[..., np.newaxis] x_tst = np.linspace(*x_range, num=n_tst).astype(np.float32) x_tst = x_tst[..., np.newaxis] return y0, y, x, x_tst y0, y, x, x_tst = load_dataset() . ax = sns.lineplot(x=x.squeeze(), y=y0) ax = sns.scatterplot(x=x.squeeze(), y=y, color=sns.color_palette()[1]) ax.set_aspect(&#39;equal&#39;); . from tensorflow.keras import models, layers mod_1 = tf.keras.Sequential([ layers.Input(1), layers.Dense(1), tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t, scale=1.0)), ]) mod_1.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 1) 2 _________________________________________________________________ distribution_lambda (Distrib multiple 0 ================================================================= Total params: 2 Trainable params: 2 Non-trainable params: 0 _________________________________________________________________ . 활성화 함수가 없으므로 첫 층은 절편과 기울기를 가진 선형 회귀로 볼 수 있다. DistributionLambda 클래스는 tfp.distributions.Distribution 인스턴스를 돌려준다. 직전 층의 출력을 확률 분포 함수의 파라미터로 삼아 확률 변수를 출력하게 된다. 한편 keras에서, 예를 들어 MSE 손실 함수는 tf.keras.losses.MSE(y_true, y_pred)로 정의되는데 첫 번째 인수가 y_true인 점에 주의하자. 앞서 정의된 negloglik(y_true, rv_y) 함수는 손실 함수로 사용할 수 있다는 점을 알 수 있다. . &#47784;&#45944;1 - &#45800;&#49692; &#49440;&#54805; &#54924;&#44480; . epochs = 500 mod_1.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01), loss=negloglik) history = mod_1.fit(x, y, epochs=epochs, verbose=False) . df = history.history df[&#39;epoch&#39;] = list(range(epochs)) df = pd.DataFrame(df) ax = sns.lineplot(data=df, x=&#39;epoch&#39;, y=&#39;loss&#39;) ax.set_yscale(&#39;log&#39;); . for w in mod_1.weights: print(w) . &lt;tf.Variable &#39;dense/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[0.13939044]], dtype=float32)&gt; &lt;tf.Variable &#39;dense/bias:0&#39; shape=(1,) dtype=float32, numpy=array([4.9089847], dtype=float32)&gt; . 절편과 가중치는 각각 4.624, 0.146으로 계산되었다. 데이터 생성에 b0=5, w0=0.125가 사용된 것에 부합된다. 모델에서 마지막 DistributionLambda 층은 어떤 역할을 한 걸까? 확률 분포를 정의할 때 Normal(t, 1)로 scale=1로 설정한 것을 되새겨 보자. 우리의 모델은 $ sigma=1$로 대표되는 범위 내에 데이터가 많이 모이도록 회귀 적합을 수행했다는 것을 알 수 있다. 다만, 모델의 출력 결과는 확률 변수로 예측 모델링의 결정론적인 출력이 아니다. mod_1은 확률 벡터를 되돌려 주므로 샘플링을 수행할 수 있다. . rv_yhat = mod_1(x_tst) assert isinstance(rv_yhat, tfd.Distribution) . fig, ax = plt.subplots(figsize=(10, 5)) ax = sns.lineplot(x=x.squeeze(), y=y0) ax = sns.scatterplot(x=x.squeeze(), y=y, color=sns.color_palette()[1]) xlim = ax.get_xlim() xlim = np.array(xlim) abline = 4.624 + 0.146 * xlim ax = sns.lineplot(x=xlim, y=abline, color=sns.color_palette()[3], linewidth=2.0, ax=ax) ax.set_aspect(&#39;equal&#39;) ax = sns.scatterplot(x=x_tst.squeeze(), y=rv_yhat.sample(1).numpy().squeeze(), ax=ax) ax.legend([&#39;base&#39;, &#39;lm fit&#39;, &#39;train=base+noise&#39;, &#39;sample&#39;]); . mod_1은 회귀 계수의 점 추정 목적을 달성하였고 확률 벡터를 얻었지만 의미 있는 통계적인 추론을 할 수 없는 점은 분명하다. 오차 구조에 대한 어떤 모델도 들어있지 않기 때문이다. 샘플 데이터는 훈련 데이터와 상당히 다르다른 것을 그림에서 확인할 수 있다. . 이 포스트에서는 모델에 확률 변수를 쓸 수 있고, 적합 후에 샘플링을 할 수 있다는 것을 확인하였다. 그리고 확률 프로그램이 keras에 부드럽게 녹아들었다는 사실이 중요하다. 다음 포스트에서는 확률적 회귀 튜토리얼을 이어 따라가면서 tfp 확률 프로그래밍을 더 알아보도록 하겠다. .",
            "url": "https://danhojin.github.io/jupyter-blog/ppl/2021/01/16/hello-tfp1.html",
            "relUrl": "/ppl/2021/01/16/hello-tfp1.html",
            "date": " • Jan 16, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Nadaraya-Watson 커널 회귀",
            "content": "&#50612;&#53584;&#49496; . d2l에서 Nadaraya-Watson 커널 회귀 방법을 사용하여 어텐션 개념을 소개하고 있다[1]. 어텐션을 일종의 회귀로 볼 수 있다는 점은 중요하다. 우리가 알고 있거나 이미 처리해 둔 입력과 출력으로 만들어진 함수가 존재할 때 새로운 입력에 대한 회귀 적합을 생각해 볼 수 있다. 선형 회귀 형식을 빌어보면 기울기 추정량은 다음과 같고, . $ hat beta = (X&#39;X)^{-1} (X&#39;Y)$ . 새로운 입력 $X_{ text{test}}$에 대하여 y를 추정하게 된다. . $ hat y = X_{ text{test}} hat beta = X_{ text{test}} (X&#39;X)^{-1} (X&#39;Y)$ . 위 식에는 훈련 데이터에서 특징 변수 내, 그리고 특징 변수와 응답 변수 사이의 관계가 들어 있다. 결국 새로운 입력은 훈련에 사용된 입력 및 응답을 참고하여 응답을 예측하게 된다. 일반적인 형식으로 표현하면 다음과 같고, 이를 어텐션 풀링(attention pooling)이라고 한다. . $f(x) = sum_{i=1}^{n} alpha(x, x_i) y_i $ . $ alpha$는 어텐션 계수가 되어 기지의 응답에 대한 가중치로 볼 수 있다. 이와 같이 어텐션의 개념은 간단하나 응용을 위하여, 가령 Seq2Seq 모델에 삽입해 RNN의 단점을 보완하는 어텐션 기능을 넣어야 하는 것은 또 다른 문제이므로 기존 연구자들의 성공적인 구성을 많이 참고해야 하겠다. . 일견 간단한 Nadaraya-Watson 커널 회귀 문제에 대한 d2l 코드를 처음 보았을 때 당혹감이 일었다. 수식은 간단하나 pytorch 프레임 내에서 어떻게 구현해야 하는지 머리에 떠오르지 않았기 때문이다. 이 포스트는 d2l 코드를 참고하면서 스스로 이해할 수 있도록 코드를 재구성의 결과이다. 다음 사항에 대하여 정리하는 계기가 되었다. . 학습 파라미터의 직접 정의 | torch.repeat 함수 및 텐서 연산에서 브로드캐스팅(broadcasting) | . 참고 . Aston Zhang, et al., Dive into Deep Learning, https://d2l.ai/chapter_attention-mechanisms/nadaraya-waston.html | 유원준, 딥 러닝을 이용한 자연어 처리 입문, https://wikidocs.net/22893 | # !pip install torch==1.7.1+cpu torchvision==0.8.2+cpu torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html . &#51064;&#44277; &#45936;&#51060;&#53552; &#48143; &#50976;&#54008;&#54632;&#49688; &#51221;&#51032; . d2l 코드를 그대로 재사용 하였다. . from d2l import torch as d2l import torch import torch.nn as nn import torch.nn.functional as F import numpy as np import seaborn as sns torch.manual_seed(95) n_train = 50 # No. of training examples x_train, _ = torch.sort(torch.rand(n_train) * 5) # Training inputs def f(x): return 2 * torch.sin(x) + x**0.8 y_train = f(x_train) + torch.normal(0.0, 0.5, (n_train,)) # Training outputs x_test = torch.arange(0, 5, 0.1) # Testing examples y_truth = f(x_test) # Ground-truth outputs for the testing examples print(f&#39;shape of train x/y: {x_train.shape}/{y_train.shape}&#39;) print(f&#39;shape of test x: {x_test.shape}&#39;) def plot_kernel_reg(y_hat): d2l.plot(x_test, [y_truth, y_hat], &#39;x&#39;, &#39;y&#39;, legend=[&#39;Truth&#39;, &#39;Pred&#39;], xlim=[0, 5], ylim=[-1, 5]) d2l.plt.plot(x_train, y_train, &#39;o&#39;, alpha=0.5); . shape of train x/y: torch.Size([50])/torch.Size([50]) shape of test x: torch.Size([50]) . &#50612;&#53584;&#49496; &#54400;&#47553; &#47784;&#45944; . 이미 알고 있는 데이터라는 점을 강조하기 위하여 keys/values 짝을 파라미터에 정의하였다. 이 데이터는 모델 생성시 넣어 주어야 한다. 학습 가능한 파라미터는 자유도 1인 weight 하나이다. . class NWKernelRegression(nn.Module): def __init__(self, keys, values): super().__init__() self.keys = nn.Parameter(keys, requires_grad=False) self.values = nn.Parameter(values, requires_grad=False) self.weight = nn.Parameter(torch.rand(1), requires_grad=True) @property def attention_weights(self): return self._attention_weights.detach() def forward(self, queries: torch.Tensor): N = queries.size(0) # Batch size self._attention_weights = queries - self.keys.repeat((N, 1)) self._attention_weights = F.softmax( torch.pow(self._attention_weights * self.weight, 2.0) / (-2.0), dim=-1) return (self._attention_weights * self.values.repeat((N, 1))).sum(dim=-1) # kind of torch.bmm . &#54617;&#49845; . torch.manual_seed(52) epochs, lr = 100, 0.5 mask = torch.randint(n_train, size=(int(n_train * 0.2),)) mod_1 = NWKernelRegression(x_train[~mask], y_train[~mask]) loss = nn.MSELoss(reduction=&#39;mean&#39;) optimizer = torch.optim.SGD(mod_1.parameters(), lr=lr) animator = d2l.Animator(xlabel=&#39;epoch&#39;, ylabel=&#39;loss&#39;, xlim=[0, epochs]) for epoch in range(1, epochs + 1): mod_1.train() pred = mod_1(x_train.view((-1, 1))) l = loss(pred[~mask], y_train[~mask]) optimizer.zero_grad() l.backward() optimizer.step() mod_1.eval() l_eval = loss(pred[mask], y_train[mask]) animator.add(epoch, [l.item(), l_eval.item()]) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-01-15T03:54:10.902598 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ 검증셋으로부터 epoch=30 정도부터 과적합이 되는 것을 알 수 있다. 이 값을 가지고 최종 적합을 진행하겠다. . epochs, lr = 30, 0.5 mod_1 = NWKernelRegression(x_train, y_train) loss = nn.MSELoss(reduction=&#39;mean&#39;) optimizer = torch.optim.SGD(mod_1.parameters(), lr=lr) animator = d2l.Animator(xlabel=&#39;epoch&#39;, ylabel=&#39;loss&#39;, xlim=[0, epochs]) for epoch in range(1, epochs + 1): mod_1.train() pred = mod_1(x_train.view((-1, 1))) l = loss(pred, y_train) optimizer.zero_grad() l.backward() optimizer.step() animator.add(epoch, [l.item()]) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-01-15T03:54:13.472796 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ &#54217;&#44032; . 학습 결과 weight=2.58을 얻었다. 히트맵에 때르면 검정 데이터 주변의 훈련 데이터를 참고하여 어텐션 가중치를 구하여 응답을 구성하는 것을 알 수 있다. . list([p for p in mod_1.parameters() if p.requires_grad]) . [Parameter containing: tensor([2.4232], requires_grad=True)] . mod_1.eval() y_hat = mod_1(x_test.view((-1, 1))) plot_kernel_reg(y_hat.detach()) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-01-15T03:54:13.652769 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ import seaborn as sns ax = sns.heatmap(mod_1.attention_weights.numpy(), cmap=&#39;YlGnBu&#39;) ax.set_xlabel(&#39;Sorted training inputs (keys)&#39;) ax.set_ylabel(&#39;Sorted testing inputs (queries)&#39;); . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-01-15T03:54:13.852701 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/",
            "url": "https://danhojin.github.io/jupyter-blog/nlp/2021/01/15/nadaraya-watson-kernel-regression.html",
            "relUrl": "/nlp/2021/01/15/nadaraya-watson-kernel-regression.html",
            "date": " • Jan 15, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Seq2Seq 기계 번역",
            "content": "이 포스트에는 Dive into Deep Learning 9장의 Seq2Seq 코드를 재구현하였다. Seq2Seq 학습의 배경이나 자세한 내용은 d2l 누리집에 들어있으나, 직접 작성해보고 나서야 코드의 자세한 부분을 이해할 수 있었다. . Seq2Seq 학습을 통하여 해결하고자 하는 문제는 Fig. 9.7.1에 나타나 있다. 영어 문장에서 출발하여 불어로 도착하는 번역을 하고자 한다. 이 문제를 풀기 위하여 생성 모델이 가능한 오토인코더 구조로 Seq2Seq 모델을 개발하였다. 도착어 입장에서 입력과 출력은 같으며 bos(begin of sentence), eos(end of sentence) 토큰 차이만 있다. . Fig. 9.7.1 Sequence to sequence learning with an RNN encoder and an RNN decoder. . from d2l import torch as d2l import numpy as np import torch import torch.nn as nn import torch.nn.functional as F . Data . 출발과 도착어의 문장은 184, 201개의 어휘로 구성되는 작은 데이터셋이다. 문장 시작 bos, 문장 끝 eos, 채움 pad, 모름 unk 등의 특별한 의미를 갖는 단어/토큰도 이용한다. 출발·도착 문장은 뜻은 대응되나 길이가 다를 수 있고, eos 이후 seq_len를 보장하기 위하여 pad가 삽입되어 있다. valid_len에 유효 토큰의 수가 들어있다. . batch_size, seq_len = 64, 10 train_iter, src_vocab, dst_vocab = d2l.load_data_nmt(batch_size, seq_len) len(src_vocab), len(dst_vocab) . (184, 201) . &#39;|&#39;.join([src_vocab.idx_to_token[k] for k in range(10)]) . &#34;&lt;unk&gt;|&lt;pad&gt;|&lt;bos&gt;|&lt;eos&gt;|.|!|i|i&#39;m|it|go&#34; . for batch in train_iter: x, x_valid_len, y, y_valid_len = batch break x.shape, x_valid_len.shape, y.shape, y_valid_len.shape . (torch.Size([64, 10]), torch.Size([64]), torch.Size([64, 10]), torch.Size([64])) . print(f&#39;number of valid tokens: {x_valid_len[0]}&#39;) &#39; &#39;.join([src_vocab.idx_to_token[i] for i in x[0]]) . number of valid tokens: 4 . &#34;i&#39;m hit ! &lt;eos&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;&#34; . print(f&#39;number of valid tokens: {x_valid_len[0]}&#39;) &#39; &#39;.join([dst_vocab.idx_to_token[i] for i in y[0]]) . number of valid tokens: 4 . &#39;je suis &lt;unk&gt; ! &lt;eos&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;&#39; . Seq2Seq model . 모델의 구조 특징은 다음과 같다. . 오토인코더를 생성 모델로 사용하기 위하여 부호기 Encoder와 복호기 Decoder의 RNN 은닉 스테이트를 공유 | 은닉 스테이트의 마지막 층은 context로 칭하여 입력에 주입 | . class Seq2SeqEnc(nn.Module): def __init__(self, src_vocab_size, embed_dim, hidden_size, num_layers, dropout=0.0): super().__init__() self.embedding = nn.Embedding(src_vocab_size, embed_dim) self.rnn = nn.GRU(embed_dim, hidden_size, num_layers, dropout=dropout) def forward(self, x): x = self.embedding(x).permute(1, 0, 2) out, state = self.rnn(x) return out, state class Seq2SeqDec(nn.Module): def __init__(self, dst_vocab_size, embed_dim, hidden_size, num_layers, dropout=0.0): super().__init__() self.embedding = nn.Embedding(dst_vocab_size, embed_dim) self.rnn = nn.GRU(embed_dim + hidden_size, hidden_size, num_layers, dropout=dropout) self.linear = nn.Linear(hidden_size, dst_vocab_size) def forward(self, y, state): y = self.embedding(y).permute(1, 0, 2) context = state[-1].repeat(y.shape[0], 1, 1) # batch x hidden -&gt; seq x batch x hidden out, state = self.rnn(torch.cat([y, context], -1), state) out = self.linear(out).permute(1, 0, 2) # out.shape batch x seq x dst vocab # state.shape layers x batch x hidden return out, state class Seq2SeqEncDec(nn.Module): def __init__(self, src_vocab_size, dst_vocab_size, embed_dim, hidden_size, num_layers, dropout=0.0): super().__init__() self.enc = Seq2SeqEnc(src_vocab_size, embed_dim, hidden_size, num_layers, dropout) self.dec = Seq2SeqDec(dst_vocab_size, embed_dim, hidden_size, num_layers, dropout) def forward(self, x, y): _, state = self.enc(x) out, state = self.dec(y, state) return out, state . Loss function . 도착어의 입력 문장 머리에 bos를 삽입 | 문장마다 의미 있는 토큰 수가 다르므로 마스크 트릭을 사용 | . embed_dim, hidden_size, num_layers, dropout = 32, 32, 3, 0.1 batch_size, seq_len = 128, 10 lr, epochs, device = 0.01, 400, torch.device(&#39;cpu&#39;) # d2l.try_gpu() train_iter, src_vocab, dst_vocab = d2l.load_data_nmt(batch_size, seq_len) mod_1 = Seq2SeqEncDec(len(src_vocab), len(dst_vocab), embed_dim, hidden_size, num_layers, dropout) mod_1.to(device) optimizer = torch.optim.Adam(mod_1.parameters(), lr=lr) loss = nn.CrossEntropyLoss() animator = d2l.Animator(xlabel=&#39;epoch&#39;, ylabel=&#39;loss&#39;, xlim=[0, epochs], yscale=&#39;log&#39;) mod_1.train() for epoch in range(1, epochs + 1): losses = 0 for batch in train_iter: X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch] bos = torch.tensor([dst_vocab[&#39;&lt;bos&gt;&#39;]] * Y.shape[0], device=device).reshape((-1, 1)) Y_in = torch.cat([bos, Y[:, :-1]], -1) Y_hat, _ = mod_1(X, Y_in) mask = torch.arange(seq_len).unsqueeze(0).to(device) &lt; Y_valid_len.unsqueeze(-1) mask = mask.reshape(-1) Y_hat = Y_hat.reshape((-1, len(dst_vocab))) Y = Y.reshape(-1) l = loss(Y_hat[mask], Y[mask]) optimizer.zero_grad() l.backward() optimizer.step() losses += l.item() animator.add(epoch, [losses]) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-01-14T06:38:07.496789 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ Prediction . 출발어 영어 입력에서 도착어 불어 생성은 ᅟpredict 함수를 이용한다. 내부절차는 다음과 같다. . 모델의 Encoder와 Decoder를 별개로 활용 | Decoder는 토큰별로 하나씩 처리하여 문장을 생성 | . def predict(model: Seq2SeqEncDec, src_sentence, src_vocab, dst_vocab, seq_len, device): model.eval() src_tokens = src_vocab[src_sentence.lower().split(&#39; &#39;)] + [src_vocab[&#39;&lt;eos&gt;&#39;]] src_valid_len = torch.tensor([min(seq_len, len(src_tokens))], device=device) src_tokens = src_tokens[:seq_len] src_tokens = src_tokens + [src_vocab[&#39;&lt;pad&gt;&#39;]] * max(0, seq_len - len(src_tokens)) src_tokens = torch.tensor(src_tokens, device=device).unsqueeze(0) # add batch dim _, state = model.enc(src_tokens) y = torch.tensor([dst_vocab[&#39;&lt;bos&gt;&#39;]], dtype=torch.long, device=device).unsqueeze(0) out = [] for _ in range(seq_len): y, state = model.dec(y, state) y = y.argmax(dim=-1) pred = y.squeeze(0).long().item() if pred == dst_vocab[&#39;&lt;eos&gt;&#39;]: break out.append(pred) return &#39; &#39;.join(dst_vocab.to_tokens(out)) . predict(mod_1, &quot;i lost .&quot;, src_vocab, dst_vocab, 10, device) . &#34;j&#39;ai perdu .&#34; . Evaluation . d2l의 코드를 거의 그대로 사용하였다. 샘플 결과만 가지고도 성능이 좋지 않은 것을 알 수 있다. 훈련 데이터가 적기도 하지만 Seq2Seq 자체만으로는 한계가 있기 때문이다. Seq2Seq 성능 개선을 위하여 어텐션 등이 개발되었으며 이에 대한 얘기는 다음으로 미루겠다. 다만 Seq2Seq 차체만으로도 RNN 이해를 위한 연습문제로 손색이 없었다. . import collections def bleu(pred_seq, label_seq, k): #@save &quot;&quot;&quot;Compute the BLEU.&quot;&quot;&quot; pred_tokens, label_tokens = pred_seq.split(&#39; &#39;), label_seq.split(&#39; &#39;) len_pred, len_label = len(pred_tokens), len(label_tokens) score = np.exp(min(0, 1 - len_label / len_pred)) for n in range(1, k + 1): num_matches, label_subs = 0, collections.defaultdict(int) for i in range(len_label - n + 1): label_subs[&#39;&#39;.join(label_tokens[i: i + n])] += 1 for i in range(len_pred - n + 1): if label_subs[&#39;&#39;.join(pred_tokens[i: i + n])] &gt; 0: num_matches += 1 label_subs[&#39;&#39;.join(pred_tokens[i: i + n])] -= 1 score *= np.power(num_matches / (len_pred - n + 1), np.power(0.5, n)) return score engs = [&#39;go .&#39;, &quot;i lost .&quot;, &#39;he &#39;s calm .&#39;, &#39;i &#39;m home .&#39;] fras = [&#39;va !&#39;, &#39;j &#39;ai perdu .&#39;, &#39;il est calme .&#39;, &#39;je suis chez moi .&#39;] for eng, fra in zip(engs, fras): translation = predict( mod_1, eng, src_vocab, dst_vocab, seq_len, device) print(f&#39;{eng} =&gt; {translation}, bleu {bleu(translation, fra, k=2):.3f}&#39;) . go . =&gt; va !, bleu 1.000 i lost . =&gt; j&#39;ai perdu ., bleu 1.000 he&#39;s calm . =&gt; bonne chance ., bleu 0.000 i&#39;m home . =&gt; je suis chez moi bon ., bleu 0.803 .",
            "url": "https://danhojin.github.io/jupyter-blog/nlp/2021/01/14/Seq2Seq.html",
            "relUrl": "/nlp/2021/01/14/Seq2Seq.html",
            "date": " • Jan 14, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "순환 신경망(RNN)을 이용한 문자열 생성",
            "content": "&#45936;&#51060;&#53552; . 낱말 3만개, 17만개의 문자열로 구성된 H. G. Well의 The Time Machine을 이용하였다. 토큰 과정이나 ngram 분석은 Dive into Deep Learning 누리집을 참고하자. 데이터는 d2l 패키지에서 불러와 바로 사용할 수 있다. . 참고 . Aston Zhang 등, Dive into deep learning, https://d2l.ai/chapter_recurrent-neural-networks/rnn-concise.html | %matplotlib inline from d2l import torch as d2l import math import torch from torch import nn from torch.nn import functional as F batch_size, seq_len = 32, 35 train_iter, vocab = d2l.load_data_time_machine(batch_size, seq_len) device = torch.device(&#39;cpu&#39;) torch.__version__ . &#39;1.7.1+cu110&#39; . 문자열은 알파벳 26자와 스페이스로 구성된다. 그 밖에 기호를 포함하여 모르는 문자는 unk에 저장되어 총 28개로 토큰화된다. . len(vocab) . 28 . &#39;, &#39;.join([vocab.idx_to_token[k] for k in range(28)]) . &#39;&lt;unk&gt;, , e, t, a, i, n, o, s, h, r, d, l, m, u, c, f, w, g, y, p, b, v, k, x, z, j, q&#39; . 훈련 데이터는 torch.long 타입으로 레이블 y는 입력 x가 한 글자씩 밀려 들어있다. 입력은 원핫 인코딩을 이용하여 변환하고 레이블은 그대로 이용하겠다. . for x, y in train_iter: break x.shape, y.shape . (torch.Size([32, 35]), torch.Size([32, 35])) . x.dtype . torch.int64 . x[0] . tensor([ 9, 2, 1, 3, 5, 13, 2, 1, 13, 4, 15, 9, 5, 6, 2, 1, 21, 19, 1, 9, 1, 18, 1, 17, 2, 12, 12, 8, 5, 3, 9, 2, 1, 3, 5]) . x0 = map(lambda idx: vocab.idx_to_token[idx], x[0]) &#39;&#39;.join(list(x0)) . &#39;he time machine by h g wellsithe ti&#39; . y0 = map(lambda idx: vocab.idx_to_token[idx], y[0]) &#39;&#39;.join(list(y0)) . &#39;e time machine by h g wellsithe tim&#39; . &#47784;&#45944; . 단방향 순환 신경층을 구성할 수 있는 Pytorch의 RNN 클래스를 이용하였다. RNN의 마지막 타임스텝 출력만이 아니라 전체 시퀀스를 이용하며 각 출력은 선형층에 연결된다. 문자열 생성이 목적으로 과적합을 막기 위한 장치는 사용하지 않았다. Pytorch의 RNN 입력은 시퀀스(L)x배치(B)x특징 차원으로 주입된다. . class SimpleRNN(nn.Module): def __init__(self, input_size, hidden_size): super().__init__() self.rnn = nn.RNN(input_size, hidden_size) # batch_first: False self.linear = nn.Linear(hidden_size, input_size) def forward(self, x): out, _ = self.rnn(x) out = F.relu(out) out = map(lambda x: F.relu(self.linear(x)), out) out = torch.stack(list(out)) return out . hidden_size = 512 input_size = vocab_size = len(vocab) mod_0 = SimpleRNN(input_size, hidden_size) mod_0.to(device) mod_0.eval() x = F.one_hot(x.T, 28).float() print(f&#39;input shape: (seq, batch, feature) {x.shape}&#39;) y_pred = mod_0(x.to(device)) y_pred.shape . input shape: (seq, batch, feature) torch.Size([35, 32, 28]) . torch.Size([35, 32, 28]) . 훈련 과정에서 lr을 크게 하면서도 수치적인 안정성을 유지하기 위하여 clip_grad_norm 함수를 사용하였다. . def train(model, train_iter, vocab, lr, num_epochs, device, use_random_iter=False): loss = nn.CrossEntropyLoss() animator = d2l.Animator(xlabel=&#39;epoch&#39;, ylabel=&#39;loss&#39;, legend=[&#39;train&#39;], xlim=[10, num_epochs]) # Initialize optimizer = torch.optim.SGD(model.parameters(), lr) # Train and predict for epoch in range(1, num_epochs + 1): l_sum = 0 for x, y in train_iter: x = F.one_hot(x.T, 28).float() y = y.T.reshape(-1) out = model(x.to(device)) l = loss(out.reshape((-1, 28)), y.to(device)) optimizer.zero_grad() l.backward() nn.utils.clip_grad_norm_(model.parameters(), 1.0) optimizer.step() l_sum += l.item() animator.add(epoch, [l_sum]) . num_epochs, lr = 500, 0.5 hidden_size = 512 input_size = vocab_size = len(vocab) mod_1 = SimpleRNN(input_size, hidden_size) mod_1.to(device) train(mod_1, train_iter, vocab, lr, num_epochs, device) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; 2021-01-07T15:12:52.816955 image/svg+xml Matplotlib v3.3.3, https://matplotlib.org/ def generate(prefix, num_preds, model, vocab, device): model.eval() outputs = vocab[list(prefix)] for _ in range(num_preds): x = torch.tensor(outputs).reshape((-1, 1)) x = F.one_hot(x, 28).float() out = model(x.to(device)) out = torch.argmax(out[-1].squeeze()) outputs.append(out.item()) return &#39;&#39;.join([vocab.idx_to_token[i] for i in outputs]) out = generate(&#39;time traveller &#39;, 60, mod_1, vocab, device) out . &#39;time traveller proceeded anyreal body must have extension in four direction&#39; . &#47610;&#51004;&#47728; . 순환 신경망을 포함하는 문자열 생성 모델을 pytorch에서 구현 | map과 같은 python 함수를 사용해도 훈련이나 예측 과정에서 문제가 없음 | clip_grad_norm 함수를 사용하여 수치 불안정을 낮춤 | .",
            "url": "https://danhojin.github.io/jupyter-blog/nlp/2021/01/07/language-model-generating-characters.html",
            "relUrl": "/nlp/2021/01/07/language-model-generating-characters.html",
            "date": " • Jan 7, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "네이버 영화 리뷰의 감성 분석",
            "content": "한글로 표기된 한국어는 어형변화가 많고 조사가 많은 역할을 한다. 조사로 말미암아 어순도 유연하게 구사할 수 있을뿐만 아니라 띄어쓰기가 내용을 전달하는데 결정적인 역할을 하지도 않는다. 이런 특징은 문장의 토큰화에 어려움을 가중시킨다. 게다가 최근 한국어 문서에서 알파벳 표기가 같이 들어오는 경우도 많은데 이를 처리하는 것도 또 다른 숙제이다. 그럼에도 다른 언어를 위하여 개발된 자연어 처리를 위한 기계학습과 심층학습의 원리는 한국어에도 마찬가지로 적용할 수 있으며, 한국어 처리를 위한 원리는 보편적으로도 유용하게 쓰일 수 있을 것이다. . 이 포스트에서는 카카오에서 공개한 khaiii 형태소 분석기를 이용하여 네이버 영화 리뷰 말뭉치에 대한 감성 분석을 진행해 보겠다. . 참고 . 전창욱, 최태균, 조중현, 신성진, 텐서플로2와 머신러닝으로 시작하는 자연어 처리, 위키북스, 2020 | 프랑소와 숄레, 박해선 옮김, 케라스 창시자에게 배우는 딥러닝 | &#45936;&#51060;&#53552; . 전체 영화 리뷰 데이터에서 형태소 분석을 진행한 후 훈련셋과 검정셋으로 나누어 진행하겠다. 20만 건의 리뷰가 있으며 긍정과 부정 레이블이 들어있다. . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline df = pd.read_csv(&#39;https://github.com/e9t/nsmc/raw/master/ratings.txt&#39;, delimiter=&#39; t&#39;) print(f&#39;The shape of dataframe: {df.shape}&#39;) df.head() . The shape of dataframe: (200000, 3) . id document label . 0 8112052 | 어릴때보고 지금다시봐도 재밌어요ㅋㅋ | 1 | . 1 8132799 | 디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산... | 1 | . 2 4655635 | 폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고. | 1 | . 3 9251303 | 와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런... | 1 | . 4 10067386 | 안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화. | 1 | . fig, ax = plt.subplots(figsize=(10, 8)) median = df[&#39;document&#39;].astype(str).apply(len).median() ax = sns.histplot(df[&#39;document&#39;].astype(str).apply(len), bins=30, ax=ax); ax.axvline(x=median, linewidth=2.5, color=&#39;blue&#39;) ax.text(median - 20, -2000, f&#39;median={median}&#39;, color=&#39;blue&#39;, fontsize=15); . 리뷰 길이는 최대 140자로 제한되어 있으며 중앙값이 27로 짧은 리뷰이다. . &#54805;&#53468;&#49548; &#48516;&#49437; . 형태소 분석에 khaiii를 이용한다. 가장 간단하게 분석을 진행하기 위하여 알파벳이나 숫자, 기타 기호 등을 제거해 보면, 빈 리뷰가 생기는데 데이터 셋에서 이를 제거하였다. document_1 열에 정리된 리뷰가 저장된다. . import re p = re.compile(r&#39;^[ s]*$&#39;) df = df.dropna() df[&#39;document_1&#39;] = (df[&#39;document&#39;].copy() .map(lambda el: re.sub(r&quot;[^가-힣ㅏ-ㅣ s]&quot;, &quot; &quot;, el)) .map(lambda el: re.sub(r&#39;[ s]+&#39;, &quot; &quot;, el)) ) df[&#39;document_2&#39;] = df[&#39;document_1&#39;].copy().map( lambda el: True if p.search(el) == None else False ) df = df[df[&#39;document_2&#39;]].drop(columns=[&#39;document_2&#39;]) print(f&#39;The shape of dataframe: {df.shape}&#39;) df.head() . The shape of dataframe: (197900, 4) . id document label document_1 . 0 8112052 | 어릴때보고 지금다시봐도 재밌어요ㅋㅋ | 1 | 어릴때보고 지금다시봐도 재밌어요 | . 1 8132799 | 디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산... | 1 | 디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업... | . 2 4655635 | 폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고. | 1 | 폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고 | . 3 9251303 | 와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런... | 1 | 와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지 | . 4 10067386 | 안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화. | 1 | 안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화 | . 2100개의 리뷰가 제거되었다. 형태소 분석을 위한 함수를 작성하고 리뷰에 적용하여 morphemes 열에 저장하였다. . from khaiii import KhaiiiApi api = KhaiiiApi() def get_morphemes(review): morphemes = [w for w in api.analyze(review)] morphemes = [m.lex for w in morphemes for m in w.morphs] return list(morphemes) df[&#39;morphemes&#39;] = df[&#39;document_1&#39;].apply(get_morphemes) df.head() . id document label document_1 morphemes . 0 8112052 | 어릴때보고 지금다시봐도 재밌어요ㅋㅋ | 1 | 어릴때보고 지금다시봐도 재밌어요 | [어리, ㄹ, 때, 보, 고, 지금다시, 보, 아도, 재밌, 어요] | . 1 8132799 | 디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산... | 1 | 디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업... | [디자인, 을, 배우, 는, 학생, 으로, 외국, 디자이, 너, 와, 그, 들, 이... | . 2 4655635 | 폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고. | 1 | 폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고 | [폴리스스토리, 시리즈, 는, 부, 터, 뉴, 까지, 버리, ㄹ께, 하나, 도, 없... | . 3 9251303 | 와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런... | 1 | 와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지 | [오, 아, 연기, 가, 진, 짜, 개, 쩔, 구나, 지루, 하, ㄹ, 거, 이, ... | . 4 10067386 | 안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화. | 1 | 안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화 | [안개, 자욱, 하, ㄴ, 밤하늘, 에, 뜨, 어, 있, 는, 초승달, 같, 은, 영화] | . morphemes = [m for r in df[&#39;morphemes&#39;] for m in r] morphemes = list(morphemes) print(f&#39;The length of morphemes: {len(morphemes)}&#39;) morphemes[:5] . The length of morphemes: 3530533 . [&#39;어리&#39;, &#39;ㄹ&#39;, &#39;때&#39;, &#39;보&#39;, &#39;고&#39;] . 약 20만개의 리뷰가 350만개의 형태소로 분석되었으니, 평균적으로 각 리뷰는 형태소 18개로 구성된다. 리뷰 길이의 중앙값은 27이었으므로 상당 수의 형태소는 길이 1 또는 2인 것을 알 수 있다. 형태소에 대하여 더 분석해 보자. . from collections import Counter morphemes_freq = Counter(morphemes) morphemes_freq = list(zip(*[(k, v) for (k, v) in morphemes_freq.items()])) morphemes_freq = pd.DataFrame({&#39;morpheme&#39;: morphemes_freq[0], &#39;n&#39;: morphemes_freq[1]}) morphemes_freq = morphemes_freq.sort_values(by=&#39;n&#39;, ascending=False) morphemes_freq = morphemes_freq.reset_index(drop=True) morphemes_freq = morphemes_freq.reset_index() morphemes_freq[&#39;index&#39;] = morphemes_freq[&#39;index&#39;] + 1 morphemes_freq.head() . index morpheme n . 0 1 | 이 | 154942 | . 1 2 | 하 | 139855 | . 2 3 | ㄴ | 99839 | . 3 4 | 는 | 89213 | . 4 5 | 다 | 75460 | . morphemes_freq.morpheme[:100].values . array([&#39;이&#39;, &#39;하&#39;, &#39;ㄴ&#39;, &#39;는&#39;, &#39;다&#39;, &#39;영화&#39;, &#39;고&#39;, &#39;보&#39;, &#39;의&#39;, &#39;가&#39;, &#39;도&#39;, &#39;에&#39;, &#39;은&#39;, &#39;을&#39;, &#39;었&#39;, &#39;지&#39;, &#39;ㄹ&#39;, &#39;어&#39;, &#39;게&#39;, &#39;들&#39;, &#39;았&#39;, &#39;나&#39;, &#39;있&#39;, &#39;것&#39;, &#39;를&#39;, &#39;없&#39;, &#39;아&#39;, &#39;되&#39;, &#39;만&#39;, &#39;좋&#39;, &#39;는데&#39;, &#39;기&#39;, &#39;로&#39;, &#39;적&#39;, &#39;주&#39;, &#39;너무&#39;, &#39;였&#39;, &#39;여&#39;, &#39;으로&#39;, &#39;음&#39;, &#39;정말&#39;, &#39;네&#39;, &#39;어요&#39;, &#39;같&#39;, &#39;지만&#39;, &#39;ㄴ다&#39;, &#39;ㅁ&#39;, &#39;에서&#39;, &#39;않&#39;, &#39;안&#39;, &#39;수&#39;, &#39;말&#39;, &#39;면&#39;, &#39;아니&#39;, &#39;과&#39;, &#39;점&#39;, &#39;거&#39;, &#39;네요&#39;, &#39;시&#39;, &#39;만들&#39;, &#39;그&#39;, &#39;뭐&#39;, &#39;연기&#39;, &#39;재미있&#39;, &#39;평점&#39;, &#39;던&#39;, &#39;진짜&#39;, &#39;재밌&#39;, &#39;나오&#39;, &#39;잘&#39;, &#39;이런&#39;, &#39;겠&#39;, &#39;라&#39;, &#39;ㅠ&#39;, &#39;습니다&#39;, &#39;듯&#39;, &#39;ㅂ니다&#39;, &#39;이것&#39;, &#39;생각&#39;, &#39;싶&#39;, &#39;왜&#39;, &#39;와&#39;, &#39;최고&#39;, &#39;요&#39;, &#39;더&#39;, &#39;내&#39;, &#39;어서&#39;, &#39;스토리&#39;, &#39;아서&#39;, &#39;사람&#39;, &#39;까지&#39;, &#39;감동&#39;, &#39;오&#39;, &#39;한&#39;, &#39;보다&#39;, &#39;재미&#39;, &#39;ㅡ&#39;, &#39;배우&#39;, &#39;때&#39;, &#39;지루&#39;], dtype=object) . 영화 리뷰인 만큼 영화라는 낱말이 많이 사용되고 있어 사실상 의미가 없는 불용어로 처리해도 되겠다. 기분을 나타내는 재미는 재미, 재밌, 재미있 등의 별개의 형태소 khaiii가 분석한 것을 알 수 있다. 과도하게 많이 사용한 형태소를 찾아 보자. . # fig = px.line(morph_freq.iloc[:200], x=&#39;index&#39;, y=&#39;n&#39;, log_y=True) # fig.show() fig, ax = plt.subplots(figsize=(10, 8)) # ax.set(yscale=&#39;log&#39;) sns.lineplot(x=&#39;index&#39;, y=&#39;n&#39;, data=morphemes_freq.iloc[:200], ax=ax); . 기울기가 급격하게 변하는 15000회 이상 사용된 다음 형태소는 불용어로 지정하겠다. . stop_words = morphemes_freq.loc[morphemes_freq.n &gt; 15000, &#39;morpheme&#39;].values stop_words = set(stop_words) &#39; &#39;.join(stop_words) . &#39;의 다 하 만 를 아 어 고 보 은 있 을 좋 았 는 도 게 영화 들 가 이 었 없 나 는데 되 지 ㄴ ㄹ 에 것&#39; . morphemes_freq[&#39;morpheme&#39;].apply(len).value_counts() . 3 35970 2 32527 4 19306 5 7683 6 2387 1 2272 7 848 8 341 9 139 10 61 11 31 12 18 13 7 14 4 17 4 16 3 15 3 21 3 24 3 18 3 28 2 30 2 22 2 31 2 129 1 36 1 35 1 39 1 133 1 29 1 26 1 23 1 20 1 47 1 Name: morpheme, dtype: int64 . 대부분의 형태소는 2, 3, 4글자인 것을 알 수 있다. 100자가 넘는 형태소도 존재하는데 다음과 같은 것이며 토큰화를 거치며 제거될 것이다. . morphemes_freq[morphemes_freq[&#39;morpheme&#39;].apply(len) &gt; 100] . index morpheme n . 40688 40689 | ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ... | 1 | . 93706 93707 | 의리의리의리의리의리의리의리의리의리의리의리의리의리의리의리의리의리의리의리의리의리의리의리... | 1 | . 수동으로 불용어를 형태소 사전에서 제거하겠다. . morphemes = [m for m in morphemes if m not in stop_words] morphemes = [(k, v) for (v, k) in enumerate(morphemes)] morphemes = dict(morphemes) len(morphemes) . 101600 . 이렇게 얻어진 10만개의 형태소를 이용하여 감성 분석을 진행해 보자. . &#53664;&#53360;&#54868;&#50752; &#54984;&#47144;&#183;&#44160;&#51221;&#49483; &#51456;&#48708; . 우선 불용어를 제거하고, 이 과정에서 길이가 0이된 리뷰를 제거한다. . df[&#39;morphemes_2&#39;] = df[&#39;morphemes&#39;].map(lambda el: [m for m in el if m not in stop_words]) df[&#39;morphemes_2_len&#39;] = df[&#39;morphemes_2&#39;].apply(len) df = df[df[&#39;morphemes_2_len&#39;] &gt; 0] df.shape . (197640, 7) . df = (df.drop(columns=[&#39;morphemes&#39;, &#39;morphemes_2_len&#39;]) .rename(columns={&#39;morphemes_2&#39;: &#39;morphemes&#39;})) df.head() . id document label document_1 morphemes . 0 8112052 | 어릴때보고 지금다시봐도 재밌어요ㅋㅋ | 1 | 어릴때보고 지금다시봐도 재밌어요 | [어리, 때, 지금다시, 아도, 재밌, 어요] | . 1 8132799 | 디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산... | 1 | 디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업... | [디자인, 배우, 학생, 으로, 외국, 디자이, 너, 와, 그, 일, 군, 전통, ... | . 2 4655635 | 폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고. | 1 | 폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고 | [폴리스스토리, 시리즈, 부, 터, 뉴, 까지, 버리, ㄹ께, 하나, 음, 최고] | . 3 9251303 | 와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런... | 1 | 와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지 | [오, 연기, 진, 짜, 개, 쩔, 구나, 지루, 거, 라고, 생각, 였, 몰입, ... | . 4 10067386 | 안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화. | 1 | 안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화 | [안개, 자욱, 밤하늘, 뜨, 초승달, 같] | . fig, ax = plt.subplots(figsize=(10, 8)) sns.histplot(x=df[&#39;morphemes&#39;].apply(len), bins=30); . 리뷰의 형태소 분포는 위와 같다. 리뷰의 길이는 가변적이므로 동일한 길이의 입력으로 변환하겠다. 길이 40 이상은 절단하고 부족한 리뷰에 대해서는 패팅을 추가한다. 이를 위해서 keras의 pad_sequence 함수를 이용할 것이다. . from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences . maxlen = 40 # The number of morphemes per review max_words = 80000 # 80% morphemes tokenizer = Tokenizer(num_words=max_words) tokenizer.fit_on_texts(df.morphemes) sequences = tokenizer.texts_to_sequences(df.morphemes) len(tokenizer.word_index) . 101600 . wi = [(k, v) for (k, v) in tokenizer.word_index.items()] wi = [kv for (_, kv) in zip(range(5), wi)] wi = list(wi) wi . [(&#39;기&#39;, 1), (&#39;로&#39;, 2), (&#39;적&#39;, 3), (&#39;주&#39;, 4), (&#39;너무&#39;, 5)] . data = pad_sequences(sequences, maxlen=maxlen) labels = df.label.values data.shape, labels.shape . ((197640, 40), (197640,)) . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( data, labels, test_size=0.3, random_state=952) . &#47784;&#45944; 1 . import tensorflow as tf from tensorflow.keras import models, layers embedding_dim = 100 mod_1 = models.Sequential([ layers.Embedding(max_words, embedding_dim, input_length=maxlen), layers.Flatten(), layers.Dropout(0.5), layers.Dense(32, activation=&#39;relu&#39;), layers.Dropout(0.5), layers.Dense(1, activation=&#39;sigmoid&#39;) ]) mod_1.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding (Embedding) (None, 40, 100) 8000000 _________________________________________________________________ flatten (Flatten) (None, 4000) 0 _________________________________________________________________ dropout (Dropout) (None, 4000) 0 _________________________________________________________________ dense (Dense) (None, 32) 128032 _________________________________________________________________ dropout_1 (Dropout) (None, 32) 0 _________________________________________________________________ dense_1 (Dense) (None, 1) 33 ================================================================= Total params: 8,128,065 Trainable params: 8,128,065 Non-trainable params: 0 _________________________________________________________________ . mod_1.compile( optimizer=&#39;rmsprop&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;] ) early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor=&#39;val_accuracy&#39;, min_delta=0.0001, patience=2) history = mod_1.fit( X_train, y_train, epochs=10, batch_size=512, validation_split=0.2, callbacks=[early_stopping_cb] ) . Epoch 1/10 217/217 [==============================] - 5s 19ms/step - loss: 0.5980 - accuracy: 0.6558 - val_loss: 0.4060 - val_accuracy: 0.8125 Epoch 2/10 217/217 [==============================] - 4s 16ms/step - loss: 0.3921 - accuracy: 0.8288 - val_loss: 0.3911 - val_accuracy: 0.8215 Epoch 3/10 217/217 [==============================] - 4s 16ms/step - loss: 0.3617 - accuracy: 0.8464 - val_loss: 0.3780 - val_accuracy: 0.8319 Epoch 4/10 217/217 [==============================] - 4s 18ms/step - loss: 0.3349 - accuracy: 0.8599 - val_loss: 0.3825 - val_accuracy: 0.8312 Epoch 5/10 217/217 [==============================] - 4s 17ms/step - loss: 0.3166 - accuracy: 0.8697 - val_loss: 0.3928 - val_accuracy: 0.8286 . results = mod_1.evaluate(X_test, y_test) results . 1853/1853 [==============================] - 2s 968us/step - loss: 0.3906 - accuracy: 0.8293 . [0.3905869126319885, 0.8293361663818359] . 정확도 82.9%를 얻었다. . &#47784;&#45944; 2: &#54633;&#49457;&#44273; &#49888;&#44221;&#47581; . 전창욱 등의 자연어 처리[1]에서 모델을 가져왔다. 다만 maxlen이나 max_words는 다르게 설정하였다. . import tensorflow as tf from tensorflow.keras import models, layers from tensorflow.keras import Input from tensorflow.keras.constraints import max_norm maxlen = 40 max_words = 80000 embedding_dim = 128 inp = Input(shape=(maxlen,)) embedded_inp = layers.Embedding(max_words, embedding_dim, input_length=maxlen)(inp) embedded_inp = layers.Dropout(0.5)(embedded_inp) conv_3 = layers.Conv1D(100, 3, activation=&#39;relu&#39;, kernel_constraint=max_norm(3.0))(embedded_inp) conv_3 = layers.GlobalMaxPooling1D()(conv_3) conv_4 = layers.Conv1D(100, 4, activation=&#39;relu&#39;, kernel_constraint=max_norm(3.0))(embedded_inp) conv_4 = layers.GlobalMaxPooling1D()(conv_4) conv_5 = layers.Conv1D(100, 5, activation=&#39;relu&#39;, kernel_constraint=max_norm(3.0))(embedded_inp) conv_5 = layers.GlobalMaxPooling1D()(conv_5) out = tf.concat([conv_3, conv_4, conv_5], axis=-1) # out = layers.Dropout(0.5)(out) # out = layers.Dense(250, &quot;relu&quot;)(out) out = layers.Dense(250, &quot;relu&quot;, kernel_constraint=max_norm(3.0))(out) # out = layers.Dropout(0.5)(out) # out = layers.Dense(1, &quot;relu&quot;)(out) out = layers.Dense(1, &quot;relu&quot;, kernel_constraint=max_norm(3.0))(out) mod_2 = models.Model(inp, out) mod_2.summary() . Model: &#34;model&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 40)] 0 __________________________________________________________________________________________________ embedding_1 (Embedding) (None, 40, 128) 10240000 input_1[0][0] __________________________________________________________________________________________________ dropout_2 (Dropout) (None, 40, 128) 0 embedding_1[0][0] __________________________________________________________________________________________________ conv1d (Conv1D) (None, 38, 100) 38500 dropout_2[0][0] __________________________________________________________________________________________________ conv1d_1 (Conv1D) (None, 37, 100) 51300 dropout_2[0][0] __________________________________________________________________________________________________ conv1d_2 (Conv1D) (None, 36, 100) 64100 dropout_2[0][0] __________________________________________________________________________________________________ global_max_pooling1d (GlobalMax (None, 100) 0 conv1d[0][0] __________________________________________________________________________________________________ global_max_pooling1d_1 (GlobalM (None, 100) 0 conv1d_1[0][0] __________________________________________________________________________________________________ global_max_pooling1d_2 (GlobalM (None, 100) 0 conv1d_2[0][0] __________________________________________________________________________________________________ tf.concat (TFOpLambda) (None, 300) 0 global_max_pooling1d[0][0] global_max_pooling1d_1[0][0] global_max_pooling1d_2[0][0] __________________________________________________________________________________________________ dense_2 (Dense) (None, 250) 75250 tf.concat[0][0] __________________________________________________________________________________________________ dense_3 (Dense) (None, 1) 251 dense_2[0][0] ================================================================================================== Total params: 10,469,401 Trainable params: 10,469,401 Non-trainable params: 0 __________________________________________________________________________________________________ . tf.keras.utils.plot_model(mod_2, show_shapes=True) . mod_2.compile( optimizer=&#39;rmsprop&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;] ) early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor=&#39;val_accuracy&#39;, min_delta=0.0001, patience=2) history = mod_2.fit( X_train, y_train, epochs=10, batch_size=512, validation_split=0.2, callbacks=[early_stopping_cb] ) . Epoch 1/10 217/217 [==============================] - 10s 36ms/step - loss: 0.8143 - accuracy: 0.6626 - val_loss: 0.5413 - val_accuracy: 0.7874 Epoch 2/10 217/217 [==============================] - 8s 36ms/step - loss: 0.4931 - accuracy: 0.8087 - val_loss: 0.6397 - val_accuracy: 0.7467 Epoch 3/10 217/217 [==============================] - 8s 36ms/step - loss: 0.4788 - accuracy: 0.8198 - val_loss: 0.4737 - val_accuracy: 0.8172 Epoch 4/10 217/217 [==============================] - 8s 37ms/step - loss: 0.4582 - accuracy: 0.8346 - val_loss: 0.4968 - val_accuracy: 0.8169 Epoch 5/10 217/217 [==============================] - 8s 37ms/step - loss: 0.4499 - accuracy: 0.8460 - val_loss: 0.5240 - val_accuracy: 0.8102 . results = mod_2.evaluate(X_test, y_test) results . 1853/1853 [==============================] - 3s 2ms/step - loss: 0.5296 - accuracy: 0.8124 . [0.5295822620391846, 0.812386155128479] . 정확도 81.2%를 얻었다. . &#47610;&#51004;&#47728; . 한국어에 대한 형태소 분석, 토큰화, 이진 분류 모델을 구성 | 이를 이용하여 네이버 영화 리뷰에 대한 감성 분류 | khaiii 형태소 분석기로도 어느 정도의 성과 확인 | .",
            "url": "https://danhojin.github.io/jupyter-blog/nlp/2021/01/01/nsmc.html",
            "relUrl": "/nlp/2021/01/01/nsmc.html",
            "date": " • Jan 1, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "모분산 불편 추정량과 모 표준편차",
            "content": "수리통계학 연습 문제 4.2.10을 살피다 지금까지 별 의심 없이 써왔던 모분산의 불편 추청량 $S^2$을 다시 생각해보게 되었다. 모 표준편차의 분편 추정량을 $ sqrt{S^2}$으로 볼 수 있냐는 문제이다. 모분산의 불편 추정량은 다음과 같다. . $S^2 = frac{ sum_{i=1}^{n} (X_i - bar X_n)^2}{n - 1}$ . $X_i sim N( mu, sigma^2)$이고, $ bar X_n = sum_{i=1}^{n} X_i$이다. 불편 추청량이라 함은 $E(S^2) = sigma^2$ 식이 성립한다는 것이다. 그렇다면, $E(S) = sigma$이라고 볼 수 있나? 그렇지 않다. 더 자세한 내용은 모 표준 편차의 불편 추청에 관한 위키피디아 문서를 참고하기 바란다[2]. 문제의 힌트를 사용해서 $E(S)$를 계산해 보겠다. . $E(S) = frac{ sigma}{ sqrt{n-1}} E left[ sqrt{ frac{(n-1)S^2}{ sigma^2}} right]$ . 제곱근 안의 항은 $ frac{(n-1)S^2}{ sigma^2} sim chi^2(n-1)$이므로 감마분포의 확률밀도함수(pdf) $f(x)$를 이용하여 다음과 같이 쓸 수 있다. . $E(S) = frac{ sigma}{ sqrt{n-1}} int_0^{ infty} x^{1/2} f(x) dx$ . 단, 확률밀도함수 f(x)는 자유도 $r$과 정의 구간 $0&lt;x&lt; infty$에 대하여 다음과 같다. . $f(x) = frac{1}{ Gamma(r/2) 2^{r/2}} x^{r/2 - 1} e^{-x/2}$ . 연습문제에서 $n=9$이고, $r=n-1=8$이다. 자유도를 적분식에 넣고 감마 함수로 정리하면 어렵지 않게 $E(s)$ 값을 계산할 수도 있지만, 여기에서는 sympy 패키지를 이용하여 적분을 풀어보겠다. . 참고 문헌 . 호그, 매킨, 크레이그, 박태영 옮김, 수리통계학 개론, 7판, Pearson/경문사, 2018 | 위키피디아, Unbiased estimation of standard deviation, 최종 편집 2020년 5월 7일, https://en.wikipedia.org/wiki/Unbiased_estimation_of_standard_deviation | from sympy import * init_printing() x, sigma = symbols(&#39;x, sigma&#39;) n = 9 r = Rational(n - 1, 1) f = 1 / gamma(r / 2) / 2**(r / 2) * x**(r / 2 - 1) * exp(-x / 2) f . $ displaystyle frac{x^{3} e^{- frac{x}{2}}}{96}$ E_S = sigma / sqrt(r) * integrate(sqrt(x) * f, [x, 0, oo]) # print(E_S) E_S . $ displaystyle frac{35 sqrt{ pi} sigma}{64}$ E_S.evalf() . $ displaystyle 0.969310699713954 sigma$ 불편 분산의 제곱근 기댓값은 $E(S) = 0.969 sigma$로 불편 분산의 제곱근은 모 표준 편차의 불편량으로 사용할 수 없다는 점을 확인하였다. . 4.2.10 (b)에서 신뢰구간이 확률변수 $t(8) = sqrt{9} ( bar X - mu) / S$에 근거를 두므로 95% 신뢰 구간의 길이는 $2t_{ alpha/2, n-1} S / sqrt{n}$이다. 마지막으로 $S$와 $ sigma$ 관계를 삽입하면 신뢰 구간의 길이를 얻을 수 있다. . from scipy.stats import t import numpy as np rv = t(n - 1) 2 * rv.ppf(0.975) / np.sqrt(9) * 0.96931 . $ displaystyle 1.49015524541946$ 최종적으로 계산된 신뢰 구간의 길이는 1.49 $ sigma$이다. .",
            "url": "https://danhojin.github.io/jupyter-blog/statistics/2020/12/27/%EB%AA%A8%EB%B6%84%EC%82%B0-%EB%AA%A8%ED%91%9C%EC%A4%80%ED%8E%B8%EC%B0%A8-%EB%B6%88%ED%8E%B8-%EC%B6%94%EC%A0%95%EB%9F%89.html",
            "relUrl": "/statistics/2020/12/27/%EB%AA%A8%EB%B6%84%EC%82%B0-%EB%AA%A8%ED%91%9C%EC%A4%80%ED%8E%B8%EC%B0%A8-%EB%B6%88%ED%8E%B8-%EC%B6%94%EC%A0%95%EB%9F%89.html",
            "date": " • Dec 27, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://danhojin.github.io/jupyter-blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://danhojin.github.io/jupyter-blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://danhojin.github.io/jupyter-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://danhojin.github.io/jupyter-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}